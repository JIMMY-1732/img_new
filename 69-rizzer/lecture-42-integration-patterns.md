# LECTURE DESIGN PHILOSOPHY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PEDAGOGICAL APPROACH                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  NAIVE IMPLEMENTATION FIRST                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  Show the broken version. Let them see WHY every pattern        â”‚
â”‚  exists before learning HOW it works.                           â”‚
â”‚                                                                 â”‚
â”‚  DELEGATION PRINCIPLE                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  The recurring theme: your backend is a COORDINATOR,            â”‚
â”‚  not an executor. Outsource to specialists.                     â”‚
â”‚                                                                 â”‚
â”‚  SYNTHESIS OVER NOVELTY                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚  Most "new" code here is ASSEMBLING tools students already      â”‚
â”‚  know: httpx, Celery, Redis, SQLAlchemy, Pydantic.             â”‚
â”‚  The lecture teaches how to COMPOSE them into real features.     â”‚
â”‚                                                                 â”‚
â”‚  CONNECT TO CAPSTONE                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  Every pattern maps directly to a capstone requirement.         â”‚
â”‚  Students should leave knowing exactly where each piece         â”‚
â”‚  fits in their project.                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# LECTURE OUTLINE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INTEGRATION PATTERNS                          â”‚
â”‚                     (3-4 Hour Lecture)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PART 1: THE INTEGRATION REALITY (30 min)                       â”‚
â”‚  â”œâ”€ 1.1 The Demo That Fell Apart                                â”‚
â”‚  â”œâ”€ 1.2 The Core Principle: Delegate, Don't Execute             â”‚
â”‚  â”œâ”€ 1.3 The Company Headquarters Analogy                        â”‚
â”‚  â””â”€ 1.4 Where Each Pattern Lives (Project Structure)            â”‚
â”‚                                                                 â”‚
â”‚  PART 2: EMAIL & NOTIFICATION INTEGRATION (45 min)              â”‚
â”‚  â”œâ”€ 2.1 The Synchronous Email Trap                              â”‚
â”‚  â”œâ”€ 2.2 The Architecture: Endpoint â†’ Task â†’ Provider            â”‚
â”‚  â”œâ”€ 2.3 Building the Email Service Abstraction                  â”‚
â”‚  â”œâ”€ 2.4 Sending via Background Task                             â”‚
â”‚  â”œâ”€ 2.5 Receiving Delivery Status (Webhooks)                    â”‚
â”‚  â””â”€ 2.6 Common Mistakes                                         â”‚
â”‚                                                                 â”‚
â”‚  PART 3: FILE HANDLING WITH OBJECT STORAGE (50 min)             â”‚
â”‚  â”œâ”€ 3.1 Where Do Files Live? (The Wrong Answers)                â”‚
â”‚  â”œâ”€ 3.2 Object Storage Mental Model                             â”‚
â”‚  â”œâ”€ 3.3 MinIO: S3-Compatible Local Development                  â”‚
â”‚  â”œâ”€ 3.4 The Presigned URL Pattern                               â”‚
â”‚  â”œâ”€ 3.5 File Metadata in Your Database                          â”‚
â”‚  â”œâ”€ 3.6 Implementation: Upload & Download                       â”‚
â”‚  â””â”€ 3.7 Common Mistakes                                         â”‚
â”‚                                                                 â”‚
â”‚  PART 4: SEARCH & EXPORT (50 min)                               â”‚
â”‚  â”œâ”€ 4.1 The LIKE '%query%' Disaster                             â”‚
â”‚  â”œâ”€ 4.2 PostgreSQL Full-Text Search in SQLAlchemy               â”‚
â”‚  â”œâ”€ 4.3 The Search Endpoint                                     â”‚
â”‚  â”œâ”€ 4.4 When You'd Need a Dedicated Search Engine               â”‚
â”‚  â”œâ”€ 4.5 Export: The Request That Never Returns                   â”‚
â”‚  â”œâ”€ 4.6 The Async Export Pipeline                                â”‚
â”‚  â””â”€ 4.7 Common Mistakes                                         â”‚
â”‚                                                                 â”‚
â”‚  PART 5: ADMIN DASHBOARD & AGGREGATION (30 min)                 â”‚
â”‚  â”œâ”€ 5.1 The Dashboard That Killed the Database                  â”‚
â”‚  â”œâ”€ 5.2 Aggregation Queries with CTEs                           â”‚
â”‚  â”œâ”€ 5.3 Caching Dashboard Data                                  â”‚
â”‚  â””â”€ 5.4 The Dashboard Endpoint                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 1: THE INTEGRATION REALITY

## 1.1 The Demo That Fell Apart

**Set the scene. Every student will recognize this moment.**

> "You've built your capstone backend. CRUD works. Auth works. Real-time works. You demo it to a stakeholder. They say: *'Looks great. Can users get email notifications when tasks are assigned? Can they attach files? Can I search across all projects? Can we export data for compliance? And I need an admin dashboard with stats.'*"

**You say "sure" and start coding naively:**

```python
# âŒ The naive implementation of ALL FIVE features
# (This is what breaks in production)

from fastapi import APIRouter, UploadFile
import smtplib
import csv
import io

router = APIRouter()


# âŒ PROBLEM 1: Email blocks the response for 3-5 seconds
@router.post("/tasks/{task_id}/assign")
async def assign_task(task_id: uuid.UUID, assignee_id: uuid.UUID):
    task = await task_repo.assign(task_id, assignee_id)
    
    # Sending email RIGHT HERE in the handler
    user = await user_repo.get(assignee_id)
    smtp = smtplib.SMTP("smtp.gmail.com", 587)  # â† Blocks the event loop!
    smtp.starttls()
    smtp.login("you@gmail.com", "password123")   # â† Credentials hardcoded!
    smtp.sendmail(
        "you@gmail.com",
        user.email,
        f"Subject: New Task\n\nYou were assigned: {task.title}"
    )
    smtp.quit()
    
    return task  # User waited 4 seconds for this response


# âŒ PROBLEM 2: File stored as BLOB in PostgreSQL
@router.post("/tasks/{task_id}/attachments")
async def upload_attachment(task_id: uuid.UUID, file: UploadFile):
    contents = await file.read()  # â† Entire file in memory
    
    # Storing raw bytes in a PostgreSQL column
    await db.execute(
        "INSERT INTO attachments (task_id, data, filename) VALUES ($1, $2, $3)",
        task_id, contents, file.filename  # â† 50MB PDF in your database!
    )
    return {"status": "uploaded"}


# âŒ PROBLEM 3: LIKE search on 500,000 rows
@router.get("/search")
async def search_tasks(q: str):
    results = await db.fetch_all(
        "SELECT * FROM tasks WHERE title LIKE $1 OR description LIKE $1",
        f"%{q}%"  # â† Full table scan. Every. Single. Time.
    )
    return results


# âŒ PROBLEM 4: Export times out at 30 seconds
@router.get("/export/tasks")
async def export_tasks(org_id: uuid.UUID):
    tasks = await task_repo.get_all_for_org(org_id)  # â† 100,000 rows
    
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(["id", "title", "status", "assignee", "created_at"])
    for task in tasks:
        writer.writerow([task.id, task.title, task.status, ...])
    
    return Response(
        content=output.getvalue(),  # â† 50MB CSV built synchronously
        media_type="text/csv",      #    Gateway timeout after 30s
    )


# âŒ PROBLEM 5: Dashboard recomputes on every refresh
@router.get("/admin/dashboard")
async def dashboard(org_id: uuid.UUID):
    # 6 expensive queries, every single page load
    total_tasks = await db.fetch_val("SELECT COUNT(*) FROM tasks WHERE org_id=$1", org_id)
    completed = await db.fetch_val("SELECT COUNT(*) FROM tasks WHERE org_id=$1 AND status='done'", org_id)
    active_users = await db.fetch_val("SELECT COUNT(DISTINCT assignee_id) FROM tasks WHERE ...", org_id)
    # ... 3 more queries ...
    
    return {
        "total_tasks": total_tasks,
        "completed": completed,
        "active_users": active_users,
        # Each dashboard refresh = 6 DB round-trips
        # 50 admins refreshing = 300 queries/minute
    }
```

**Now ask the class:**

> "Five features, five disasters waiting to happen. Can you identify what breaks in each one â€” and *when* it breaks? It might work with 10 users in development. What happens at 1,000?"

**Let them discuss. Then summarize:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             WHAT BREAKS AND WHEN                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  EMAIL:   Response takes 4+ seconds. SMTP server down?         â”‚
â”‚           Your entire endpoint fails. User sees 500.            â”‚
â”‚           Blocks the async event loop (smtplib is sync).        â”‚
â”‚                                                                 â”‚
â”‚  FILES:   Database balloons from 500MB to 50GB. Backups take   â”‚
â”‚           hours. A single 100MB upload exhausts server RAM.     â”‚
â”‚           PostgreSQL was never designed for binary storage.      â”‚
â”‚                                                                 â”‚
â”‚  SEARCH:  0.01s with 100 rows. 4.7s with 500,000 rows.        â”‚
â”‚           LIKE '%word%' cannot use indexes. Full table scan.    â”‚
â”‚                                                                 â”‚
â”‚  EXPORT:  Works for 100 tasks. Times out at 10,000.            â”‚
â”‚           Gateway returns 504. User retries. Server overloads.  â”‚
â”‚                                                                 â”‚
â”‚  DASHBOARD: Works for 1 admin. 50 admins refreshing every      â”‚
â”‚           10 seconds = 300 queries/minute of pure aggregation.  â”‚
â”‚           Database CPU at 100%.                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.2 The Core Principle: Delegate, Don't Execute

**Every pattern in this lecture follows ONE principle:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚       YOUR BACKEND IS A COORDINATOR, NOT AN EXECUTOR.           â”‚
â”‚                                                                 â”‚
â”‚  âŒ Don't send emails          â†’ Delegate to an email service   â”‚
â”‚  âŒ Don't store files          â†’ Delegate to object storage     â”‚
â”‚  âŒ Don't scan every row       â†’ Delegate to a search index     â”‚
â”‚  âŒ Don't build exports inline â†’ Delegate to a background task  â”‚
â”‚  âŒ Don't recompute stats      â†’ Delegate to a cache layer      â”‚
â”‚                                                                 â”‚
â”‚  Your request handler's job:                                    â”‚
â”‚  1. Validate the request                                        â”‚
â”‚  2. Coordinate the right services                               â”‚
â”‚  3. Return a response FAST                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.3 The Company Headquarters Analogy

**This analogy frames the entire lecture.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            YOUR BACKEND = COMPANY HEADQUARTERS                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   YOUR BACKEND   â”‚                          â”‚
â”‚                    â”‚  (Headquarters)  â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                             â”‚                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚          â”‚         â”‚        â”‚        â”‚          â”‚               â”‚
â”‚          â–¼         â–¼        â–¼        â–¼          â–¼               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚     â”‚ COURIER â”‚â”‚WARE-  â”‚â”‚FILINGâ”‚â”‚REPORTS â”‚â”‚EXECUTIVEâ”‚          â”‚
â”‚     â”‚ SERVICE â”‚â”‚HOUSE  â”‚â”‚INDEX â”‚â”‚DEPT    â”‚â”‚ BOARD   â”‚          â”‚
â”‚     â”‚         â”‚â”‚       â”‚â”‚      â”‚â”‚        â”‚â”‚         â”‚          â”‚
â”‚     â”‚SendGrid â”‚â”‚  S3/  â”‚â”‚Full- â”‚â”‚Celery  â”‚â”‚ Redis   â”‚          â”‚
â”‚     â”‚ Mailgun â”‚â”‚ MinIO â”‚â”‚Text  â”‚â”‚Export  â”‚â”‚ Cached  â”‚          â”‚
â”‚     â”‚         â”‚â”‚       â”‚â”‚Searchâ”‚â”‚Tasks   â”‚â”‚ Stats   â”‚          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  HQ doesn't deliver mail    â†’ Hires FedEx (SendGrid)           â”‚
â”‚  HQ doesn't store boxes     â†’ Rents storage units (S3)         â”‚
â”‚  HQ doesn't search by hand  â†’ Maintains an index (tsvector)    â”‚
â”‚  HQ doesn't print reports   â†’ Sends to print shop (Celery)     â”‚
â”‚  HQ doesn't recount daily   â†’ Reads the dashboard (Redis)      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.4 Where Each Pattern Lives (Project Structure)

> "In Lecture 1 this week, we defined the project structure for larger FastAPI applications. Here's exactly where each integration pattern fits."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PROJECT STRUCTURE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  app/                                                           â”‚
â”‚  â”œâ”€â”€ integrations/            â† NEW: External service clients   â”‚
â”‚  â”‚   â”œâ”€â”€ __init__.py                                            â”‚
â”‚  â”‚   â”œâ”€â”€ email.py             â† Email service abstraction       â”‚
â”‚  â”‚   â””â”€â”€ storage.py           â† S3/MinIO service abstraction    â”‚
â”‚  â”‚                                                              â”‚
â”‚  â”œâ”€â”€ tasks/                   â† Celery tasks (from Week 11)     â”‚
â”‚  â”‚   â”œâ”€â”€ __init__.py                                            â”‚
â”‚  â”‚   â”œâ”€â”€ notifications.py     â† Email sending tasks             â”‚
â”‚  â”‚   â””â”€â”€ exports.py           â† Data export tasks               â”‚
â”‚  â”‚                                                              â”‚
â”‚  â”œâ”€â”€ routers/                                                   â”‚
â”‚  â”‚   â”œâ”€â”€ webhooks.py          â† Webhook receivers               â”‚
â”‚  â”‚   â”œâ”€â”€ files.py             â† Upload/download endpoints       â”‚
â”‚  â”‚   â”œâ”€â”€ search.py            â† Search endpoints                â”‚
â”‚  â”‚   â”œâ”€â”€ exports.py           â† Export request/status           â”‚
â”‚  â”‚   â””â”€â”€ admin.py             â† Dashboard endpoints             â”‚
â”‚  â”‚                                                              â”‚
â”‚  â”œâ”€â”€ repositories/                                              â”‚
â”‚  â”‚   â”œâ”€â”€ search.py            â† Full-text search queries        â”‚
â”‚  â”‚   â””â”€â”€ analytics.py         â† Aggregation queries             â”‚
â”‚  â”‚                                                              â”‚
â”‚  â”œâ”€â”€ models/                  â† SQLAlchemy (from Week 6)        â”‚
â”‚  â”‚   â”œâ”€â”€ file_attachment.py   â† File metadata model             â”‚
â”‚  â”‚   â”œâ”€â”€ notification_log.py  â† Email tracking model            â”‚
â”‚  â”‚   â””â”€â”€ export_job.py        â† Export status tracking          â”‚
â”‚  â”‚                                                              â”‚
â”‚  â””â”€â”€ schemas/                 â† Pydantic (from Week 3)          â”‚
â”‚      â”œâ”€â”€ files.py                                               â”‚
â”‚      â”œâ”€â”€ notifications.py                                       â”‚
â”‚      â”œâ”€â”€ exports.py                                             â”‚
â”‚      â””â”€â”€ analytics.py                                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The key insight:**

> "Notice the `integrations/` directory. This is where external service clients live, isolated behind abstractions. Your routes and tasks never call SendGrid or S3 directly â€” they call your abstraction. If you switch from SendGrid to Mailgun, you change ONE file."

---

# PART 2: EMAIL & NOTIFICATION INTEGRATION

## 2.1 The Synchronous Email Trap

**Recall the broken code from Part 1. Let's trace exactly why it fails:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THE SYNCHRONOUS EMAIL DISASTER                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  User clicks "Assign Task"                                      â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  POST /tasks/{id}/assign                         â”‚           â”‚
â”‚  â”‚                                                  â”‚           â”‚
â”‚  â”‚  1. Update DB (50ms)                   âœ… Fast   â”‚           â”‚
â”‚  â”‚  2. Connect to SMTP server (800ms)     ğŸ˜° Slow   â”‚           â”‚
â”‚  â”‚  3. TLS handshake (200ms)              ğŸ˜° Slow   â”‚           â”‚
â”‚  â”‚  4. Send email (1500ms)                ğŸ˜° Slow   â”‚           â”‚
â”‚  â”‚  5. Wait for confirmation (500ms)      ğŸ˜° Slow   â”‚           â”‚
â”‚  â”‚  6. Return response                    âœ… Done   â”‚           â”‚
â”‚  â”‚                                                  â”‚           â”‚
â”‚  â”‚  Total: ~3,050ms                                 â”‚           â”‚
â”‚  â”‚  Of which ~50ms was YOUR actual work             â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â”‚  But that's the HAPPY path. What if:                            â”‚
â”‚                                                                 â”‚
â”‚  â€¢ SMTP server is down?        â†’ 30s timeout â†’ 504 Gateway     â”‚
â”‚  â€¢ Email address is invalid?   â†’ Bounce after sending â†’ ???     â”‚
â”‚  â€¢ Rate limit from provider?   â†’ 429 error â†’ Your endpoint 500 â”‚
â”‚  â€¢ smtplib is synchronous?     â†’ Blocks your event loop!       â”‚
â”‚    (Remember Week 1, Lecture 3: blocking calls in async code    â”‚
â”‚     freeze ALL concurrent request handling)                     â”‚
â”‚                                                                 â”‚
â”‚  WORST: The task WAS assigned in the DB, but user sees an       â”‚
â”‚  error because the EMAIL failed. Data is inconsistent.          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ask the class:**

> "The task assignment succeeded. The email failed. Should the user see an error? Should we rollback the assignment? Is sending an email part of the core operation, or a side effect?"

Answer: **It's a side effect. The user's action is 'assign the task.' The email is a notification ABOUT that action. They are different concerns with different failure modes.**

---

## 2.2 The Architecture: Endpoint â†’ Task â†’ Provider

**The correct pattern separates the action from its side effects:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EMAIL NOTIFICATION ARCHITECTURE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  User Request                                                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   FastAPI       â”‚     â”‚   Celery     â”‚     â”‚   SendGrid    â”‚   â”‚
â”‚  â”‚   Handler       â”‚â”€â”€â”€â”€â–¶â”‚   Worker     â”‚â”€â”€â”€â”€â–¶â”‚   API         â”‚   â”‚
â”‚  â”‚                 â”‚     â”‚             â”‚     â”‚               â”‚   â”‚
â”‚  â”‚ 1. Assign task  â”‚     â”‚ 1. Build    â”‚     â”‚ 1. Accept     â”‚   â”‚
â”‚  â”‚ 2. Log to DB    â”‚     â”‚    email    â”‚     â”‚    email      â”‚   â”‚
â”‚  â”‚ 3. Queue task   â”‚     â”‚ 2. Call     â”‚     â”‚ 2. Deliver    â”‚   â”‚
â”‚  â”‚ 4. Return 200   â”‚     â”‚    provider â”‚     â”‚ 3. Report     â”‚   â”‚
â”‚  â”‚    (50ms)       â”‚     â”‚ 3. Update   â”‚     â”‚    status     â”‚   â”‚
â”‚  â”‚                 â”‚     â”‚    log      â”‚     â”‚               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                                             â”‚           â”‚
â”‚       â”‚ Fast response                   Webhook     â”‚           â”‚
â”‚       â”‚ to user                         callback    â”‚           â”‚
â”‚       â–¼                                             â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ User sees      â”‚                    â”‚ POST /webhooks/   â”‚    â”‚
â”‚  â”‚ "Task assigned"â”‚                    â”‚ sendgrid          â”‚    â”‚
â”‚  â”‚ immediately    â”‚                    â”‚                   â”‚    â”‚
â”‚  â”‚                â”‚                    â”‚ Update delivery   â”‚    â”‚
â”‚  â”‚                â”‚                    â”‚ status in DB      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  KEY: The user NEVER waits for the email.                       â”‚
â”‚  The email happens AFTER the response is sent.                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Map it to the analogy:**

> "You're at HQ. A client says 'assign this to John.' You update the ledger (DB), drop a letter in the outbox (Celery queue), and tell the client 'done.' The mail courier (SendGrid) picks up the outbox later and delivers. If the courier has a flat tire, that's their problem â€” the assignment is already recorded."

---

## 2.3 Building the Email Service Abstraction

**We use a Protocol (Week 1, Lecture 2: Python patterns) to define the contract. This lets us swap providers without touching business logic.**

```python
# app/integrations/email.py

from typing import Protocol
import httpx
from app.core.config import settings


class EmailService(Protocol):
    """Contract for any email provider.
    
    Why Protocol, not ABC?
    Structural subtyping â€” any class with this method signature works.
    No inheritance required. (Week 1, Lecture 2: Protocols)
    """

    async def send(
        self,
        to: str,
        subject: str,
        html_body: str,
        idempotency_key: str | None = None,
    ) -> str:
        """Send an email. Return the provider's message ID."""
        ...
```

**Now the SendGrid implementation. Notice: we're using httpx (Week 8), not a SendGrid-specific library. Any REST API is just HTTP calls.**

```python
class SendGridEmailService:
    """SendGrid implementation using httpx.
    
    Why httpx and not the sendgrid Python library?
    1. You already know httpx (Week 8).
    2. It keeps dependencies minimal.
    3. It proves the point: any REST API = HTTP calls.
    """
    
    def __init__(self, api_key: str, from_email: str, from_name: str):
        self.api_key = api_key
        self.from_email = from_email
        self.from_name = from_name
        self.base_url = "https://api.sendgrid.com/v3"
    
    async def send(
        self,
        to: str,
        subject: str,
        html_body: str,
        idempotency_key: str | None = None,
    ) -> str:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        # Idempotency key prevents duplicate sends on retry
        # (Week 11: idempotent tasks â€” same concept, different layer)
        if idempotency_key:
            headers["X-Idempotency-Key"] = idempotency_key
        
        payload = {
            "personalizations": [{"to": [{"email": to}]}],
            "from": {"email": self.from_email, "name": self.from_name},
            "subject": subject,
            "content": [{"type": "text/html", "value": html_body}],
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/mail/send",
                headers=headers,
                json=payload,
                timeout=10.0,
            )
            response.raise_for_status()
        
        # SendGrid returns message ID in header on 202 Accepted
        return response.headers.get("X-Message-Id", "unknown")
```

**Why the abstraction matters:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                WHY ABSTRACT THE PROVIDER?                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Today:     EmailService â”€â”€â–¶ SendGridEmailService               â”‚
â”‚  Tomorrow:  EmailService â”€â”€â–¶ MailgunEmailService                â”‚
â”‚  Testing:   EmailService â”€â”€â–¶ FakeEmailService (logs to list)    â”‚
â”‚                                                                 â”‚
â”‚  Your Celery tasks, your routes, your business logic â€”          â”‚
â”‚  NONE of them know which provider is behind EmailService.       â”‚
â”‚  They call send(). That's it.                                   â”‚
â”‚                                                                 â”‚
â”‚  # In tests:                                                    â”‚
â”‚  class FakeEmailService:                                        â”‚
â”‚      def __init__(self):                                        â”‚
â”‚          self.sent: list[dict] = []                              â”‚
â”‚                                                                 â”‚
â”‚      async def send(self, to, subject, html_body, **kw) -> str: â”‚
â”‚          self.sent.append({"to": to, "subject": subject})       â”‚
â”‚          return "fake-message-id"                                â”‚
â”‚                                                                 â”‚
â”‚  # Override in test via dependency_overrides (Week 4)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.4 Sending via Background Task

**The email goes into a Celery task (Week 11), not in the request handler.**

First, the notification log model â€” we track every email we send:

```python
# app/models/notification_log.py

class NotificationLog(Base):
    __tablename__ = "notification_logs"
    
    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)
    recipient_email: Mapped[str] = mapped_column(String(255))
    subject: Mapped[str] = mapped_column(String(500))
    
    # Track lifecycle
    status: Mapped[str] = mapped_column(
        String(20), default="queued"
    )  # queued â†’ sent â†’ delivered | bounced | failed
    
    provider_message_id: Mapped[str | None] = mapped_column(String(255))
    error_message: Mapped[str | None] = mapped_column(Text)
    
    # Tenant isolation (Week 13, Lecture 2)
    organization_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("organizations.id"))
    
    created_at: Mapped[datetime] = mapped_column(default=func.now())
    updated_at: Mapped[datetime] = mapped_column(default=func.now(), onupdate=func.now())
```

Now the Celery task:

```python
# app/tasks/notifications.py

from app.core.celery_app import celery_app
from app.integrations.email import SendGridEmailService
from app.core.config import settings
import httpx


@celery_app.task(
    bind=True,
    autoretry_for=(httpx.HTTPStatusError, httpx.ConnectError),
    retry_backoff=True,       # Exponential backoff (Week 11)
    retry_backoff_max=600,    # Cap at 10 minutes
    max_retries=5,
    acks_late=True,           # Only acknowledge AFTER success
)
def send_email_task(
    self,
    notification_log_id: str,
    to: str,
    subject: str,
    html_body: str,
) -> None:
    """Send email via provider. Runs in Celery worker (sync context).
    
    IMPORTANT: Celery tasks run synchronously.
    We use httpx SYNC client here, not async.
    (Week 11: Celery workers have their own process â€” no event loop)
    """
    import asyncio
    
    email_service = SendGridEmailService(
        api_key=settings.SENDGRID_API_KEY,
        from_email=settings.FROM_EMAIL,
        from_name=settings.FROM_NAME,
    )
    
    # Run the async send in a one-shot event loop
    # (This is one of the few places asyncio.run() inside non-async is correct)
    try:
        message_id = asyncio.run(
            email_service.send(
                to=to,
                subject=subject,
                html_body=html_body,
                idempotency_key=notification_log_id,  # Prevents duplicates on retry
            )
        )
        
        # Update log: sent
        update_notification_status(
            notification_log_id, "sent", provider_message_id=message_id
        )
        
    except httpx.HTTPStatusError as exc:
        # Update log: failed (will be retried by Celery)
        update_notification_status(
            notification_log_id, "failed", error=str(exc)
        )
        raise  # Re-raise so Celery retries
```

**And the endpoint that triggers it all:**

```python
# app/routers/tasks.py

@router.post("/{task_id}/assign", response_model=TaskResponse)
async def assign_task(
    task_id: uuid.UUID,
    body: AssignTaskRequest,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
):
    # 1. Core operation: assign the task
    task = await task_repo.assign(session, task_id, body.assignee_id)
    assignee = await user_repo.get(session, body.assignee_id)
    
    # 2. Log the notification intent
    log = NotificationLog(
        recipient_email=assignee.email,
        subject=f"You've been assigned: {task.title}",
        status="queued",
        organization_id=current_user.organization_id,
    )
    session.add(log)
    await session.commit()
    
    # 3. Queue the email (fire-and-forget from this handler's perspective)
    send_email_task.delay(
        notification_log_id=str(log.id),
        to=assignee.email,
        subject=log.subject,
        html_body=render_assignment_email(task, assignee, current_user),
    )
    
    # 4. Return immediately â€” user waited ~60ms, not 3 seconds
    return task
```

**The critical timeline comparison:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BEFORE vs AFTER                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  BEFORE (synchronous email):                                    â”‚
â”‚                                                                 â”‚
â”‚  Request â”€â”€[DB 50ms]â”€â”€[SMTP 3000ms]â”€â”€â–¶ Response                 â”‚
â”‚                                        Total: ~3050ms           â”‚
â”‚                                        If SMTP fails: 500 error â”‚
â”‚                                                                 â”‚
â”‚  AFTER (background task):                                       â”‚
â”‚                                                                 â”‚
â”‚  Request â”€â”€[DB 50ms]â”€â”€[Queue 5ms]â”€â”€â–¶ Response                   â”‚
â”‚                            â”‚          Total: ~55ms              â”‚
â”‚                            â”‚          Email failure: invisible  â”‚
â”‚                            â–¼            to user                 â”‚
â”‚                     [Celery Worker]                              â”‚
â”‚                     [SMTP 3000ms ]                              â”‚
â”‚                     (async, with retries)                       â”‚
â”‚                                                                 â”‚
â”‚  Response time: 55x faster                                      â”‚
â”‚  Reliability: retries handle transient failures                 â”‚
â”‚  Decoupling: email provider outage â‰  API outage                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.5 Receiving Delivery Status (Webhooks)

> "You sent the email. But did it arrive? SendGrid tells you via webhooks â€” HTTP callbacks to YOUR server. You learned the concept in Week 8. Now we implement the receiving side."

```python
# app/schemas/notifications.py

class SendGridWebhookEvent(BaseModel):
    """One event from SendGrid's Event Webhook.
    
    SendGrid POSTs a JSON array of these to your endpoint.
    Always validate with Pydantic â€” never trust external data.
    (Week 8, Lecture 3: external vs internal models)
    """
    email: str
    event: str  # "delivered", "bounce", "dropped", "open", "click"
    sg_message_id: str | None = None
    reason: str | None = None   # Bounce reason
    timestamp: int              # Unix timestamp
    
    model_config = ConfigDict(extra="ignore")  # Ignore fields we don't care about
```

```python
# app/routers/webhooks.py

from fastapi import APIRouter, Request, HTTPException, status
import hashlib
import hmac

router = APIRouter(prefix="/webhooks", tags=["webhooks"])


def verify_sendgrid_signature(request: Request, body: bytes) -> bool:
    """Verify the webhook actually came from SendGrid, not an attacker.
    
    Same principle as JWT signature verification (Week 9):
    The sender signs the payload with a shared secret.
    We verify the signature before trusting the data.
    """
    signature = request.headers.get("X-Twilio-Email-Event-Webhook-Signature", "")
    timestamp = request.headers.get("X-Twilio-Email-Event-Webhook-Timestamp", "")
    
    verification_key = settings.SENDGRID_WEBHOOK_VERIFICATION_KEY
    
    # Construct the signed payload
    payload = timestamp.encode() + body
    expected = hmac.new(
        verification_key.encode(), payload, hashlib.sha256
    ).hexdigest()
    
    return hmac.compare_digest(signature, expected)


@router.post("/sendgrid", status_code=status.HTTP_200_OK)
async def handle_sendgrid_webhook(
    request: Request,
    session: AsyncSession = Depends(get_session),
):
    body = await request.body()
    
    if not verify_sendgrid_signature(request, body):
        raise HTTPException(status_code=401, detail="Invalid signature")
    
    import json
    events = [SendGridWebhookEvent(**e) for e in json.loads(body)]
    
    for event in events:
        if event.event == "delivered":
            await notification_repo.update_status(
                session, event.sg_message_id, "delivered"
            )
        elif event.event in ("bounce", "dropped"):
            await notification_repo.update_status(
                session, event.sg_message_id, "bounced",
                error=event.reason,
            )
    
    await session.commit()
    
    # ALWAYS return 200 quickly.
    # If you return an error, SendGrid will retry â€” you'll get duplicates.
    return {"status": "processed", "count": len(events)}
```

**The full webhook lifecycle:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                WEBHOOK LIFECYCLE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  YOUR APP                    SENDGRID                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚      â”‚                                                          â”‚
â”‚      â”‚â”€â”€ POST /v3/mail/send â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                          â”‚
â”‚      â”‚                               â”‚                          â”‚
â”‚      â”‚â—€â”€â”€â”€â”€â”€â”€â”€ 202 Accepted â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                          â”‚
â”‚      â”‚   (message_id: "abc123")      â”‚                          â”‚
â”‚      â”‚                               â”‚                          â”‚
â”‚      â”‚                               â”‚â”€â”€ Delivers email â”€â”€â–¶ ğŸ“§  â”‚
â”‚      â”‚                               â”‚                          â”‚
â”‚      â”‚                               â”‚                          â”‚
â”‚      â”‚â—€â”€â”€ POST /webhooks/sendgrid â”€â”€â”€â”‚                          â”‚
â”‚      â”‚    [{                         â”‚                          â”‚
â”‚      â”‚      "event": "delivered",    â”‚                          â”‚
â”‚      â”‚      "sg_message_id": "abc",  â”‚                          â”‚
â”‚      â”‚      "timestamp": 170000000   â”‚                          â”‚
â”‚      â”‚    }]                         â”‚                          â”‚
â”‚      â”‚                               â”‚                          â”‚
â”‚      â”‚â”€â”€â”€ 200 OK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                          â”‚
â”‚      â”‚                               â”‚                          â”‚
â”‚      â”‚   (Update notification_logs:                             â”‚
â”‚      â”‚    status = "delivered")                                  â”‚
â”‚      â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.6 Common Mistakes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EMAIL INTEGRATION MISTAKES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 1: Sending email in the request handler             â”‚
â”‚     Fix: Always use a background task (Celery)                  â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 2: No idempotency key on retries                    â”‚
â”‚     Result: User gets 5 identical emails when Celery retries    â”‚
â”‚     Fix: Pass a unique key (notification_log_id) to provider    â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 3: Returning error from webhook endpoint            â”‚
â”‚     Result: Provider retries â†’ you process same event 10 times  â”‚
â”‚     Fix: Always return 200, handle errors internally            â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 4: Not verifying webhook signatures                 â”‚
â”‚     Result: Attacker can forge "delivered" events               â”‚
â”‚     Fix: HMAC verification on every webhook request             â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 5: Hardcoding provider in business logic            â”‚
â”‚     Result: Switching providers = rewriting half the codebase   â”‚
â”‚     Fix: Protocol abstraction, swap via dependency injection    â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 6: Not logging email attempts                       â”‚
â”‚     Result: "Did the email send?" â†’ Shrug.                      â”‚
â”‚     Fix: notification_logs table tracks every attempt + status  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 3: FILE HANDLING WITH OBJECT STORAGE

## 3.1 Where Do Files Live? (The Wrong Answers)

**Ask the class:**

> "A user wants to attach a PDF to their task. Where do you store the file? You have 3 seconds."

**The three wrong answers students always give:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THREE WRONG ANSWERS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âŒ ANSWER 1: "In a BYTEA column in PostgreSQL"                 â”‚
â”‚                                                                 â”‚
â”‚     Problems:                                                   â”‚
â”‚     â€¢ 10,000 tasks Ã— 5MB avg attachment = 50GB in your DB      â”‚
â”‚     â€¢ pg_dump backup takes hours instead of minutes             â”‚
â”‚     â€¢ Every query on the tasks table gets slower                â”‚
â”‚     â€¢ Database connection held open during entire download      â”‚
â”‚     â€¢ PostgreSQL max row size complications                     â”‚
â”‚                                                                 â”‚
â”‚  âŒ ANSWER 2: "On the server filesystem (/uploads/...)"         â”‚
â”‚                                                                 â”‚
â”‚     Problems:                                                   â”‚
â”‚     â€¢ Server gets replaced during deployment â†’ files gone       â”‚
â”‚     â€¢ Multiple servers? Files only on one of them               â”‚
â”‚     â€¢ Disk fills up â†’ entire server crashes                     â”‚
â”‚     â€¢ No CDN, no geographic distribution                        â”‚
â”‚     â€¢ Backup = copy entire filesystem (slow, fragile)           â”‚
â”‚                                                                 â”‚
â”‚  âŒ ANSWER 3: "Base64 encode and put in a JSON field"           â”‚
â”‚                                                                 â”‚
â”‚     Problems:                                                   â”‚
â”‚     â€¢ 33% size overhead (5MB file â†’ 6.7MB stored)              â”‚
â”‚     â€¢ Every API response with the file = massive JSON          â”‚
â”‚     â€¢ Parsing/serializing megabytes of Base64 = CPU burn       â”‚
â”‚     â€¢ All the PostgreSQL problems from Answer 1, but worse     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.2 Object Storage Mental Model

**The right answer: Object Storage (S3, MinIO, GCS, Azure Blob).**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OBJECT STORAGE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Think of it as an infinite, organized filing cabinet           â”‚
â”‚  that lives OUTSIDE your application:                           â”‚
â”‚                                                                 â”‚
â”‚  CONCEPTS:                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  Bucket   = A named container (like a filing cabinet)           â”‚
â”‚             e.g., "my-saas-attachments"                         â”‚
â”‚                                                                 â”‚
â”‚  Object   = A file stored in a bucket                           â”‚
â”‚             Has a KEY (path) and the DATA (bytes)               â”‚
â”‚             e.g., key = "orgs/abc/tasks/123/report.pdf"         â”‚
â”‚                                                                 â”‚
â”‚  Key      = The "path" to an object (like a file path)          â”‚
â”‚             NOT a real filesystem â€” just a string               â”‚
â”‚             Slashes are convention, not directories              â”‚
â”‚                                                                 â”‚
â”‚  Metadata = Info about the object (content-type, size, etc.)    â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  WHY IT'S BETTER:                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚  â€¢ Designed for binary data (TB-scale, not MB-scale)            â”‚
â”‚  â€¢ Independent scaling (storage grows without affecting DB)     â”‚
â”‚  â€¢ Built-in redundancy (99.999999999% durability for S3)        â”‚
â”‚  â€¢ CDN integration (serve files from edge locations)            â”‚
â”‚  â€¢ Access control via signed URLs (no auth on your server)      â”‚
â”‚  â€¢ Pay per GB stored + GB transferred (cents, not dollars)      â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  YOUR DATABASE STORES:     OBJECT STORAGE STORES:               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚  File metadata             File bytes                           â”‚
â”‚  â€¢ id (UUID)               â€¢ The actual PDF, image, etc.        â”‚
â”‚  â€¢ filename                                                     â”‚
â”‚  â€¢ content_type                                                 â”‚
â”‚  â€¢ size_bytes                                                   â”‚
â”‚  â€¢ storage_key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Used to locate the object      â”‚
â”‚  â€¢ uploaded_by                                                  â”‚
â”‚  â€¢ created_at                                                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.3 MinIO: S3-Compatible Local Development

> "You know Docker from Week 5 (running PostgreSQL) and Week 10 (running Redis). MinIO is another container â€” an S3-compatible object store you run locally."

**Add to your Docker Compose:**

```yaml
# docker-compose.yml (extend your existing file)
services:
  # ... postgres, redis, celery worker from previous weeks ...
  
  minio:
    image: minio/minio
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Web console (like pgAdmin but for files)
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

volumes:
  minio_data:
```

**The beauty: boto3 works identically with MinIO and real AWS S3.**

```python
# app/integrations/storage.py

import boto3
from botocore.config import Config
from app.core.config import settings


def get_s3_client():
    """Create S3 client. Points to MinIO locally, AWS S3 in production.
    
    The ONLY difference between local and production:
    - endpoint_url: http://localhost:9000 vs None (uses AWS default)
    - credentials: minioadmin vs real AWS IAM credentials
    
    Your application code doesn't change AT ALL.
    """
    return boto3.client(
        "s3",
        endpoint_url=settings.S3_ENDPOINT_URL,       # MinIO locally, None for AWS
        aws_access_key_id=settings.S3_ACCESS_KEY,
        aws_secret_access_key=settings.S3_SECRET_KEY,
        region_name=settings.S3_REGION,
        config=Config(signature_version="s3v4"),
    )
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LOCAL vs PRODUCTION â€” SAME CODE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  LOCAL (.env):                                                  â”‚
â”‚  S3_ENDPOINT_URL=http://localhost:9000                           â”‚
â”‚  S3_ACCESS_KEY=minioadmin                                       â”‚
â”‚  S3_SECRET_KEY=minioadmin                                       â”‚
â”‚  S3_BUCKET=my-saas-dev                                          â”‚
â”‚                                                                 â”‚
â”‚  PRODUCTION (.env):                                             â”‚
â”‚  S3_ENDPOINT_URL=              â† Empty: uses AWS default        â”‚
â”‚  S3_ACCESS_KEY=AKIA...         â† Real IAM credentials           â”‚
â”‚  S3_SECRET_KEY=wJal...                                          â”‚
â”‚  S3_BUCKET=my-saas-prod                                         â”‚
â”‚                                                                 â”‚
â”‚  Your Python code: IDENTICAL in both environments.              â”‚
â”‚  Config via pydantic-settings (Week 15, Lecture 2).             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.4 The Presigned URL Pattern

**This is the most important concept in this section. Read carefully.**

> "What if I told you the file bytes should NEVER pass through your FastAPI server? The client uploads DIRECTLY to S3. Your server just issues a permission slip."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TWO UPLOAD APPROACHES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âŒ THROUGH YOUR SERVER (simple, but doesn't scale):            â”‚
â”‚                                                                 â”‚
â”‚  Client â”€â”€[50MB file]â”€â”€â–¶ FastAPI â”€â”€[50MB file]â”€â”€â–¶ S3            â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â”œâ”€ 50MB in server RAM                 â”‚
â”‚                           â”œâ”€ Connection held for upload duration â”‚
â”‚                           â”œâ”€ Server bandwidth = bottleneck      â”‚
â”‚                           â””â”€ 10 simultaneous uploads = crash    â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  âœ… PRESIGNED URL (production pattern):                         â”‚
â”‚                                                                 â”‚
â”‚  Client â”€â”€[request]â”€â”€â–¶ FastAPI â”€â”€[generate URL]â”€â”€â–¶ Client       â”‚
â”‚                         (5ms)                        â”‚          â”‚
â”‚                                                      â”‚          â”‚
â”‚            Client â”€â”€[50MB file]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ S3 directly  â”‚
â”‚                     (your server never sees the bytes)          â”‚
â”‚                                                                 â”‚
â”‚  Server work: Generate a signed URL. That's it.                 â”‚
â”‚  No RAM pressure. No bandwidth bottleneck. Scales infinitely.   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What IS a presigned URL?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PRESIGNED URL ANATOMY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  A presigned URL is a normal S3 URL with a cryptographic        â”‚
â”‚  signature appended. It says:                                   â”‚
â”‚                                                                 â”‚
â”‚  "The holder of this URL is allowed to PUT a file               â”‚
â”‚   to this exact key, with this exact content type,              â”‚
â”‚   for the next 15 minutes."                                     â”‚
â”‚                                                                 â”‚
â”‚  https://my-bucket.s3.amazonaws.com/orgs/abc/file.pdf           â”‚
â”‚    ?X-Amz-Algorithm=AWS4-HMAC-SHA256                            â”‚
â”‚    &X-Amz-Credential=AKIA.../20260214/us-east-1/s3/...         â”‚
â”‚    &X-Amz-Date=20260214T100000Z                                 â”‚
â”‚    &X-Amz-Expires=900             â† 15 minutes                  â”‚
â”‚    &X-Amz-Signature=a8b3c...     â† Cryptographic signature     â”‚
â”‚                                                                 â”‚
â”‚  PROPERTIES:                                                    â”‚
â”‚  â€¢ Time-limited (expires after N seconds)                       â”‚
â”‚  â€¢ Operation-specific (PUT only, or GET only)                   â”‚
â”‚  â€¢ Key-specific (can only write to the specified path)          â”‚
â”‚  â€¢ Signed with YOUR credentials (but doesn't expose them)       â”‚
â”‚  â€¢ Generated LOCALLY â€” no network call to AWS!                  â”‚
â”‚                                                                 â”‚
â”‚  ANALOGY: It's like a JWT (Week 9) for file operations.         â”‚
â”‚  Signed, time-limited, scoped to a specific action.             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.5 File Metadata in Your Database

**Your database tracks files. S3 stores files. They're linked by the storage key.**

```python
# app/models/file_attachment.py

class FileAttachment(Base):
    __tablename__ = "file_attachments"
    
    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)
    
    # What the user sees
    original_filename: Mapped[str] = mapped_column(String(255))
    content_type: Mapped[str] = mapped_column(String(100))
    size_bytes: Mapped[int] = mapped_column(BigInteger)
    
    # Where it lives in S3 â€” THE CRITICAL LINK
    storage_key: Mapped[str] = mapped_column(String(500), unique=True)
    # e.g., "orgs/abc123/tasks/def456/2026-02-14_report.pdf"
    
    # Upload lifecycle
    upload_status: Mapped[str] = mapped_column(
        String(20), default="pending"
    )  # pending â†’ confirmed â†’ deleted
    
    # Ownership and tenant isolation (Week 13, Lecture 2)
    uploaded_by: Mapped[uuid.UUID] = mapped_column(ForeignKey("users.id"))
    organization_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("organizations.id"))
    
    # Polymorphic attachment: which entity is this attached to?
    attached_to_type: Mapped[str] = mapped_column(String(50))   # "task", "comment"
    attached_to_id: Mapped[uuid.UUID] = mapped_column()
    
    created_at: Mapped[datetime] = mapped_column(default=func.now())
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DB vs S3 SEPARATION                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   PostgreSQL (file_attachments table)                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚ id         â”‚ filename  â”‚ storage_key                  â”‚     â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚   â”‚ aaa-111    â”‚ spec.pdf  â”‚ orgs/xyz/tasks/1/spec.pdf   â”‚â”€â”€â”  â”‚
â”‚   â”‚ bbb-222    â”‚ logo.png  â”‚ orgs/xyz/tasks/2/logo.png   â”‚â”€â”€â”¼â” â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚ â”‚
â”‚                                                               â”‚â”‚ â”‚
â”‚   S3 Bucket (my-saas-attachments)                            â”‚â”‚ â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚â”‚ â”‚
â”‚   â”‚ orgs/xyz/tasks/1/spec.pdf  [4.2MB] â—€â”˜â”‚                   â”‚ â”‚
â”‚   â”‚ orgs/xyz/tasks/2/logo.png  [150KB] â—€â”€â”€â”˜                  â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚ â”‚
â”‚                                                               â”‚ â”‚
â”‚   storage_key is the bridge between your DB and S3.           â”‚ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why `upload_status`?**

> "There's a gap between 'we generated the presigned URL' and 'the client actually uploaded the file.' The `pending` status means we issued a URL but haven't confirmed the upload arrived. This prevents phantom records in your DB for uploads that never completed."

---

## 3.6 Implementation: Upload & Download

**The storage service abstraction:**

```python
# app/integrations/storage.py

class StorageService:
    """Wraps S3/MinIO operations behind a clean interface."""
    
    def __init__(self, s3_client, bucket_name: str):
        self.s3 = s3_client
        self.bucket = bucket_name
    
    def generate_upload_url(
        self,
        key: str,
        content_type: str,
        expires_in: int = 900,  # 15 minutes
    ) -> str:
        """Generate a presigned URL for uploading.
        
        This is a LOCAL computation â€” no network call.
        It signs the request parameters with your credentials.
        Safe to call from an async handler without blocking.
        """
        return self.s3.generate_presigned_url(
            ClientMethod="put_object",
            Params={
                "Bucket": self.bucket,
                "Key": key,
                "ContentType": content_type,
            },
            ExpiresIn=expires_in,
        )
    
    def generate_download_url(
        self,
        key: str,
        expires_in: int = 3600,  # 1 hour
        filename: str | None = None,
    ) -> str:
        """Generate a presigned URL for downloading."""
        params = {"Bucket": self.bucket, "Key": key}
        
        if filename:
            # Forces browser to download with original filename
            params["ResponseContentDisposition"] = f'attachment; filename="{filename}"'
        
        return self.s3.generate_presigned_url(
            ClientMethod="get_object",
            Params=params,
            ExpiresIn=expires_in,
        )
    
    def check_object_exists(self, key: str) -> bool:
        """Verify an object was actually uploaded."""
        try:
            self.s3.head_object(Bucket=self.bucket, Key=key)
            return True
        except self.s3.exceptions.ClientError:
            return False
```

**The upload endpoint:**

```python
# app/routers/files.py

router = APIRouter(prefix="/files", tags=["files"])

# Allowed types and size limits â€” your security boundary
ALLOWED_CONTENT_TYPES = {
    "application/pdf",
    "image/png",
    "image/jpeg",
    "text/csv",
}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB


class UploadRequest(BaseModel):
    filename: str = Field(min_length=1, max_length=255)
    content_type: str
    size_bytes: int = Field(gt=0)
    attached_to_type: str = Field(pattern=r"^(task|comment|project)$")
    attached_to_id: uuid.UUID


class UploadResponse(BaseModel):
    file_id: uuid.UUID
    upload_url: str
    expires_in: int


@router.post("/upload-url", response_model=UploadResponse, status_code=201)
async def request_upload_url(
    body: UploadRequest,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
    storage: StorageService = Depends(get_storage_service),
):
    """Step 1: Client requests a presigned upload URL.
    
    This does NOT upload the file. It gives the client
    a time-limited, signed URL to upload DIRECTLY to S3.
    """
    # Validate content type
    if body.content_type not in ALLOWED_CONTENT_TYPES:
        raise HTTPException(
            status_code=422,
            detail=f"Content type '{body.content_type}' not allowed. "
                   f"Allowed: {ALLOWED_CONTENT_TYPES}"
        )
    
    # Validate size
    if body.size_bytes > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=422,
            detail=f"File too large. Maximum: {MAX_FILE_SIZE // 1024 // 1024}MB"
        )
    
    # Build the storage key â€” tenant-isolated path
    storage_key = (
        f"orgs/{current_user.organization_id}/"
        f"{body.attached_to_type}s/{body.attached_to_id}/"
        f"{uuid.uuid4().hex[:8]}_{body.filename}"
    )
    
    # Create DB record (status: pending)
    file_record = FileAttachment(
        original_filename=body.filename,
        content_type=body.content_type,
        size_bytes=body.size_bytes,
        storage_key=storage_key,
        upload_status="pending",
        uploaded_by=current_user.id,
        organization_id=current_user.organization_id,
        attached_to_type=body.attached_to_type,
        attached_to_id=body.attached_to_id,
    )
    session.add(file_record)
    await session.commit()
    
    # Generate presigned URL (local computation, fast)
    expires_in = 900
    upload_url = storage.generate_upload_url(
        key=storage_key,
        content_type=body.content_type,
        expires_in=expires_in,
    )
    
    return UploadResponse(
        file_id=file_record.id,
        upload_url=upload_url,
        expires_in=expires_in,
    )
```

**The confirmation endpoint:**

```python
@router.post("/{file_id}/confirm", response_model=FileResponse)
async def confirm_upload(
    file_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
    storage: StorageService = Depends(get_storage_service),
):
    """Step 3: Client confirms the upload completed.
    
    We verify the object actually exists in S3 before
    marking the record as confirmed. Trust but verify.
    """
    file_record = await file_repo.get_by_id_and_org(
        session, file_id, current_user.organization_id
    )
    if not file_record:
        raise HTTPException(status_code=404, detail="File not found")
    
    if file_record.upload_status != "pending":
        raise HTTPException(status_code=409, detail="File already confirmed")
    
    # Verify the file actually made it to S3
    if not storage.check_object_exists(file_record.storage_key):
        raise HTTPException(
            status_code=400,
            detail="File not found in storage. Upload may have failed."
        )
    
    file_record.upload_status = "confirmed"
    await session.commit()
    
    return file_record
```

**The download endpoint:**

```python
@router.get("/{file_id}/download")
async def get_download_url(
    file_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
    storage: StorageService = Depends(get_storage_service),
):
    """Generate a time-limited download URL.
    
    Client receives the URL, then fetches the file DIRECTLY
    from S3. Our server never touches the file bytes.
    """
    file_record = await file_repo.get_by_id_and_org(
        session, file_id, current_user.organization_id
    )
    if not file_record or file_record.upload_status != "confirmed":
        raise HTTPException(status_code=404, detail="File not found")
    
    download_url = storage.generate_download_url(
        key=file_record.storage_key,
        filename=file_record.original_filename,
    )
    
    return {"download_url": download_url, "expires_in": 3600}
```

**The complete upload-download flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FULL FILE LIFECYCLE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   CLIENT                 YOUR API              S3 / MINIO       â”‚
â”‚   â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚  UPLOAD:                    â”‚                      â”‚            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚   1. â”‚â”€â”€ POST /files/       â”‚                      â”‚            â”‚
â”‚      â”‚   upload-url â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                      â”‚            â”‚
â”‚      â”‚   {filename,         â”‚ Validate             â”‚            â”‚
â”‚      â”‚    content_type,     â”‚ Create DB record     â”‚            â”‚
â”‚      â”‚    size_bytes}       â”‚ Generate signed URL   â”‚            â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚      â”‚â—€â”€â”€ 201 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚            â”‚
â”‚      â”‚   {file_id,          â”‚                      â”‚            â”‚
â”‚      â”‚    upload_url}       â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚   2. â”‚â”€â”€ PUT upload_url â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚            â”‚
â”‚      â”‚   [file bytes]       â”‚                    â”‚ Store file  â”‚
â”‚      â”‚                      â”‚                    â”‚             â”‚
â”‚      â”‚â—€â”€â”€ 200 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚   3. â”‚â”€â”€ POST /files/       â”‚                      â”‚            â”‚
â”‚      â”‚   {file_id}/confirm â–¶â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚â”€â”€ HEAD object â”€â”€â”€â”€â”€â”€â–¶â”‚            â”‚
â”‚      â”‚                      â”‚â—€â”€â”€ 200 (exists) â”€â”€â”€â”€â”‚            â”‚
â”‚      â”‚                      â”‚ Mark "confirmed"     â”‚            â”‚
â”‚      â”‚â—€â”€â”€ 200 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚  DOWNLOAD:                  â”‚                      â”‚            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚   4. â”‚â”€â”€ GET /files/        â”‚                      â”‚            â”‚
â”‚      â”‚   {file_id}/downloadâ–¶â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚ Check permissions    â”‚            â”‚
â”‚      â”‚                      â”‚ Generate signed URL   â”‚            â”‚
â”‚      â”‚â—€â”€â”€ 200 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                      â”‚            â”‚
â”‚      â”‚   {download_url}     â”‚                      â”‚            â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚   5. â”‚â”€â”€ GET download_url â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚            â”‚
â”‚      â”‚                      â”‚                    â”‚ Serve file  â”‚
â”‚      â”‚â—€â”€â”€ 200 [file bytes] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
â”‚      â”‚                      â”‚                      â”‚            â”‚
â”‚                                                                 â”‚
â”‚  Steps 2 and 5: Your server is NOT involved.                    â”‚
â”‚  The heavy lifting (file transfer) goes directly to/from S3.    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.7 Common Mistakes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               FILE HANDLING MISTAKES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 1: Storing file bytes in PostgreSQL                 â”‚
â”‚     Fix: Use object storage. DB stores metadata only.           â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 2: Not validating content type                      â”‚
â”‚     Result: Users upload .exe files disguised as .pdf           â”‚
â”‚     Fix: Allowlist of content types, validate BEFORE giving URL â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 3: Predictable storage keys                         â”‚
â”‚     "attachments/1.pdf", "attachments/2.pdf"                    â”‚
â”‚     Result: Anyone can guess and enumerate files                â”‚
â”‚     Fix: Include random prefix (uuid hex) in key               â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 4: No confirmation step                             â”‚
â”‚     Result: DB has records for uploads that never completed     â”‚
â”‚     Fix: pending â†’ confirmed lifecycle with HEAD check          â”‚
â”‚     ALSO: Periodic cleanup job (Celery Beat, Week 11)           â”‚
â”‚           to delete stale "pending" records after 24h           â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 5: Presigned URLs with no expiration limit          â”‚
â”‚     Fix: Upload URLs: 15 min max. Download: 1 hour max.        â”‚
â”‚     A leaked URL becomes useless once expired.                  â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE 6: Not isolating storage keys by tenant             â”‚
â”‚     Result: Org A could potentially access Org B's files        â”‚
â”‚     Fix: Key structure includes org_id:                         â”‚
â”‚          "orgs/{org_id}/tasks/{task_id}/{filename}"             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 4: SEARCH & EXPORT

## 4.1 The LIKE '%query%' Disaster

**Start with a demonstration of the scale problem:**

```sql
-- Your table has 500,000 tasks across all tenants.
-- User searches for "quarterly report"

-- âŒ The naive query:
SELECT * FROM tasks
WHERE organization_id = 'abc-123'
  AND (title LIKE '%quarterly report%' 
       OR description LIKE '%quarterly report%');
```

**Run EXPLAIN ANALYZE (Week 7):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LIKE '%...%' EXPLAIN RESULTS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Seq Scan on tasks  (cost=0.00..28541.00 rows=250 width=384)   â”‚
â”‚    Filter: ((title ~~ '%quarterly report%') OR                  â”‚
â”‚             (description ~~ '%quarterly report%'))              â”‚
â”‚    Rows Removed by Filter: 499750                               â”‚
â”‚  Planning Time: 0.15 ms                                         â”‚
â”‚  Execution Time: 4,723.41 ms    â† Almost 5 seconds!            â”‚
â”‚                                                                 â”‚
â”‚  WHY: Leading wildcard (%) prevents index use.                  â”‚
â”‚  PostgreSQL must read EVERY ROW and check the text.             â”‚
â”‚  This is O(n) â€” gets worse linearly with table size.            â”‚
â”‚                                                                 â”‚
â”‚  At 100 rows:     0.01s  â† "Works fine in development"         â”‚
â”‚  At 10,000 rows:  0.09s  â† "Still fast enough"                 â”‚
â”‚  At 100,000 rows: 0.94s  â† "Getting slow..."                   â”‚
â”‚  At 500,000 rows: 4.72s  â† "Users are leaving"                 â”‚
â”‚                                                                 â”‚
â”‚  Remember Week 7: "EXPLAIN before you ship."                    â”‚
â”‚  LIKE '%...%' is the most common query performance trap.        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.2 PostgreSQL Full-Text Search in SQLAlchemy

> "In Week 5, Lecture 3, you learned about tsvector and tsquery in raw SQL. In Week 7, you learned about GIN indexes. Now we bring them together in SQLAlchemy for production use."

**The mental model: full-text search is an inverted index for text.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LIKE vs FULL-TEXT SEARCH                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  LIKE '%quarterly report%':                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚  Read every row â†’ check if text contains substring              â”‚
â”‚  Like reading every page of every book to find a phrase.        â”‚
â”‚                                                                 â”‚
â”‚  FULL-TEXT SEARCH:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  Pre-built index maps words â†’ rows that contain them            â”‚
â”‚  Like using the index at the back of a textbook.                â”‚
â”‚                                                                 â”‚
â”‚  tsvector:  Preprocessed, indexed representation of text        â”‚
â”‚             "Quarterly Budget Report" â†’                         â”‚
â”‚             'budget':2 'quarter':1 'report':3                   â”‚
â”‚             (stemmed: "quarterly" â†’ "quarter")                  â”‚
â”‚                                                                 â”‚
â”‚  tsquery:   Your search query in the same format                â”‚
â”‚             "quarterly report" â†’ 'quarter' & 'report'           â”‚
â”‚                                                                 â”‚
â”‚  GIN Index: Pre-computed lookup table                           â”‚
â”‚             'budget'  â†’ [row 45, row 1203, row 8891]           â”‚
â”‚             'quarter' â†’ [row 45, row 302, row 7721]            â”‚
â”‚             'report'  â†’ [row 45, row 1203, row 5502]           â”‚
â”‚                                                                 â”‚
â”‚  Search: 'quarter' & 'report' â†’ intersect sets â†’ row 45        â”‚
â”‚  Time: O(log n) â€” barely changes with table size                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Adding full-text search to your SQLAlchemy model:**

```python
# app/models/task.py â€” Add search_vector to existing model

from sqlalchemy import Index, String, Text, func
from sqlalchemy.dialects.postgresql import TSVECTOR
from sqlalchemy.schema import Computed


class Task(Base):
    __tablename__ = "tasks"
    
    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)
    title: Mapped[str] = mapped_column(String(255))
    description: Mapped[str | None] = mapped_column(Text)
    organization_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("organizations.id"))
    # ... other columns from your existing model ...
    
    # â”€â”€ NEW: Generated tsvector column â”€â”€
    # PostgreSQL computes this automatically when title/description change.
    # You never set it manually. It's DERIVED data.
    search_vector: Mapped[str | None] = mapped_column(
        TSVECTOR,
        Computed(
            "to_tsvector('english', coalesce(title, '') || ' ' || coalesce(description, ''))",
            persisted=True,  # Stored on disk, not recomputed per query
        ),
    )
    
    __table_args__ = (
        # GIN index on the tsvector column (Week 7: index types)
        # Without this index, full-text search still does a sequential scan!
        Index(
            "ix_tasks_search_vector",
            "search_vector",
            postgresql_using="gin",
        ),
        # Keep your existing indexes...
    )
```

**The Alembic migration for this (Week 6):**

```python
# After running: alembic revision --autogenerate -m "add full-text search to tasks"
# Review the generated migration carefully:

def upgrade():
    op.add_column(
        "tasks",
        sa.Column(
            "search_vector",
            TSVECTOR,
            sa.Computed(
                "to_tsvector('english', coalesce(title, '') || ' ' || coalesce(description, ''))",
                persisted=True,
            ),
        ),
    )
    op.create_index(
        "ix_tasks_search_vector",
        "tasks",
        ["search_vector"],
        postgresql_using="gin",
    )

def downgrade():
    op.drop_index("ix_tasks_search_vector")
    op.drop_column("tasks", "search_vector")
```

**The search repository:**

```python
# app/repositories/search.py

from sqlalchemy import select, func, desc
from sqlalchemy.ext.asyncio import AsyncSession


async def search_tasks(
    session: AsyncSession,
    query: str,
    organization_id: uuid.UUID,
    limit: int = 20,
    offset: int = 0,
) -> tuple[list[Task], int]:
    """Full-text search across task titles and descriptions.
    
    plainto_tsquery: Converts plain text to tsquery.
      "quarterly report" â†’ 'quarter' & 'report'
      Handles stemming, stop words, and spaces automatically.
      Much safer than raw tsquery (no syntax errors from user input).
    
    ts_rank: Scores how well each row matches the query.
      Higher rank = more relevant. Used for ORDER BY.
    """
    search_query = func.plainto_tsquery("english", query)
    
    # Count total matches (for pagination metadata)
    count_stmt = (
        select(func.count())
        .select_from(Task)
        .where(Task.organization_id == organization_id)
        .where(Task.search_vector.match(query))
    )
    total = (await session.execute(count_stmt)).scalar() or 0
    
    # Fetch ranked results
    results_stmt = (
        select(Task)
        .where(Task.organization_id == organization_id)
        .where(Task.search_vector.match(query))
        .order_by(
            desc(func.ts_rank(Task.search_vector, search_query))
        )
        .limit(limit)
        .offset(offset)
    )
    
    results = (await session.execute(results_stmt)).scalars().all()
    return list(results), total
```

**Verify with EXPLAIN (always):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FULL-TEXT SEARCH EXPLAIN RESULTS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Bitmap Heap Scan on tasks                                      â”‚
â”‚    Recheck Cond: (search_vector @@ '''quarter'' & ''report'''   â”‚
â”‚    â†’ Bitmap Index Scan on ix_tasks_search_vector                â”‚
â”‚        Index Cond: (search_vector @@ '''quarter'' & ''report''' â”‚
â”‚  Planning Time: 0.28 ms                                         â”‚
â”‚  Execution Time: 2.14 ms   â† From 4,723ms to 2ms!             â”‚
â”‚                                                                 â”‚
â”‚  That's 2,200x faster.                                          â”‚
â”‚  And it STAYS fast as the table grows.                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.3 The Search Endpoint

```python
# app/routers/search.py

router = APIRouter(prefix="/search", tags=["search"])


class SearchResultItem(BaseModel):
    id: uuid.UUID
    title: str
    description: str | None
    status: str
    # Include a text snippet with highlighted matches
    model_config = ConfigDict(from_attributes=True)


class SearchResponse(BaseModel):
    results: list[SearchResultItem]
    total: int
    query: str


@router.get("", response_model=SearchResponse)
async def search(
    q: str = Query(min_length=1, max_length=200, description="Search query"),
    limit: int = Query(default=20, ge=1, le=100),
    offset: int = Query(default=0, ge=0),
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
):
    """Search tasks within the user's organization.
    
    Uses PostgreSQL full-text search with GIN index.
    Results ranked by relevance.
    Automatically scoped to user's tenant (Week 13, Lecture 2).
    """
    results, total = await search_tasks(
        session=session,
        query=q,
        organization_id=current_user.organization_id,
        limit=limit,
        offset=offset,
    )
    
    return SearchResponse(results=results, total=total, query=q)
```

---

## 4.4 When You'd Need a Dedicated Search Engine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SEARCH TECHNOLOGY DECISION FRAMEWORK                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                    START HERE                                    â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â–¼                                        â”‚
â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚             â”‚ Do you need search â”‚                               â”‚
â”‚             â”‚ across 1-2 tables? â”‚                               â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                â”‚           â”‚                                    â”‚
â”‚               YES          NO                                   â”‚
â”‚                â”‚           â”‚                                    â”‚
â”‚                â–¼           â–¼                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚    â”‚ < 10M rows?  â”‚  â”‚ Multiple data        â”‚                    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ sources, complex     â”‚                    â”‚
â”‚       â”‚       â”‚      â”‚ faceting, fuzzy,     â”‚                    â”‚
â”‚      YES      NO     â”‚ autocomplete,        â”‚                    â”‚
â”‚       â”‚       â”‚      â”‚ geo-spatial?         â”‚                    â”‚
â”‚       â–¼       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                 â”‚                               â”‚
â”‚  â”‚ PG Full- â”‚ â”‚                 â–¼                               â”‚
â”‚  â”‚ Text is  â”‚ â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ ENOUGH   â”‚ â”‚       â”‚ Elasticsearch/  â”‚                       â”‚
â”‚  â”‚ âœ…       â”‚ â”‚       â”‚ Meilisearch/    â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚       â”‚ Typesense       â”‚                       â”‚
â”‚               â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚               â–¼                                                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚      â”‚ PG Full-Text â”‚                                           â”‚
â”‚      â”‚ + Partitionedâ”‚                                           â”‚
â”‚      â”‚ tables       â”‚                                           â”‚
â”‚      â”‚ (still works â”‚                                           â”‚
â”‚      â”‚  for most    â”‚                                           â”‚
â”‚      â”‚  SaaS apps)  â”‚                                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚                                                                 â”‚
â”‚  RULE OF THUMB: PostgreSQL full-text search handles 90% of      â”‚
â”‚  SaaS search needs. Don't add Elasticsearch until PG can't      â”‚
â”‚  keep up. More infrastructure = more things to break.           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.5 Export: The Request That Never Returns

**Show the failure:**

```python
# âŒ This WILL time out for any non-trivial data set

@router.get("/export/tasks")
async def export_tasks(
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
):
    # Step 1: Fetch all tasks (might be 100,000 rows)
    tasks = await task_repo.get_all_for_org(
        session, current_user.organization_id
    )  # 3 seconds for 100k rows
    
    # Step 2: Build CSV in memory
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(["ID", "Title", "Status", "Assignee", "Created"])
    for task in tasks:
        writer.writerow([...])
    # 5 seconds for 100k rows, 200MB string in memory
    
    # Step 3: Return the response
    return Response(
        content=output.getvalue(),  # 200MB response body
        media_type="text/csv",
    )
    # Total: 8+ seconds
    # Load balancer / reverse proxy timeout: 30 seconds
    # With 500k rows: 40+ seconds â†’ 504 GATEWAY TIMEOUT
```

**Ask the class:**

> "What are the THREE things wrong with this code? Think back to what you've learned about background tasks (Week 11), object storage (today), and API design (Week 4)."

**Answer:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           THREE PROBLEMS WITH SYNC EXPORT                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. TIME: Building a large export takes minutes,                â”‚
â”‚     but HTTP requests have timeout limits (30-60s).             â”‚
â”‚     â†’ SOLUTION: Background task (Celery, Week 11)               â”‚
â”‚                                                                 â”‚
â”‚  2. MEMORY: Loading 500k rows + building a CSV string           â”‚
â”‚     consumes hundreds of MB of server RAM.                      â”‚
â”‚     One export request could OOM-kill your server.              â”‚
â”‚     â†’ SOLUTION: Stream rows, write to file, upload to S3        â”‚
â”‚                                                                 â”‚
â”‚  3. DELIVERY: Returning a 200MB response body ties up           â”‚
â”‚     a server connection for the entire transfer duration.       â”‚
â”‚     â†’ SOLUTION: Upload to S3, give client a presigned URL       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.6 The Async Export Pipeline

**The correct pattern: request â†’ accept â†’ process â†’ notify â†’ download.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ASYNC EXPORT ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  CLIENT                 YOUR API        CELERY         S3       â”‚
â”‚  â”€â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€         â”€â”€       â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚  1. â”‚â”€â”€ POST /exports â”€â”€â”€â”€â–¶â”‚               â”‚           â”‚        â”‚
â”‚     â”‚   {format: "csv"}    â”‚ Create job    â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚ record (DB)   â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚ Queue task    â”‚           â”‚        â”‚
â”‚     â”‚â—€â”€â”€ 202 Accepted â”€â”€â”€â”€â”€â”‚               â”‚           â”‚        â”‚
â”‚     â”‚   {export_id,        â”‚               â”‚           â”‚        â”‚
â”‚     â”‚    status: "queued"} â”‚               â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚     â”‚   (User can close browser â€” export continues)    â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚            2. â”‚ Query DB   â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚ Build CSV  â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚ Stream rowsâ”‚        â”‚
â”‚     â”‚                      â”‚               â”‚â”€â”€Uploadâ”€â”€â–¶â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚ Store  â”‚
â”‚     â”‚                      â”‚               â”‚â—€â”€â”€ OK â”€â”€â”€â”€â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚â—€â”€ Update DB â”€â”€â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚  status =     â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚  "completed"  â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚  3. â”‚â”€â”€ GET /exports/      â”‚               â”‚           â”‚        â”‚
â”‚     â”‚   {export_id} â”€â”€â”€â”€â”€â”€â–¶â”‚               â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚ Check status  â”‚           â”‚        â”‚
â”‚     â”‚â—€â”€â”€ 200 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚           â”‚        â”‚
â”‚     â”‚   {status:"completed"â”‚               â”‚           â”‚        â”‚
â”‚     â”‚    download_url:     â”‚               â”‚           â”‚        â”‚
â”‚     â”‚    "https://s3/..."}â”‚               â”‚           â”‚        â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚  4. â”‚â”€â”€ GET download_url â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚         â”‚
â”‚     â”‚â—€â”€â”€ 200 [file bytes] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚
â”‚     â”‚                      â”‚               â”‚           â”‚        â”‚
â”‚                                                                 â”‚
â”‚  ALTERNATIVE TO POLLING (step 3):                               â”‚
â”‚  Push notification via WebSocket (Week 12) when export done.    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The export job model:**

```python
# app/models/export_job.py

class ExportJob(Base):
    __tablename__ = "export_jobs"
    
    id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)
    
    # What to export
    export_type: Mapped[str] = mapped_column(String(50))  # "tasks", "members", "audit_log"
    format: Mapped[str] = mapped_column(String(10))        # "csv", "json"
    
    # Lifecycle tracking
    status: Mapped[str] = mapped_column(
        String(20), default="queued"
    )  # queued â†’ processing â†’ completed | failed
    
    progress_percent: Mapped[int] = mapped_column(default=0)  # 0-100
    
    # Result (populated on completion)
    storage_key: Mapped[str | None] = mapped_column(String(500))
    file_size_bytes: Mapped[int | None] = mapped_column(BigInteger)
    row_count: Mapped[int | None] = mapped_column()
    error_message: Mapped[str | None] = mapped_column(Text)
    
    # Ownership and tenant isolation
    requested_by: Mapped[uuid.UUID] = mapped_column(ForeignKey("users.id"))
    organization_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("organizations.id"))
    
    created_at: Mapped[datetime] = mapped_column(default=func.now())
    completed_at: Mapped[datetime | None] = mapped_column()
```

**The Celery export task:**

```python
# app/tasks/exports.py

import csv
import io
import json
from app.core.celery_app import celery_app
from app.integrations.storage import get_storage_service_sync
from app.core.database import get_sync_session  # Sync session for Celery


@celery_app.task(bind=True, max_retries=2)
def generate_export_task(
    self,
    export_job_id: str,
    organization_id: str,
    export_type: str,
    format: str,
) -> None:
    """Generate data export in background.
    
    Runs in Celery worker (sync context).
    Uses sync DB session and sync S3 client.
    """
    storage = get_storage_service_sync()
    
    with get_sync_session() as session:
        # Update status: processing
        job = session.get(ExportJob, export_job_id)
        job.status = "processing"
        session.commit()
        
        try:
            # â”€â”€ Build the export file â”€â”€
            if export_type == "tasks":
                rows = fetch_all_tasks_sync(session, organization_id)
            else:
                raise ValueError(f"Unknown export type: {export_type}")
            
            if format == "csv":
                file_bytes, content_type = build_csv(rows)
            elif format == "json":
                file_bytes, content_type = build_json(rows)
            else:
                raise ValueError(f"Unknown format: {format}")
            
            # â”€â”€ Upload to S3 â”€â”€
            storage_key = (
                f"exports/{organization_id}/"
                f"{export_job_id}.{format}"
            )
            storage.upload_bytes(storage_key, file_bytes, content_type)
            
            # â”€â”€ Update job: completed â”€â”€
            job.status = "completed"
            job.storage_key = storage_key
            job.file_size_bytes = len(file_bytes)
            job.row_count = len(rows)
            job.completed_at = func.now()
            session.commit()
            
        except Exception as exc:
            job.status = "failed"
            job.error_message = str(exc)
            session.commit()
            raise self.retry(exc=exc)


def build_csv(rows: list[dict]) -> tuple[bytes, str]:
    """Build CSV from row dicts. Returns (bytes, content_type)."""
    output = io.StringIO()
    if not rows:
        return b"", "text/csv"
    
    writer = csv.DictWriter(output, fieldnames=rows[0].keys())
    writer.writeheader()
    writer.writerows(rows)
    
    return output.getvalue().encode("utf-8"), "text/csv"


def build_json(rows: list[dict]) -> tuple[bytes, str]:
    """Build JSON from row dicts. Returns (bytes, content_type)."""
    # Use default=str for UUID and datetime serialization
    data = json.dumps(rows, default=str, indent=2)
    return data.encode("utf-8"), "application/json"
```

**The endpoints:**

```python
# app/routers/exports.py

router = APIRouter(prefix="/exports", tags=["exports"])


class ExportRequest(BaseModel):
    export_type: str = Field(pattern=r"^(tasks|members|audit_log)$")
    format: str = Field(pattern=r"^(csv|json)$")


class ExportStatusResponse(BaseModel):
    id: uuid.UUID
    status: str
    progress_percent: int
    download_url: str | None = None
    row_count: int | None = None
    error_message: str | None = None
    created_at: datetime


@router.post("", response_model=ExportStatusResponse, status_code=202)
async def request_export(
    body: ExportRequest,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
):
    """Request a data export. Returns immediately with job ID.
    
    HTTP 202 Accepted: "I've received your request and will
    process it, but it's not done yet."
    (Week 4: HTTP status code semantics)
    """
    job = ExportJob(
        export_type=body.export_type,
        format=body.format,
        requested_by=current_user.id,
        organization_id=current_user.organization_id,
    )
    session.add(job)
    await session.commit()
    
    # Queue background task
    generate_export_task.delay(
        export_job_id=str(job.id),
        organization_id=str(current_user.organization_id),
        export_type=body.export_type,
        format=body.format,
    )
    
    return ExportStatusResponse(
        id=job.id,
        status="queued",
        progress_percent=0,
        created_at=job.created_at,
    )


@router.get("/{export_id}", response_model=ExportStatusResponse)
async def get_export_status(
    export_id: uuid.UUID,
    current_user: User = Depends(get_current_user),
    session: AsyncSession = Depends(get_session),
    storage: StorageService = Depends(get_storage_service),
):
    """Poll export status. Returns download URL when complete."""
    job = await export_repo.get_by_id_and_org(
        session, export_id, current_user.organization_id
    )
    if not job:
        raise HTTPException(status_code=404, detail="Export not found")
    
    download_url = None
    if job.status == "completed" and job.storage_key:
        download_url = storage.generate_download_url(
            key=job.storage_key,
            filename=f"{job.export_type}_{job.id}.{job.format}",
        )
    
    return ExportStatusResponse(
        id=job.id,
        status=job.status,
        progress_percent=job.progress_percent,
        download_url=download_url,
        row_count=job.row_count,
        error_message=job.error_message,
        created_at=job.created_at,
    )
```

---

## 4.7 Common Mistakes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SEARCH & EXPORT MISTAKES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  SEARCH:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  âŒ MISTAKE: Forgetting the GIN index                           â”‚
â”‚     Result: Full-text search works but does sequential scan     â”‚
â”‚     Fix: Always create a GIN index on tsvector columns          â”‚
â”‚     Verify with EXPLAIN (Week 7)                                â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE: Using raw tsquery with user input                  â”‚
â”‚     Input: "hello & | world" â†’ syntax error â†’ 500              â”‚
â”‚     Fix: Use plainto_tsquery() â€” handles any input safely       â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE: Not scoping search to tenant                       â”‚
â”‚     Result: User searches across ALL organizations' data        â”‚
â”‚     Fix: Always include organization_id in WHERE clause         â”‚
â”‚                                                                 â”‚
â”‚  EXPORT:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  âŒ MISTAKE: Loading all rows into memory at once               â”‚
â”‚     Result: OOM-kill with large datasets                        â”‚
â”‚     Fix: Use streaming/batched queries for very large exports   â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE: No expiration on export files in S3                â”‚
â”‚     Result: Storage costs grow forever                          â”‚
â”‚     Fix: S3 lifecycle policy OR Celery Beat cleanup job         â”‚
â”‚     Delete export files older than 7 days                       â”‚
â”‚                                                                 â”‚
â”‚  âŒ MISTAKE: No rate limit on export requests                   â”‚
â”‚     Result: User queues 50 exports â†’ overwhelms workers         â”‚
â”‚     Fix: Limit to N active exports per user/org                 â”‚
â”‚     (Check in endpoint before queuing)                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 5: ADMIN DASHBOARD & AGGREGATION

## 5.1 The Dashboard That Killed the Database

**Recall the naive version from Part 1. Six separate queries, every page load:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           THE DASHBOARD DEATH SPIRAL                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Admin loads dashboard:                                         â”‚
â”‚                                                                 â”‚
â”‚  SELECT COUNT(*) FROM tasks WHERE org_id = $1;          â”€â”     â”‚
â”‚  SELECT COUNT(*) FROM tasks WHERE status = 'done' ...;   â”‚     â”‚
â”‚  SELECT COUNT(DISTINCT user_id) FROM task_assignments ..;â”‚     â”‚
â”‚  SELECT status, COUNT(*) FROM tasks GROUP BY status;     â”œâ”€ 6  â”‚
â”‚  SELECT DATE_TRUNC('day', ...), COUNT(*) ...             â”‚ DB  â”‚
â”‚     GROUP BY 1 ORDER BY 1;                               â”‚ hitsâ”‚
â”‚  SELECT u.name, COUNT(t.id) FROM users u                 â”‚     â”‚
â”‚     JOIN tasks t ON ... GROUP BY u.name LIMIT 10;       â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚  Total: ~200ms per dashboard load                               â”‚
â”‚                                                                 â”‚
â”‚  Now multiply:                                                  â”‚
â”‚  â€¢ 50 admins across all tenants                                 â”‚
â”‚  â€¢ Auto-refresh every 10 seconds (common in dashboards)         â”‚
â”‚  â€¢ 50 Ã— 6 queries Ã— 6 per minute = 1,800 aggregation           â”‚
â”‚    queries per minute                                           â”‚
â”‚                                                                 â”‚
â”‚  These are EXPENSIVE queries â€” GROUP BY, COUNT, JOIN â€”          â”‚
â”‚  exactly the kind that make PostgreSQL sweat.                   â”‚
â”‚                                                                 â”‚
â”‚  The irony: Dashboard data barely changes minute to minute.     â”‚
â”‚  You're recomputing the SAME numbers 1,800 times.              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The fix has two parts:**

1. **Consolidate queries** (one query with CTEs instead of six round-trips)
2. **Cache the result** (Redis, from Week 10)

---

## 5.2 Aggregation Queries with CTEs

> "You learned CTEs (Common Table Expressions) in Week 5 and used raw SQL via `text()` in Week 7. For complex dashboard queries, a well-structured CTE is clearer and faster than six separate ORM calls."

```python
# app/repositories/analytics.py

from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession


async def get_org_dashboard_stats(
    session: AsyncSession,
    organization_id: uuid.UUID,
) -> dict:
    """Single query that computes all dashboard metrics.
    
    Uses CTEs to organize sub-queries.
    PostgreSQL executes this as one plan â€” much more efficient
    than 6 separate round-trips.
    
    Why raw SQL here and not ORM?
    Complex aggregations with CTEs, date_trunc, json_agg are
    cleaner and more readable in SQL. The ORM adds verbosity
    without adding safety for read-only analytics queries.
    (Week 7, Lecture 2: "Raw SQL when ORM isn't enough")
    """
    query = text("""
        WITH task_counts AS (
            SELECT
                COUNT(*) AS total,
                COUNT(*) FILTER (WHERE status = 'done') AS completed,
                COUNT(*) FILTER (WHERE status = 'in_progress') AS in_progress,
                COUNT(*) FILTER (WHERE status = 'todo') AS todo,
                COUNT(*) FILTER (
                    WHERE created_at >= NOW() - INTERVAL '7 days'
                ) AS created_last_week
            FROM tasks
            WHERE organization_id = :org_id
              AND deleted_at IS NULL
        ),
        active_members AS (
            SELECT COUNT(DISTINCT user_id) AS count
            FROM task_assignments
            WHERE organization_id = :org_id
              AND assigned_at >= NOW() - INTERVAL '7 days'
        ),
        daily_trend AS (
            SELECT
                DATE_TRUNC('day', created_at)::date AS day,
                COUNT(*) AS created,
                COUNT(*) FILTER (WHERE status = 'done') AS completed
            FROM tasks
            WHERE organization_id = :org_id
              AND created_at >= NOW() - INTERVAL '30 days'
              AND deleted_at IS NULL
            GROUP BY DATE_TRUNC('day', created_at)::date
            ORDER BY day
        ),
        top_members AS (
            SELECT
                u.id,
                u.full_name,
                COUNT(t.id) AS tasks_completed
            FROM users u
            JOIN tasks t ON t.assignee_id = u.id
            WHERE t.organization_id = :org_id
              AND t.status = 'done'
              AND t.completed_at >= NOW() - INTERVAL '30 days'
            GROUP BY u.id, u.full_name
            ORDER BY tasks_completed DESC
            LIMIT 5
        )
        SELECT
            (SELECT row_to_json(task_counts) FROM task_counts)
                AS task_summary,
            (SELECT count FROM active_members)
                AS active_member_count,
            (SELECT json_agg(daily_trend) FROM daily_trend)
                AS daily_trend,
            (SELECT json_agg(top_members) FROM top_members)
                AS top_members
    """)
    
    result = await session.execute(query, {"org_id": str(organization_id)})
    row = result.mappings().one()
    
    return {
        "task_summary": row["task_summary"],
        "active_member_count": row["active_member_count"] or 0,
        "daily_trend": row["daily_trend"] or [],
        "top_members": row["top_members"] or [],
    }
```

**Why CTEs instead of six queries:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            6 QUERIES vs 1 CTE QUERY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  6 SEPARATE QUERIES:                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚  App â”€â”€[query 1]â”€â”€â–¶ DB â”€â”€â–¶ App                                  â”‚
â”‚  App â”€â”€[query 2]â”€â”€â–¶ DB â”€â”€â–¶ App                                  â”‚
â”‚  App â”€â”€[query 3]â”€â”€â–¶ DB â”€â”€â–¶ App                                  â”‚
â”‚  App â”€â”€[query 4]â”€â”€â–¶ DB â”€â”€â–¶ App                                  â”‚
â”‚  App â”€â”€[query 5]â”€â”€â–¶ DB â”€â”€â–¶ App                                  â”‚
â”‚  App â”€â”€[query 6]â”€â”€â–¶ DB â”€â”€â–¶ App                                  â”‚
â”‚                                                                 â”‚
â”‚  6 round-trips Ã— ~5ms network latency = 30ms overhead           â”‚
â”‚  6 query plan compilations                                      â”‚
â”‚  Total: ~200ms                                                  â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  1 CTE QUERY:                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  App â”€â”€[one big query]â”€â”€â–¶ DB â”€â”€â–¶ App                            â”‚
â”‚                                                                 â”‚
â”‚  1 round-trip = 5ms overhead                                    â”‚
â”‚  1 query plan (PostgreSQL optimizes CTEs together)              â”‚
â”‚  Total: ~80ms                                                   â”‚
â”‚                                                                 â”‚
â”‚  2.5x faster, even BEFORE caching.                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.3 Caching Dashboard Data

> "You learned cache-aside pattern in Week 10. Dashboards are the textbook use case: expensive to compute, tolerant of staleness, frequently accessed."

```python
# app/services/dashboard.py

from app.repositories.analytics import get_org_dashboard_stats


class DashboardService:
    """Dashboard data with Redis caching.
    
    Cache strategy: Time-based invalidation (TTL).
    Why not event-based? Dashboard aggregates over thousands
    of rows â€” invalidating on every task update would defeat
    the purpose. A 5-minute TTL means data is at most
    5 minutes stale. For dashboards, that's fine.
    """
    
    CACHE_TTL = 300  # 5 minutes
    
    def __init__(self, redis, session):
        self.redis = redis
        self.session = session
    
    async def get_dashboard(self, organization_id: uuid.UUID) -> dict:
        cache_key = f"dashboard:{organization_id}"
        
        # 1. Try cache (Week 10: cache-aside)
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # 2. Cache miss: compute from DB
        stats = await get_org_dashboard_stats(self.session, organization_id)
        
        # 3. Store in cache with TTL
        await self.redis.setex(
            cache_key,
            self.CACHE_TTL,
            json.dumps(stats, default=str),
        )
        
        return stats
```

**Impact:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            WITH CACHING: THE MATH                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WITHOUT CACHE:                                                 â”‚
â”‚  50 admins Ã— 6 refreshes/min = 300 DB queries/min              â”‚
â”‚  Each query: ~80ms of DB CPU time                               â”‚
â”‚  Total DB load: 24,000ms/min = 40% of one DB CPU core          â”‚
â”‚                                                                 â”‚
â”‚  WITH 5-MIN CACHE:                                              â”‚
â”‚  First request per org per 5 min = DB query (cache miss)        â”‚
â”‚  All subsequent requests = Redis GET (sub-millisecond)          â”‚
â”‚                                                                 â”‚
â”‚  If 10 orgs active:                                             â”‚
â”‚  DB queries: 10 orgs Ã— 12 per hour = 120/hour (was 18,000)     â”‚
â”‚  Redis GETs: ~18,000/hour (fast, cheap, what Redis is for)      â”‚
â”‚                                                                 â”‚
â”‚  DB load reduction: 99.3%                                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.4 The Dashboard Endpoint

```python
# app/routers/admin.py

router = APIRouter(prefix="/admin", tags=["admin"])


class TaskSummary(BaseModel):
    total: int
    completed: int
    in_progress: int
    todo: int
    created_last_week: int


class DailyTrendPoint(BaseModel):
    day: date
    created: int
    completed: int


class TopMember(BaseModel):
    id: uuid.UUID
    full_name: str
    tasks_completed: int


class DashboardResponse(BaseModel):
    task_summary: TaskSummary
    active_member_count: int
    daily_trend: list[DailyTrendPoint]
    top_members: list[TopMember]
    cached: bool  # Let the frontend know if data is from cache
    generated_at: datetime


@router.get("/dashboard", response_model=DashboardResponse)
async def get_dashboard(
    current_user: User = Depends(get_current_admin_user),  # Admin only (Week 9 RBAC)
    session: AsyncSession = Depends(get_session),
    redis: Redis = Depends(get_redis),
):
    service = DashboardService(redis=redis, session=session)
    stats = await service.get_dashboard(current_user.organization_id)
    
    return DashboardResponse(
        task_summary=TaskSummary(**stats["task_summary"]),
        active_member_count=stats["active_member_count"],
        daily_trend=[DailyTrendPoint(**d) for d in stats["daily_trend"]],
        top_members=[TopMember(**m) for m in stats["top_members"]],
        cached=True,  # Could track actual cache hit/miss
        generated_at=datetime.utcnow(),
    )
```

---

# Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INTEGRATION PATTERNS QUICK REFERENCE               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  EMAIL:                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚    Endpoint â†’ Celery task â†’ Email provider API (httpx)          â”‚
â”‚    Webhook endpoint receives delivery status                    â”‚
â”‚    Always: idempotency key, signature verification, DB logging  â”‚
â”‚    Never: send email in request handler                         â”‚
â”‚                                                                 â”‚
â”‚  FILES:                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚    DB stores metadata. S3/MinIO stores bytes.                   â”‚
â”‚    Presigned URLs: client uploads/downloads directly to S3      â”‚
â”‚    Flow: request URL â†’ upload to S3 â†’ confirm â†’ download URL    â”‚
â”‚    Never: store files in PostgreSQL or on server filesystem     â”‚
â”‚                                                                 â”‚
â”‚  SEARCH:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚    PostgreSQL tsvector + GIN index = fast full-text search       â”‚
â”‚    Computed column auto-updates when text fields change          â”‚
â”‚    plainto_tsquery() for safe user input handling               â”‚
â”‚    ts_rank() for relevance ordering                             â”‚
â”‚    Never: LIKE '%query%' at scale                               â”‚
â”‚                                                                 â”‚
â”‚  EXPORT:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚    POST /exports â†’ 202 Accepted {export_id}                     â”‚
â”‚    Celery task builds file â†’ uploads to S3                      â”‚
â”‚    GET /exports/{id} â†’ poll status â†’ get download URL           â”‚
â”‚    Never: build large exports in request handler                â”‚
â”‚                                                                 â”‚
â”‚  DASHBOARD:                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚    Single CTE query for all metrics (not 6 round-trips)         â”‚
â”‚    Cache in Redis with TTL (5 min typical)                      â”‚
â”‚    Never: recompute aggregations on every page load             â”‚
â”‚                                                                 â”‚
â”‚  COMMON THREAD:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚    Your request handler COORDINATES. It doesn't EXECUTE.        â”‚
â”‚    Heavy work â†’ Celery. Storage â†’ S3. Repeated reads â†’ Redis.   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Summary: How All Five Patterns Connect

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  THE INTEGRATION MAP                                            â”‚
â”‚  (How patterns reference each other in your capstone)           â”‚
â”‚                                                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                    â”‚   FastAPI Route   â”‚                          â”‚
â”‚                    â”‚   Handlers        â”‚                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                             â”‚                                   â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚           â”‚        â”‚        â”‚        â”‚        â”‚                 â”‚
â”‚           â–¼        â–¼        â–¼        â–¼        â–¼                 â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”            â”‚
â”‚       â”‚Email â”‚ â”‚Files â”‚ â”‚Searchâ”‚ â”‚Exportâ”‚ â”‚Dash- â”‚            â”‚
â”‚       â”‚      â”‚ â”‚      â”‚ â”‚      â”‚ â”‚      â”‚ â”‚board â”‚            â”‚
â”‚       â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜            â”‚
â”‚          â”‚        â”‚        â”‚        â”‚        â”‚                 â”‚
â”‚          â”‚        â”‚        â”‚        â”‚        â”‚                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”          â”‚
â”‚    â”‚ Celery â”‚  â”‚  S3  â”‚ â”‚ PG  â”‚  â”‚ Celery â”‚ â”‚Redis â”‚          â”‚
â”‚    â”‚ Task   â”‚  â”‚MinIO â”‚ â”‚ GIN â”‚  â”‚ Task   â”‚ â”‚Cache â”‚          â”‚
â”‚    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜ â”‚Indexâ”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚        â”‚        â–²       â””â”€â”€â”€â”€â”€â”˜      â”‚   â–²                    â”‚
â”‚        â”‚        â”‚                    â”‚   â”‚                    â”‚
â”‚        â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                    â”‚
â”‚        â”‚         Export uploads to S3    â”‚                    â”‚
â”‚        â”‚                                 â”‚                    â”‚
â”‚        â–¼                                 â”‚                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚    â”‚SendGridâ”‚                    â”‚ Dashboard query            â”‚
â”‚    â”‚  API   â”‚                    â”‚ hits PostgreSQL on         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚ cache miss only            â”‚
â”‚                                  â–¼                            â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                           â”‚PostgreSQLâ”‚                         â”‚
â”‚                           â”‚  (CTEs)  â”‚                         â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                 â”‚
â”‚  All five patterns use tools you already know.                  â”‚
â”‚  The lecture taught you how to COMPOSE them into features.      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Connection to Upcoming Lectures

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHERE THIS LEADS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WEEK 13, LECTURE 4 (Next):                                     â”‚
â”‚  â””â”€ Code Review & Technical Communication                       â”‚
â”‚     You'll review each other's integration implementations.     â”‚
â”‚     Can your teammate understand your email service             â”‚
â”‚     abstraction? Is your presigned URL flow documented?         â”‚
â”‚                                                                 â”‚
â”‚  WEEK 14 (Capstone Completion):                                 â”‚
â”‚  â””â”€ Implement these patterns in YOUR capstone project.          â”‚
â”‚     Every feature from today maps to a capstone requirement:    â”‚
â”‚     â€¢ Email â†’ "Background jobs (email notifications)"          â”‚
â”‚     â€¢ Files â†’ "File attachment support"                        â”‚
â”‚     â€¢ Search â†’ "Search and filtering with pagination"          â”‚
â”‚     â€¢ Export â†’ Background job, not listed but expected          â”‚
â”‚     â€¢ Dashboard â†’ Implicit in admin/aggregation features       â”‚
â”‚                                                                 â”‚
â”‚  WEEK 15 (Docker & CI/CD):                                      â”‚
â”‚  â””â”€ MinIO goes into your Docker Compose.                        â”‚
â”‚     SendGrid API key goes into secrets management.              â”‚
â”‚     CI tests mock both S3 and email services.                   â”‚
â”‚                                                                 â”‚
â”‚  WEEK 16 (System Design):                                       â”‚
â”‚  â””â”€ "Design a file upload system" is a classic interview        â”‚
â”‚     question. You've BUILT one. You can draw the presigned      â”‚
â”‚     URL architecture from memory.                               â”‚
â”‚     "Design a notification system" â€” same advantage.            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```