# LECTURE DESIGN PHILOSOPHY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PEDAGOGICAL APPROACH                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  SCENARIO FIRST, TOOL SECOND                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  Students must feel the operational pain of unmonitored,        â”‚
â”‚  unscheduled background work before they see the fix.           â”‚
â”‚  We'll let things break on screen.                              â”‚
â”‚                                                                 â”‚
â”‚  OPERATIONS MINDSET SHIFT                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  This is not about writing code. It is about running systems.   â”‚
â”‚  The question is no longer "does it work?"                      â”‚
â”‚  It is "does it work at 3 AM when nobody is watching?"          â”‚
â”‚                                                                 â”‚
â”‚  ANALOGY-DRIVEN                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  A factory production floor analogy carries through every       â”‚
â”‚  concept. Schedule â†’ Protect â†’ Observe â†’ React.                 â”‚
â”‚                                                                 â”‚
â”‚  CONNECT TO PRIOR LECTURES                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  Celery tasks (Lecture 2) â†’ now we schedule them                â”‚
â”‚  Error handling / retries (Lecture 2) â†’ now we handle the       â”‚
â”‚    cases when retries aren't enough                             â”‚
â”‚  Logging (Week 3) â†’ now we build alerting on top of it          â”‚
â”‚  Redis (Week 10) â†’ Beat uses Redis for schedule state           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# LECTURE OUTLINE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SCHEDULING & MONITORING                        â”‚
â”‚                     (3-4 Hour Lecture)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PART 1: THE PROBLEM (25 min)                                   â”‚
â”‚  â”œâ”€ 1.1 The 3 AM Question (Demonstration)                       â”‚
â”‚  â”œâ”€ 1.2 The while-True Anti-Pattern                             â”‚
â”‚  â””â”€ 1.3 The Factory Floor Analogy                               â”‚
â”‚                                                                 â”‚
â”‚  PART 2: CELERY BEAT â€” THE SCHEDULER (50 min)                   â”‚
â”‚  â”œâ”€ 2.1 What is Celery Beat?                                    â”‚
â”‚  â”œâ”€ 2.2 Interval Schedules (timedelta)                          â”‚
â”‚  â”œâ”€ 2.3 Crontab Schedules (crontab)                             â”‚
â”‚  â”œâ”€ 2.4 Configuring beat_schedule                               â”‚
â”‚  â”œâ”€ 2.5 Running Beat                                            â”‚
â”‚  â””â”€ 2.6 The Single-Beat Rule                                    â”‚
â”‚                                                                 â”‚
â”‚  PART 3: TASK TIMEOUTS â€” SAFETY SWITCHES (40 min)               â”‚
â”‚  â”œâ”€ 3.1 Why Tasks Need Timeouts                                 â”‚
â”‚  â”œâ”€ 3.2 soft_time_limit (The Warning Bell)                      â”‚
â”‚  â”œâ”€ 3.3 time_limit (The Emergency Stop)                         â”‚
â”‚  â”œâ”€ 3.4 Setting Timeouts (Per-Task and Global)                  â”‚
â”‚  â””â”€ 3.5 Graceful Cleanup on Timeout                             â”‚
â”‚                                                                 â”‚
â”‚  PART 4: DEAD LETTER HANDLING (30 min)                          â”‚
â”‚  â”œâ”€ 4.1 What is a Poison Message?                               â”‚
â”‚  â”œâ”€ 4.2 When Retries Aren't Enough                              â”‚
â”‚  â”œâ”€ 4.3 The on_failure Hook                                     â”‚
â”‚  â””â”€ 4.4 Building a Dead Letter Store                            â”‚
â”‚                                                                 â”‚
â”‚  PART 5: MONITORING WITH FLOWER (30 min)                        â”‚
â”‚  â”œâ”€ 5.1 Why Monitoring Matters                                  â”‚
â”‚  â”œâ”€ 5.2 Flower Setup                                            â”‚
â”‚  â”œâ”€ 5.3 Reading the Dashboard                                   â”‚
â”‚  â””â”€ 5.4 Worker Inspection from Code                             â”‚
â”‚                                                                 â”‚
â”‚  PART 6: ALERTING ON FAILURES (25 min)                          â”‚
â”‚  â”œâ”€ 6.1 The Dashboard Nobody Watches                            â”‚
â”‚  â”œâ”€ 6.2 Celery Signals                                          â”‚
â”‚  â”œâ”€ 6.3 Health Check Endpoints                                  â”‚
â”‚  â””â”€ 6.4 Common Mistakes                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 1: THE PROBLEM

## 1.1 The 3 AM Question

**Start with a scenario. Not a concept â€” a disaster.**

> "You shipped your background task system from Lecture 2. Tasks trigger when users do things â€” great. But your PM just posted three new requirements."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  NEW REQUIREMENTS (Monday Morning)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Every hour: delete expired JWT refresh tokens from Redis    â”‚
â”‚     (Remember Week 9 auth? Tokens pile up. Redis isn't free.)   â”‚
â”‚                                                                 â”‚
â”‚  2. Every midnight: generate a daily usage report and store it  â”‚
â”‚     in the database for the admin dashboard                     â”‚
â”‚                                                                 â”‚
â”‚  3. Every 5 minutes: check if the external APIs we depend on    â”‚
â”‚     (from Week 8) are still responding                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "None of these are triggered by a user action. There is no HTTP request. There is no button. These need to happen **on a clock**, automatically, forever. How would you build this with what you know right now?"

**Pause. Let them think.**

Most students will arrive at the same answer. Let them say it.

---

## 1.2 The while-True Anti-Pattern

**Show the answer they're thinking of:**

```python
# naive_scheduler.py â€” "It works on my laptop"
import time
from tasks import cleanup_tokens, generate_report, check_api_health

def run_scheduler():
    last_cleanup = 0
    last_report = 0
    last_health = 0

    while True:
        now = time.time()

        if now - last_health >= 300:          # Every 5 minutes
            check_api_health.delay()
            last_health = now

        if now - last_cleanup >= 3600:        # Every hour
            cleanup_tokens.delay()
            last_cleanup = now

        if now - last_report >= 86400:        # Every 24 hours
            generate_report.delay()
            last_report = now

        time.sleep(10)  # Check every 10 seconds

if __name__ == "__main__":
    run_scheduler()
```

**Now ask the class:**

> "This works. Run the script, tasks get scheduled. Ship it. What could go wrong?"

**Let them brainstorm. Then reveal the answer, one disaster at a time:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHAT GOES WRONG WITH while-True                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â¶ THE SCRIPT CRASHES                                           â”‚
â”‚     An unhandled exception at 2:47 AM. The process dies.        â”‚
â”‚     No more scheduled tasks. Nobody notices until Monday.       â”‚
â”‚     3 days of missed token cleanup. Redis is full.              â”‚
â”‚                                                                 â”‚
â”‚  â· THE SERVER REBOOTS                                           â”‚
â”‚     Kernel update, hardware issue, cloud provider maintenance.  â”‚
â”‚     Your script was running in a terminal tab.                  â”‚
â”‚     It did not come back.                                       â”‚
â”‚                                                                 â”‚
â”‚  â¸ TIME DRIFT                                                   â”‚
â”‚     time.sleep(10) doesn't sleep for exactly 10 seconds.        â”‚
â”‚     Over weeks, your "every midnight" report drifts.            â”‚
â”‚     Now it runs at 12:04 AM. Then 12:11 AM. Then...            â”‚
â”‚                                                                 â”‚
â”‚  â¹ YOU DEPLOY A NEW VERSION                                     â”‚
â”‚     You restart the app. The script restarts.                   â”‚
â”‚     All the last_cleanup / last_report timestamps reset to 0.   â”‚
â”‚     Every scheduled task fires immediately. At once.            â”‚
â”‚                                                                 â”‚
â”‚  âº A TASK HANGS FOREVER                                         â”‚
â”‚     generate_report calls an external API that never responds.  â”‚
â”‚     The task sits there. Consuming a worker. Holding a DB       â”‚
â”‚     connection. Forever. You have no idea it happened.          â”‚
â”‚                                                                 â”‚
â”‚  â» TASKS FAIL SILENTLY                                          â”‚
â”‚     cleanup_tokens has a bug. It raises an exception.           â”‚
â”‚     Celery retries 3 times, gives up.                           â”‚
â”‚     Nobody notices. Expired tokens pile up for weeks.           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The real question:**

> "This lecture answers one thing: **How do you build background work that runs itself, protects itself, and tells you when it's broken?** That means four capabilities: scheduling, timeouts, dead letter handling, and monitoring with alerts."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               THE FOUR CAPABILITIES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚        SCHEDULE          â†’   Celery Beat                        â”‚
â”‚        "Run this task every hour"                               â”‚
â”‚                                                                 â”‚
â”‚        PROTECT           â†’   Task Timeouts                      â”‚
â”‚        "Kill it if it hangs longer than 2 minutes"              â”‚
â”‚                                                                 â”‚
â”‚        ESCALATE          â†’   Dead Letter Handling               â”‚
â”‚        "If it fails 3 times, stop retrying and flag it"         â”‚
â”‚                                                                 â”‚
â”‚        OBSERVE + REACT   â†’   Flower + Alerting                  â”‚
â”‚        "Show me what's running, and page me when it breaks"     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.3 The Factory Floor Analogy

**This analogy will carry through the entire lecture.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 THE FACTORY FLOOR ANALOGY                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Your background task system is a FACTORY.                      â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  PRODUCTION â”‚    â”‚   MACHINES   â”‚    â”‚   CONTROL    â”‚       â”‚
â”‚  â”‚  SCHEDULE   â”‚â”€â”€â”€â–¶â”‚  (Workers)   â”‚â”€â”€â”€â–¶â”‚     ROOM     â”‚       â”‚
â”‚  â”‚  (Beat)     â”‚    â”‚              â”‚    â”‚   (Flower)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                            â”‚                                    â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                     â”‚              â”‚                            â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚               â”‚  SAFETY   â”‚  â”‚  REJECT   â”‚                     â”‚
â”‚               â”‚  SHUTOFF  â”‚  â”‚    BIN    â”‚                     â”‚
â”‚               â”‚ (Timeout) â”‚  â”‚(Dead Ltrs)â”‚                     â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  Factory Concept           â”‚  Celery Equivalent                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  Production schedule board â”‚  beat_schedule configuration      â”‚
â”‚  Shift manager             â”‚  Celery Beat process              â”‚
â”‚  Machines on the floor     â”‚  Celery workers                   â”‚
â”‚  Products being made       â”‚  Tasks being executed             â”‚
â”‚  Safety auto-shutoff       â”‚  soft_time_limit                  â”‚
â”‚  Emergency power cut       â”‚  time_limit (hard kill)           â”‚
â”‚  Defective â†’ reject bin    â”‚  Dead letter handling             â”‚
â”‚  Control room monitors     â”‚  Flower dashboard                 â”‚
â”‚  Alarm siren / pager       â”‚  Celery signals + alerts          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "A well-run factory doesn't need the owner standing on the floor 24/7. The schedule runs itself. The machines have safety switches. Defective products get caught and set aside. The control room shows everything. And alarms go off when something is actually wrong. That's what we're building today."

---

# PART 2: CELERY BEAT â€” THE SCHEDULER

## 2.1 What is Celery Beat?

**Celery Beat is a separate process that sends task messages on a schedule.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CELERY BEAT                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Beat does NOT execute tasks.                                   â”‚
â”‚  Beat SENDS task messages to the broker at the right time.      â”‚
â”‚  Workers pick them up and execute them â€” same as always.        â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   sends message   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   picks up        â”‚
â”‚   â”‚  CELERY  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  REDIS  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶      â”‚
â”‚   â”‚   BEAT   â”‚   at 00:00 UTC    â”‚ (broker)â”‚             â”Œâ”€â”€â”€â”€â”€â”â”‚
â”‚   â”‚(schedulerâ”‚   at */5 min      â”‚  queue  â”‚             â”‚WORKRâ”‚â”‚
â”‚   â”‚ process) â”‚   at */1 hour     â”‚         â”‚             â”‚     â”‚â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  This is exactly like .delay() or .apply_async()                â”‚
â”‚  from Lecture 2 â€” but instead of YOUR CODE sending the          â”‚
â”‚  message, BEAT sends it on a clock.                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key insight:**

> "Beat is the shift manager who walks up to the production schedule board every few seconds, checks the clock, and says 'It's midnight â€” time to start the reporting job.' Then it walks over to the conveyor belt (Redis queue) and drops a work order on it. The machines (workers) pick it up and do the work. Beat never touches the product itself."

---

## 2.2 Interval Schedules (timedelta)

**The simplest schedule: "do this every N seconds/minutes/hours."**

```python
from datetime import timedelta

# These are the tasks you already have from Lecture 2.
# Nothing changes about how tasks are DEFINED.
# Only HOW THEY GET TRIGGERED is new.

# "Run check_api_health every 5 minutes"
# schedule = timedelta(minutes=5)

# "Run cleanup_tokens every 1 hour"
# schedule = timedelta(hours=1)

# "Run sync_external_data every 30 seconds"
# schedule = timedelta(seconds=30)
```

**How intervals work internally:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  INTERVAL SCHEDULE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  schedule = timedelta(minutes=5)                                â”‚
â”‚                                                                 â”‚
â”‚  Time:  00:00   00:05   00:10   00:15   00:20                   â”‚
â”‚           â”‚       â”‚       â”‚       â”‚       â”‚                     â”‚
â”‚    Beat:  ğŸ“¤      ğŸ“¤      ğŸ“¤      ğŸ“¤      ğŸ“¤                     â”‚
â”‚           â”‚       â”‚       â”‚       â”‚       â”‚                     â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚              5 min   5 min   5 min   5 min                      â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“¤ = Beat sends a task message to the broker                   â”‚
â”‚                                                                 â”‚
â”‚  Interval starts from when Beat STARTS, not from when           â”‚
â”‚  the task FINISHES. If the task takes 3 minutes and the         â”‚
â”‚  interval is 5 minutes, the next send is still 5 minutes        â”‚
â”‚  after the LAST SEND, not 5 minutes after completion.           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to use intervals:**

> "Intervals are for tasks where the exact wall-clock time doesn't matter. 'Every 5 minutes' â€” you don't care if it's 12:00 or 12:02. You just want roughly 5-minute gaps."

---

## 2.3 Crontab Schedules (crontab)

**For when exact wall-clock time matters: "every day at midnight," "every Monday at 9 AM."**

```python
from celery.schedules import crontab
```

**Crontab is modeled after the UNIX cron format, but with named keyword arguments instead of the cryptic five-field string:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CRONTAB FIELDS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  UNIX cron (for reference â€” you won't type this in Celery):     â”‚
â”‚                                                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€ minute (0-59)                                        â”‚
â”‚    â”‚ â”Œâ”€â”€â”€â”€ hour (0-23)                                          â”‚
â”‚    â”‚ â”‚ â”Œâ”€â”€ day of month (1-31)                                  â”‚
â”‚    â”‚ â”‚ â”‚ â”Œâ”€ month (1-12)                                        â”‚
â”‚    â”‚ â”‚ â”‚ â”‚ â”Œâ”€ day of week (0=Mon ... 6=Sun)                     â”‚
â”‚    â”‚ â”‚ â”‚ â”‚ â”‚                                                    â”‚
â”‚    * * * * *   â† UNIX cron syntax                               â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  Celery crontab (what you actually write):                      â”‚
â”‚                                                                 â”‚
â”‚    crontab(                                                     â”‚
â”‚        minute=0,             # default: '*' (every minute)      â”‚
â”‚        hour=0,               # default: '*' (every hour)        â”‚
â”‚        day_of_week='*',      # default: '*' (every day)         â”‚
â”‚        day_of_month='*',     # default: '*' (every day)         â”‚
â”‚        month_of_year='*',    # default: '*' (every month)       â”‚
â”‚    )                                                            â”‚
â”‚                                                                 â”‚
â”‚  The '*' means "every" â€” same as UNIX cron.                     â”‚
â”‚  Any field you DON'T specify defaults to '*'.                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Common schedule patterns â€” read these carefully:**

```python
from celery.schedules import crontab

# â”€â”€ EVERY MINUTE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab()
# All fields default to '*' â†’ every minute of every hour
# of every day. Almost never what you want.

# â”€â”€ EVERY HOUR, ON THE HOUR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute=0)
# minute=0 â†’ only at minute 0
# hour='*' â†’ every hour
# Result: 00:00, 01:00, 02:00, ... 23:00

# â”€â”€ EVERY DAY AT MIDNIGHT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute=0, hour=0)
# minute=0, hour=0 â†’ only at 00:00
# day_of_week='*' â†’ every day
# Result: 00:00 daily

# â”€â”€ EVERY 15 MINUTES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute='*/15')
# '*/15' means "every 15th minute"
# Result: :00, :15, :30, :45 of every hour

# â”€â”€ EVERY MONDAY AT 9:00 AM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute=0, hour=9, day_of_week=1)
# day_of_week: 0=Monday, 1=Tuesday, ... 6=Sunday
# Result: Tuesday 09:00 every week
#
# âš ï¸  WAIT. Read that again. Celery uses 0=Monday.
#     UNIX cron uses 0=Sunday. This is a common trap.

# â”€â”€ WEEKDAYS AT 8:30 AM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute=30, hour=8, day_of_week='mon-fri')
# You can use names: 'mon', 'tue', 'wed', 'thu', 'fri',
#                     'sat', 'sun'
# Ranges: 'mon-fri'
# Result: 08:30, Monday through Friday

# â”€â”€ FIRST DAY OF EVERY MONTH AT 6:00 AM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute=0, hour=6, day_of_month=1)
# Result: 06:00 on the 1st of every month

# â”€â”€ EVERY 3 HOURS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
crontab(minute=0, hour='*/3')
# Result: 00:00, 03:00, 06:00, 09:00, ... 21:00
```

**Ask the class:**

> "What does `crontab(minute=0, hour='8,12,18')` mean?"

Answer: Runs at 08:00, 12:00, and 18:00 every day. You can pass comma-separated values for specific times.

**Map it visually:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             CRONTAB VISUAL: crontab(minute=0, hour='*/3')      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Day timeline:                                                  â”‚
â”‚                                                                 â”‚
â”‚  00  01  02  03  04  05  06  07  08  09  10  11  12             â”‚
â”‚   â”‚           â”‚           â”‚           â”‚           â”‚             â”‚
â”‚   ğŸ“¤          ğŸ“¤          ğŸ“¤          ğŸ“¤          ğŸ“¤             â”‚
â”‚                                                                 â”‚
â”‚  12  13  14  15  16  17  18  19  20  21  22  23  00             â”‚
â”‚   â”‚           â”‚           â”‚           â”‚           â”‚             â”‚
â”‚   ğŸ“¤          ğŸ“¤          ğŸ“¤          ğŸ“¤        (next day)       â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“¤ = task message sent to broker                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2.4 Configuring beat_schedule

**Now we connect the schedule to actual tasks.**

You already have tasks defined from Lecture 2. Nothing about the task definition changes. You add a configuration dictionary that tells Beat *which* task to send and *when*.

```python
# celery_app.py â€” add this to your existing Celery config

from celery import Celery
from celery.schedules import crontab
from datetime import timedelta

celery_app = Celery(
    "worker",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/1",
)

# â”€â”€ THIS IS THE NEW PART â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
celery_app.conf.beat_schedule = {

    # â”€â”€ Key: a human-readable name (your choice) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "cleanup-expired-tokens": {
        "task": "tasks.cleanup_expired_tokens",  # Dotted path to task
        "schedule": crontab(minute=0),           # Every hour, on the hour
        # No args needed for this one
    },

    "generate-daily-report": {
        "task": "tasks.generate_daily_report",
        "schedule": crontab(minute=0, hour=0),   # Every day at midnight
        "args": ("usage",),                      # Positional arguments
    },

    "check-external-apis": {
        "task": "tasks.check_api_health",
        "schedule": timedelta(minutes=5),        # Every 5 minutes
        "kwargs": {"timeout": 10},               # Keyword arguments
    },

    "archive-old-tasks": {
        "task": "tasks.archive_completed_tasks",
        "schedule": crontab(                     # Sundays at 3 AM
            minute=0,
            hour=3,
            day_of_week="sun",
        ),
        "args": (30,),  # Archive tasks older than 30 days
    },
}

# â”€â”€ TIMEZONE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Critical: crontab hours are interpreted in this timezone.
# Always set this explicitly. Never rely on the default.
celery_app.conf.timezone = "UTC"
```

**The structure of each entry:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               beat_schedule ENTRY STRUCTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  "human-readable-name": {                                       â”‚
â”‚      "task":     "dotted.path.to.task.function",   # REQUIRED   â”‚
â”‚      "schedule": crontab(...) or timedelta(...),   # REQUIRED   â”‚
â”‚      "args":     (arg1, arg2),          # optional, positional  â”‚
â”‚      "kwargs":   {"key": "value"},      # optional, keyword     â”‚
â”‚      "options":  {"queue": "high"},     # optional, task opts   â”‚
â”‚  }                                                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ "task" is a STRING â€” the import path, not a Python  â”‚        â”‚
â”‚  â”‚ reference. This is because Beat sends a MESSAGE     â”‚        â”‚
â”‚  â”‚ with the task name. The WORKER resolves the name    â”‚        â”‚
â”‚  â”‚ to actual code. Beat never imports your tasks.      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**And the tasks themselves â€” nothing special:**

```python
# tasks.py â€” these are normal tasks from Lecture 2
# Beat doesn't change how you DEFINE tasks.

import structlog
from celery_app import celery_app

logger = structlog.get_logger()


@celery_app.task
def cleanup_expired_tokens() -> int:
    """Remove expired refresh tokens from Redis."""
    # Your Redis cleanup logic from Week 10
    count = redis_client.delete_expired_tokens()
    logger.info("token_cleanup_complete", deleted=count)
    return count


@celery_app.task
def generate_daily_report(report_type: str) -> dict:
    """Generate and store a daily usage report."""
    # Query database, aggregate, store result
    report = build_report(report_type)
    store_report(report)
    logger.info("daily_report_generated", type=report_type)
    return {"type": report_type, "rows": report["row_count"]}


@celery_app.task
def check_api_health(timeout: int = 10) -> dict:
    """Ping external APIs and log their status."""
    results = {}
    for api_name, url in EXTERNAL_APIS.items():
        try:
            response = httpx.get(url, timeout=timeout)
            results[api_name] = response.status_code
        except httpx.TimeoutException:
            results[api_name] = "TIMEOUT"
            logger.warning("api_health_timeout", api=api_name)
    return results


@celery_app.task
def archive_completed_tasks(days_old: int) -> int:
    """Move completed tasks older than N days to archive."""
    cutoff = datetime.utcnow() - timedelta(days=days_old)
    count = move_to_archive(completed_before=cutoff)
    logger.info("archive_complete", archived=count, older_than_days=days_old)
    return count
```

> "Notice: the tasks are identical to what you wrote in Lecture 2. They have no awareness that Beat exists. Beat is entirely external â€” it just sends messages on a timer. The task doesn't know or care whether it was triggered by `task.delay()` in an API endpoint, or by Beat on a schedule, or by you calling it from a Python shell."

---

## 2.5 Running Beat

**Beat is its own process â€” separate from your workers.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  YOUR RUNNING PROCESSES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  BEFORE THIS LECTURE (Lecture 2):                                â”‚
â”‚                                                                 â”‚
â”‚    Terminal 1:  uvicorn app:app --reload     (FastAPI server)   â”‚
â”‚    Terminal 2:  celery -A celery_app worker  (Task executor)    â”‚
â”‚    Terminal 3:  redis-server                 (Message broker)   â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  AFTER THIS LECTURE:                                            â”‚
â”‚                                                                 â”‚
â”‚    Terminal 1:  uvicorn app:app --reload     (FastAPI server)   â”‚
â”‚    Terminal 2:  celery -A celery_app worker  (Task executor)    â”‚
â”‚    Terminal 3:  redis-server                 (Message broker)   â”‚
â”‚    Terminal 4:  celery -A celery_app beat    (Scheduler) â† NEW  â”‚
â”‚    Terminal 5:  celery -A celery_app flower  (Monitor)   â† NEW  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Starting Beat:**

```bash
# Basic â€” start the Beat scheduler
celery -A celery_app beat --loglevel=info

# What you'll see in the terminal:
# celery beat v5.3.6 (emerald-rush) is starting.
# __    -    ... __   -        _
# LocalTime -> 2025-01-15 00:00:00
# Configuration ->
#     . broker -> redis://localhost:6379/0
#     . loader -> celery.loaders.app.AppLoader
#     . scheduler -> celery.beat.PersistentScheduler
#     . db -> celerybeat-schedule          â† schedule state file
#     . logfile -> [stderr]@%INFO
#     . maxinterval -> 5.00 minutes (300s)
```

**The schedule state file:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BEAT STATE FILE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Beat creates a file called celerybeat-schedule in your         â”‚
â”‚  working directory. This file stores WHEN each task was         â”‚
â”‚  last sent.                                                     â”‚
â”‚                                                                 â”‚
â”‚  WHY IT MATTERS:                                                â”‚
â”‚  If you restart Beat, it reads this file and knows:             â”‚
â”‚  "I last sent cleanup-expired-tokens at 14:00.                  â”‚
â”‚  It's now 14:23. The schedule is every hour.                    â”‚
â”‚  Next send at 15:00 â€” not now."                                 â”‚
â”‚                                                                 â”‚
â”‚  Without this file, Beat would fire ALL tasks immediately       â”‚
â”‚  on restart â€” exactly the while-True problem from Part 1.       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Add celerybeat-schedule to your .gitignore.  â”‚               â”‚
â”‚  â”‚ It is local state, not source code.          â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Convenience: running Beat and worker in the same process (development only):**

```bash
# For local development, you can embed Beat inside the worker
celery -A celery_app worker --beat --loglevel=info

# The --beat flag makes the worker process ALSO run the scheduler.
# Convenient for development. NEVER use this in production.
# (We'll explain why in the next section.)
```

---

## 2.6 The Single-Beat Rule

**This is the most important operational rule in this entire section.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  âš ï¸  THERE MUST BE EXACTLY ONE BEAT PROCESS. EVER.              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why? Because Beat doesn't execute tasks â€” it sends messages.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                THE DUPLICATE BEAT DISASTER                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ONE Beat (correct):                                            â”‚
â”‚                                                                 â”‚
â”‚    Beat â”€â”€â”€â”€ 00:00 â”€â”€â”€â”€ sends 1 message â”€â”€â”€â”€ 1 report runs     â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  TWO Beats (disaster):                                          â”‚
â”‚                                                                 â”‚
â”‚    Beat 1 â”€â”€ 00:00 â”€â”€â”€â”€ sends 1 message â”€â”€â”                    â”‚
â”‚                                            â”œâ”€â”€ 2 reports run!   â”‚
â”‚    Beat 2 â”€â”€ 00:00 â”€â”€â”€â”€ sends 1 message â”€â”€â”˜                    â”‚
â”‚                                                                 â”‚
â”‚  Each Beat independently reads the schedule and sends           â”‚
â”‚  messages. They do not coordinate with each other.              â”‚
â”‚  Two Beats = every task runs twice. Three = three times.        â”‚
â”‚                                                                 â”‚
â”‚  Imagine your "generate invoice" task running twice.            â”‚
â”‚  Two invoices. Two charges to the customer.                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "This is why `--beat` on the worker is dangerous in production. If you scale to 3 workers with `--beat`, you get 3 Beat schedulers. Every scheduled task runs 3 times. In development, you run one worker, so it's fine. In production, run Beat as its own dedicated process."

**Factory analogy:**

> "A factory has one production schedule board and one shift manager reading it. If you accidentally hire three shift managers and give each a copy of the schedule, they'll all tell the machines to start the same job at the same time. You get three times the output you wanted â€” or three times the waste."

**How to enforce the single-Beat rule in production:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             ENFORCING SINGLE BEAT                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Option 1: Separate process (simplest)                          â”‚
â”‚  Run Beat as its own container/process in your deployment.      â”‚
â”‚  Scale workers to 5, 10, 50 â€” but only 1 Beat.                 â”‚
â”‚                                                                 â”‚
â”‚  Option 2: Pidfile                                              â”‚
â”‚  celery -A celery_app beat --pidfile=/var/run/celerybeat.pid    â”‚
â”‚  If a second Beat tries to start, it sees the pidfile and       â”‚
â”‚  refuses. Basic protection against accidental duplicates.       â”‚
â”‚                                                                 â”‚
â”‚  Option 3: Database-backed scheduler                            â”‚
â”‚  celery -A celery_app beat \                                    â”‚
â”‚    --scheduler django_celery_beat.schedulers:DatabaseScheduler   â”‚
â”‚  (or similar for non-Django projects)                           â”‚
â”‚  Stores schedule state in the database with a lock.             â”‚
â”‚  Safer in distributed deployments.                              â”‚
â”‚                                                                 â”‚
â”‚  For this course: Option 1. Separate process.                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Timezone reminder:**

```python
# ALWAYS set your timezone explicitly
celery_app.conf.timezone = "UTC"

# If you don't set this, Beat uses the system timezone.
# Your laptop: America/New_York.
# The production server: UTC.
# Your "midnight" report now runs at different times
# depending on where the code is running. Set. The. Timezone.
```

---

# PART 3: TASK TIMEOUTS â€” SAFETY SWITCHES

## 3.1 Why Tasks Need Timeouts

**A task without a timeout is a machine without a safety switch.**

```python
# This task calls an external API.
# What if the API never responds?

@celery_app.task(bind=True, max_retries=3)
def sync_user_data(self, user_id: int) -> dict:
    # httpx has a timeout â€” but what if the task itself
    # has a logic error? An infinite loop? A deadlock?
    response = httpx.get(
        f"https://api.example.com/users/{user_id}",
        timeout=30,
    )
    
    # Or what if this database query hangs because of a lock?
    db.execute(update_user_query, response.json())
    
    return {"user_id": user_id, "synced": True}
```

> "You set a timeout on your HTTP request â€” good. But the HTTP timeout protects the *request*. It doesn't protect the *task*. What if the code before or after the request hangs? What if the database query waits on a lock forever? The task itself needs a ceiling on how long it can run."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHAT HAPPENS WITHOUT TIMEOUTS                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Worker has 4 worker processes (default concurrency).           â”‚
â”‚                                                                 â”‚
â”‚  13:00  Task A starts, hangs on DB lock    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€ â”‚
â”‚  13:01  Task B starts, hangs on API        [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€  â”‚
â”‚  13:02  Task C starts, infinite loop bug   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€   â”‚
â”‚  13:03  Task D starts, hangs on DB lock    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”€â”€    â”‚
â”‚  13:04  Task E arrives...                                       â”‚
â”‚                                                                 â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚         â”‚  All 4 processes are stuck.                  â”‚        â”‚
â”‚         â”‚  Task E sits in the queue. Waiting.          â”‚        â”‚
â”‚         â”‚  Task F, G, H... all pile up.                â”‚        â”‚
â”‚         â”‚  Your entire background system is FROZEN.    â”‚        â”‚
â”‚         â”‚  No scheduled tasks run. No user-triggered   â”‚        â”‚
â”‚         â”‚  tasks run. Everything stops.                â”‚        â”‚
â”‚         â”‚                                              â”‚        â”‚
â”‚         â”‚  And nothing in your logs tells you why.     â”‚        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â”‚  Timeouts prevent one stuck task from killing the system.       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Factory analogy:**

> "If a machine on the factory floor jams, it doesn't just stop making its product â€” it blocks the production line. Other products can't get to the next machine. A safety switch detects the jam and shuts down the machine so a technician can fix it and the line keeps moving."

---

## 3.2 soft_time_limit (The Warning Bell)

**`soft_time_limit` raises a special exception INSIDE the running task.**

The task is still alive. It can catch the exception, clean up resources, save partial progress, and exit gracefully.

```python
from celery.exceptions import SoftTimeLimitExceeded

@celery_app.task(soft_time_limit=120)  # 120 seconds = 2 minutes
def generate_large_report(report_type: str) -> dict:
    """Generate a report that might take a while."""
    
    partial_results = []
    
    try:
        for chunk in get_data_chunks(report_type):
            processed = process_chunk(chunk)
            partial_results.append(processed)
            
    except SoftTimeLimitExceeded:
        # â”€â”€ THE WARNING BELL RANG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # We have a few seconds to clean up before the
        # hard time_limit kills us (if one is set).
        logger.warning(
            "report_soft_timeout",
            report_type=report_type,
            chunks_completed=len(partial_results),
        )
        # Save what we have so the work isn't lost
        save_partial_report(report_type, partial_results)
        return {
            "status": "partial",
            "chunks_completed": len(partial_results),
        }
    
    return {
        "status": "complete",
        "chunks_completed": len(partial_results),
    }
```

**How it works:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               soft_time_limit = 120                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Time:  0s         60s        120s       150s                   â”‚
â”‚         â”‚           â”‚          â”‚          â”‚                     â”‚
â”‚  Task:  [â•â•â• working â•â•â•â•â•â•â•â•]  â”‚          â”‚                     â”‚
â”‚                                â”‚          â”‚                     â”‚
â”‚                         SoftTimeLimitExceeded                   â”‚
â”‚                         raised INSIDE the task                  â”‚
â”‚                                â”‚          â”‚                     â”‚
â”‚                                [cleanup]  â”‚                     â”‚
â”‚                                save data  â”‚                     â”‚
â”‚                                log warningâ”‚                     â”‚
â”‚                                return     â”‚                     â”‚
â”‚                                           â”‚                     â”‚
â”‚  The task KEEPS RUNNING after the exception.                    â”‚
â”‚  It can catch the exception and do useful work.                 â”‚
â”‚  It should exit soon â€” the hard limit is coming.                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What SoftTimeLimitExceeded actually is:**

```python
# SoftTimeLimitExceeded inherits from Exception.
# It is raised using a signal (SIGUSR1) from the worker
# parent process into the task's thread/process.

# You catch it exactly like any other exception.
# (Remember exception hierarchies from Week 1, Lecture 2.)

try:
    do_work()
except SoftTimeLimitExceeded:
    # Clean up gracefully
    ...
```

> "The soft limit is the warning bell on the factory floor. The machine buzzes: 'You've been running too long. Wrap it up.' The operator (your code) can hear the buzzer, save the current product, and shut down cleanly."

---

## 3.3 time_limit (The Emergency Stop)

**`time_limit` kills the task's worker process. No exceptions. No cleanup. Dead.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  time_limit = 180                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Time:  0s         120s             180s                        â”‚
â”‚         â”‚           â”‚                â”‚                          â”‚
â”‚  Task:  [â•â•â• working â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•]                           â”‚
â”‚                                      â”‚                          â”‚
â”‚                                    SIGKILL                      â”‚
â”‚                                  Worker process                 â”‚
â”‚                                  terminated.                    â”‚
â”‚                                      â”‚                          â”‚
â”‚                                      â–¼                          â”‚
â”‚                                  New worker                     â”‚
â”‚                                  process spawned                â”‚
â”‚                                  automatically.                 â”‚
â”‚                                                                 â”‚
â”‚  The task CANNOT catch this. It is an OS-level kill.            â”‚
â”‚  The worker's parent process detects the dead child             â”‚
â”‚  and spawns a replacement. The task is marked FAILURE.          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The critical difference:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            soft_time_limit VS time_limit                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                    soft_time_limit        time_limit             â”‚
â”‚                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  Mechanism:        Raises exception       Sends SIGKILL         â”‚
â”‚  Task can catch:   Yes âœ…                 No âŒ                  â”‚
â”‚  Cleanup possible: Yes âœ…                 No âŒ                  â”‚
â”‚  Task survives:    Yes (if caught)        No (process dies)     â”‚
â”‚  Analogy:          Warning buzzer         Emergency power cut   â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  RECOMMENDED PATTERN: Use both together.                        â”‚
â”‚                                                                 â”‚
â”‚      soft_time_limit = X    â† Give task a chance to clean up   â”‚
â”‚      time_limit = X + 30    â† Hard kill if cleanup hangs too   â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  Time:  0        soft(X)    hard(X+30)                          â”‚
â”‚         â”‚           â”‚           â”‚                               â”‚
â”‚  Task:  [â•â• work â•â•]           â”‚                                â”‚
â”‚                     [cleanup]  â”‚                                â”‚
â”‚                         â”‚      â”‚                                â”‚
â”‚                         â–¼      â”‚                                â”‚
â”‚                     exits OK   â”‚                                â”‚
â”‚                                â”‚                                â”‚
â”‚         OR if cleanup hangs:   â”‚                                â”‚
â”‚                     [cleanup â”€â”€â”€â”€â”€â”€]                             â”‚
â”‚                                â”‚                                â”‚
â”‚                             SIGKILL  â† last resort              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.4 Setting Timeouts (Per-Task and Global)

**Per-task (preferred â€” each task has different expected durations):**

```python
# Quick task â€” should finish in under 30 seconds
@celery_app.task(soft_time_limit=30, time_limit=60)
def check_api_health(timeout: int = 10) -> dict:
    ...

# Medium task â€” may take a few minutes
@celery_app.task(soft_time_limit=300, time_limit=330)
def generate_daily_report(report_type: str) -> dict:
    ...

# Long task â€” batch processing, may take up to 30 minutes
@celery_app.task(soft_time_limit=1800, time_limit=1860)
def archive_completed_tasks(days_old: int) -> int:
    ...
```

**Global defaults (safety net for all tasks):**

```python
# celery_app.py â€” add to your configuration

celery_app.conf.task_soft_time_limit = 300   # 5 minutes default
celery_app.conf.task_time_limit = 330        # 5.5 minutes hard kill

# Per-task settings OVERRIDE these globals.
# Globals exist to catch tasks where you FORGOT to set a timeout.
# Every task should have an explicit timeout, but humans forget.
# The global default is your safety net.
```

**How to think about choosing timeout values:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             CHOOSING TIMEOUT VALUES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Measure how long the task NORMALLY takes.                   â”‚
â”‚     Run it 10 times. Note the slowest run.                      â”‚
â”‚                                                                 â”‚
â”‚  2. Set soft_time_limit to 2-3x the normal worst case.          â”‚
â”‚     If it normally takes 10s at worst â†’ soft limit = 30s.       â”‚
â”‚     This gives room for slow networks, heavy DB load, etc.      â”‚
â”‚                                                                 â”‚
â”‚  3. Set time_limit to soft_time_limit + cleanup time.           â”‚
â”‚     If cleanup takes at most 10s â†’ time_limit = 40s.            â”‚
â”‚                                                                 â”‚
â”‚  4. If you can't measure yet, start generous and tighten.       â”‚
â”‚     Better to start with soft=300, hard=330 and collect         â”‚
â”‚     data, than to start with soft=5 and kill healthy tasks.     â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  A timeout that's too tight is WORSE than no       â”‚         â”‚
â”‚  â”‚  timeout at all. Tight timeouts kill healthy tasks  â”‚         â”‚
â”‚  â”‚  under load, masking the real problem.             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.5 Graceful Cleanup on Timeout

**A complete production pattern combining both limits:**

```python
from celery.exceptions import SoftTimeLimitExceeded
import structlog

logger = structlog.get_logger()

@celery_app.task(
    bind=True,
    soft_time_limit=120,
    time_limit=150,
)
def sync_user_data(self, user_id: int) -> dict:
    """
    Sync user data from external API to our database.
    
    Timeout strategy:
    - soft_time_limit=120: After 2 min, get a chance to clean up.
    - time_limit=150: After 2.5 min, hard kill. This gives 30s
      for the cleanup code in the except block.
    """
    log = logger.bind(task_id=self.request.id, user_id=user_id)
    db_session = None
    
    try:
        log.info("sync_started")
        
        # Step 1: Fetch from external API
        external_data = fetch_from_external_api(user_id)
        
        # Step 2: Transform
        transformed = transform_user_data(external_data)
        
        # Step 3: Write to database
        db_session = get_db_session()
        update_user_in_db(db_session, user_id, transformed)
        db_session.commit()
        
        log.info("sync_completed")
        return {"user_id": user_id, "status": "synced"}
    
    except SoftTimeLimitExceeded:
        log.warning("sync_soft_timeout")
        
        # Roll back any uncommitted DB work
        if db_session is not None:
            db_session.rollback()
        
        # Return a meaningful result so the caller knows
        return {"user_id": user_id, "status": "timeout"}
    
    except Exception as exc:
        log.error("sync_failed", error=str(exc))
        
        if db_session is not None:
            db_session.rollback()
        
        raise  # Let Celery's retry mechanism handle it
    
    finally:
        # Always close the session
        # (Remember context managers from Week 1 â€” same idea,
        #  but we need manual control here because the task
        #  lifecycle is different from a with block.)
        if db_session is not None:
            db_session.close()
```

**The execution timeline for this task:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HAPPY PATH               TIMEOUT PATH                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Happy (finishes in 45s):    Timeout (API hangs):               â”‚
â”‚                                                                 â”‚
â”‚  0s   sync_started           0s   sync_started                  â”‚
â”‚  â”‚    fetch API (20s)        â”‚    fetch API ...                  â”‚
â”‚  â”‚    transform (5s)         â”‚    ... waiting ...                â”‚
â”‚  â”‚    DB write (10s)         â”‚    ... waiting ...                â”‚
â”‚  â”‚    commit (1s)            â”‚    ... waiting ...                â”‚
â”‚  45s  sync_completed         120s SoftTimeLimitExceeded          â”‚
â”‚       return "synced"        â”‚    rollback DB                   â”‚
â”‚       âœ… done                â”‚    return "timeout"              â”‚
â”‚                              125s âœ… done (cleanup took 5s)     â”‚
â”‚                                                                 â”‚
â”‚                              OR if cleanup also hangs:          â”‚
â”‚                              120s SoftTimeLimitExceeded          â”‚
â”‚                              â”‚    rollback hangs...             â”‚
â”‚                              â”‚    ... stuck ...                 â”‚
â”‚                              150s SIGKILL ğŸ’€                    â”‚
â”‚                                   process killed                â”‚
â”‚                                   new worker spawns             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 4: DEAD LETTER HANDLING

## 4.1 What is a Poison Message?

**A poison message is a task that ALWAYS fails, no matter how many times you retry it.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   POISON MESSAGES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TRANSIENT FAILURE (retries help):                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚  Attempt 1: API timeout   â†’ retry                              â”‚
â”‚  Attempt 2: API timeout   â†’ retry                              â”‚
â”‚  Attempt 3: API responds! â†’ success âœ…                          â”‚
â”‚                                                                 â”‚
â”‚  The failure was temporary. Network blip. Server busy.          â”‚
â”‚  Retries (from Lecture 2) solve this perfectly.                 â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  POISON MESSAGE (retries are useless):                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Attempt 1: User ID 99999 doesn't exist in external API â†’ fail â”‚
â”‚  Attempt 2: User ID 99999 doesn't exist in external API â†’ fail â”‚
â”‚  Attempt 3: User ID 99999 doesn't exist in external API â†’ fail â”‚
â”‚  All retries exhausted â†’ FAILURE âŒ                             â”‚
â”‚                                                                 â”‚
â”‚  The failure is PERMANENT. Retrying the same input with the     â”‚
â”‚  same code will always produce the same error. The message      â”‚
â”‚  is poisoned â€” it can never succeed.                            â”‚
â”‚                                                                 â”‚
â”‚  Other examples:                                                â”‚
â”‚  â€¢ Invalid data format (bad JSON in the arguments)              â”‚
â”‚  â€¢ Permission revoked (API key expired permanently)             â”‚
â”‚  â€¢ Database constraint violation (duplicate key)                â”‚
â”‚  â€¢ Bug in the task code itself                                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Factory analogy:**

> "A defective raw material arrives at the machine. The machine tries to process it â€” fails. Quality control sends it back. The machine tries again â€” fails. QC sends it back again. After three attempts, QC doesn't send it back a fourth time. They put it in the **reject bin** for a human to inspect. Maybe the raw material was wrong. Maybe the machine needs recalibration. Either way, the reject bin prevents the machine from spinning its wheels on something it can never fix."

---

## 4.2 When Retries Aren't Enough

**Recall from Lecture 2 â€” your retry configuration:**

```python
# This is what you already know.
@celery_app.task(
    bind=True,
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError),
    retry_backoff=True,
)
def unreliable_api_call(self, url: str) -> dict:
    response = httpx.get(url, timeout=10)
    response.raise_for_status()
    return response.json()
```

**What happens after max_retries is exhausted?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           AFTER MAX RETRIES: WHAT CELERY DOES                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Attempt 1: ConnectionError â†’ retry (wait 1s)                  â”‚
â”‚  Attempt 2: ConnectionError â†’ retry (wait 2s)                  â”‚
â”‚  Attempt 3: ConnectionError â†’ retry (wait 4s)                  â”‚
â”‚  Attempt 4: ConnectionError â†’ MAX RETRIES REACHED              â”‚
â”‚                                                                 â”‚
â”‚  Celery does the following:                                     â”‚
â”‚  1. Marks the task state as FAILURE                             â”‚
â”‚  2. Stores the exception in the result backend (Redis)          â”‚
â”‚  3. Calls the task's on_failure() method (if defined)           â”‚
â”‚  4. Moves on                                                    â”‚
â”‚                                                                 â”‚
â”‚  What Celery does NOT do:                                       â”‚
â”‚  âŒ Send you an email                                           â”‚
â”‚  âŒ Log a special warning                                       â”‚
â”‚  âŒ Store the failed task for manual review                     â”‚
â”‚  âŒ Alert anyone                                                â”‚
â”‚                                                                 â”‚
â”‚  The task quietly dies. If you aren't checking the result       â”‚
â”‚  backend, you will never know it failed.                        â”‚
â”‚                                                                 â”‚
â”‚  This is the "silent failure at 3 AM" from Part 1.              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "Celery's default behavior after exhausting retries is: mark it as failed and forget about it. That's fine for tasks where failure is acceptable â€” a cache refresh that'll run again in 5 minutes anyway. But for tasks where failure means **lost data** or **broken business logic**, you need to explicitly handle the dead letter."

---

## 4.3 The on_failure Hook

**Every Celery task has lifecycle hooks. `on_failure` fires when the task fails after all retries.**

```python
@celery_app.task(
    bind=True,
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError),
    retry_backoff=True,
)
def sync_user_data(self, user_id: int) -> dict:
    response = httpx.get(
        f"https://api.example.com/users/{user_id}",
        timeout=10,
    )
    response.raise_for_status()
    return response.json()
```

**Option 1 â€” Override on_failure directly on the task:**

```python
@celery_app.task(
    bind=True,
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError),
    retry_backoff=True,
)
def sync_user_data(self, user_id: int) -> dict:
    response = httpx.get(
        f"https://api.example.com/users/{user_id}",
        timeout=10,
    )
    response.raise_for_status()
    return response.json()

# This method is defined OUTSIDE the task function,
# but on the same task object â€” using the base class.
# It's cleaner to use a custom base class (shown next).
```

**Option 2 (preferred) â€” Custom base task class:**

```python
# base_task.py
import structlog
from celery import Task

logger = structlog.get_logger()


class AlertingTask(Task):
    """
    Custom base task that handles failures after all retries
    are exhausted. Use this as the base for any task where
    silent failure is not acceptable.
    """
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """
        Called when the task fails permanently.
        
        Parameters (provided by Celery â€” you don't pass these):
            exc:     The exception that caused the failure
            task_id: Unique ID of this task execution
            args:    Positional arguments the task was called with
            kwargs:  Keyword arguments the task was called with
            einfo:   ExceptionInfo object (contains traceback)
        """
        logger.error(
            "task_dead_letter",
            task_name=self.name,
            task_id=task_id,
            args=args,
            kwargs=kwargs,
            error=str(exc),
            error_type=type(exc).__name__,
            traceback=str(einfo),
        )
        
        # Call the parent to preserve default behavior
        super().on_failure(exc, task_id, args, kwargs, einfo)
```

**Using the custom base:**

```python
# tasks.py
from base_task import AlertingTask

@celery_app.task(
    base=AlertingTask,    # â† Use our custom base
    bind=True,
    max_retries=3,
    autoretry_for=(ConnectionError, TimeoutError),
    retry_backoff=True,
)
def sync_user_data(self, user_id: int) -> dict:
    response = httpx.get(
        f"https://api.example.com/users/{user_id}",
        timeout=10,
    )
    response.raise_for_status()
    return response.json()


# Now when sync_user_data fails after 3 retries,
# AlertingTask.on_failure fires automatically.
# Every task using base=AlertingTask gets this behavior.
```

**What `on_failure` gives you:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              on_failure PARAMETERS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  exc      The exception instance.                               â”‚
â”‚           You can check its type to decide what to do.          â”‚
â”‚           isinstance(exc, NotFoundError) â†’ different from       â”‚
â”‚           isinstance(exc, ConnectionError)                      â”‚
â”‚                                                                 â”‚
â”‚  task_id  The unique ID of this specific execution.             â”‚
â”‚           "abc123-def456-..."                                    â”‚
â”‚           Useful for tracing: "which run failed?"               â”‚
â”‚                                                                 â”‚
â”‚  args     The positional args the task was called with.         â”‚
â”‚           sync_user_data.delay(42) â†’ args = (42,)              â”‚
â”‚           You know WHAT input caused the failure.               â”‚
â”‚                                                                 â”‚
â”‚  kwargs   The keyword args. Same idea.                          â”‚
â”‚                                                                 â”‚
â”‚  einfo    Contains the full traceback.                          â”‚
â”‚           str(einfo) gives you the formatted traceback          â”‚
â”‚           string â€” same as what you'd see in the terminal.      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.4 Building a Dead Letter Store

**Logging is not enough. Logs scroll off the screen and get rotated. For tasks where failure matters, store the dead letter in a place humans can review it.**

```python
# dead_letters.py
"""
Dead letter storage.

A dead letter record contains everything needed to understand
what failed and to retry it manually later.
"""

from datetime import datetime
from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy.dialects.postgresql import JSONB
from database import Base  # Your existing SQLAlchemy Base


class DeadLetter(Base):
    """
    Stores tasks that failed permanently after all retries.
    
    This table is an inspection queue for humans and admin
    dashboards. When on-call engineers check the system each
    morning, they review this table.
    """
    __tablename__ = "dead_letters"

    id = Column(Integer, primary_key=True)
    task_name = Column(String, nullable=False, index=True)
    task_id = Column(String, nullable=False, unique=True)
    args = Column(JSONB, default=list)
    kwargs = Column(JSONB, default=dict)
    exception_type = Column(String, nullable=False)
    exception_message = Column(Text, nullable=False)
    traceback = Column(Text)
    failed_at = Column(DateTime, default=datetime.utcnow, index=True)
    resolved = Column(
        String, 
        default="pending",  # "pending", "retried", "ignored"
        index=True,
    )
```

**Now update the base task to store dead letters:**

```python
# base_task.py â€” extended
import structlog
from celery import Task
from datetime import datetime
from dead_letters import DeadLetter
from database import SyncSessionLocal  # Sync session â€” see note below

logger = structlog.get_logger()


class AlertingTask(Task):
    """
    Base task that stores dead letters in PostgreSQL
    when all retries are exhausted.
    """
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        # â”€â”€ 1. LOG IT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.error(
            "task_dead_letter",
            task_name=self.name,
            task_id=task_id,
            args=args,
            error_type=type(exc).__name__,
            error=str(exc),
        )
        
        # â”€â”€ 2. STORE IT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # NOTE: Celery workers can use sync DB sessions.
        # The worker is already a separate process â€”
        # it's not your async FastAPI event loop.
        try:
            session = SyncSessionLocal()
            dead_letter = DeadLetter(
                task_name=self.name,
                task_id=task_id,
                args=list(args) if args else [],
                kwargs=dict(kwargs) if kwargs else {},
                exception_type=type(exc).__name__,
                exception_message=str(exc),
                traceback=str(einfo),
                failed_at=datetime.utcnow(),
            )
            session.add(dead_letter)
            session.commit()
        except Exception as store_exc:
            # If even storing the dead letter fails, 
            # at least the log entry above exists.
            logger.error(
                "dead_letter_store_failed",
                original_task=self.name,
                store_error=str(store_exc),
            )
        finally:
            session.close()
        
        # â”€â”€ 3. CALL PARENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        super().on_failure(exc, task_id, args, kwargs, einfo)
```

**Why we use a sync session in the worker (the note above):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SYNC vs ASYNC IN CELERY WORKERS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Your FastAPI app uses ASYNC SQLAlchemy (AsyncSession)          â”‚
â”‚  because FastAPI runs in an async event loop.                   â”‚
â”‚                                                                 â”‚
â”‚  Celery workers are SEPARATE PROCESSES with their own           â”‚
â”‚  runtime. The on_failure hook runs in a synchronous context     â”‚
â”‚  inside the worker. Using sync SQLAlchemy here is correct       â”‚
â”‚  and simpler.                                                   â”‚
â”‚                                                                 â”‚
â”‚  FastAPI process â†’ AsyncSession (async event loop)              â”‚
â”‚  Celery worker   â†’ SyncSession (separate process, no loop)     â”‚
â”‚                                                                 â”‚
â”‚  You can use async in Celery workers (it's possible), but       â”‚
â”‚  for hooks like on_failure that do a simple DB insert,          â”‚
â”‚  sync is easier and has no drawbacks.                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The full lifecycle with dead letter handling:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            TASK LIFECYCLE WITH DEAD LETTERS                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Task called                                                    â”‚
â”‚     â”‚                                                           â”‚
â”‚     â–¼                                                           â”‚
â”‚  Attempt 1 â”€â”€ fails â”€â”€ autoretry (wait 1s)                      â”‚
â”‚     â”‚                                                           â”‚
â”‚     â–¼                                                           â”‚
â”‚  Attempt 2 â”€â”€ fails â”€â”€ autoretry (wait 2s)                      â”‚
â”‚     â”‚                                                           â”‚
â”‚     â–¼                                                           â”‚
â”‚  Attempt 3 â”€â”€ fails â”€â”€ autoretry (wait 4s)                      â”‚
â”‚     â”‚                                                           â”‚
â”‚     â–¼                                                           â”‚
â”‚  Attempt 4 â”€â”€ fails â”€â”€ max_retries reached!                     â”‚
â”‚     â”‚                                                           â”‚
â”‚     â”œâ”€â”€â”€â–¶ on_failure() fires                                    â”‚
â”‚     â”‚     â”œâ”€ Log error with structlog                           â”‚
â”‚     â”‚     â”œâ”€ Store DeadLetter in PostgreSQL                     â”‚
â”‚     â”‚     â””â”€ (Later: send alert â€” Part 6)                       â”‚
â”‚     â”‚                                                           â”‚
â”‚     â–¼                                                           â”‚
â”‚  Task state = FAILURE in result backend                         â”‚
â”‚  Worker moves on to next task                                   â”‚
â”‚                                                                 â”‚
â”‚  LATER: Human reviews dead_letters table                        â”‚
â”‚     â”œâ”€ Finds the record                                         â”‚
â”‚     â”œâ”€ Investigates: "Why did user_id=99999 fail?"              â”‚
â”‚     â”œâ”€ Fix: "That user was deleted from external system"        â”‚
â”‚     â””â”€ Mark as "resolved" or manually re-queue                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 5: MONITORING WITH FLOWER

## 5.1 Why Monitoring Matters

> "You now have Beat scheduling tasks, timeouts protecting workers, and dead letter handling catching permanent failures. But imagine managing all of this by reading raw log output in five terminal windows. At 3 AM. After a page wakes you up."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             MONITORING = THE CONTROL ROOM                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Without monitoring, you're a factory owner who locked          â”‚
â”‚  the control room door and threw away the key.                  â”‚
â”‚                                                                 â”‚
â”‚  You can hear the machines running.                             â”‚
â”‚  You can't see which ones.                                      â”‚
â”‚  You can't see the queue of raw materials piling up.            â”‚
â”‚  You can't see the reject bin overflowing.                      â”‚
â”‚  You find out something broke when customers complain.          â”‚
â”‚                                                                 â”‚
â”‚  QUESTIONS MONITORING ANSWERS:                                  â”‚
â”‚  â”œâ”€ How many tasks ran in the last hour? Succeeded? Failed?     â”‚
â”‚  â”œâ”€ Are any workers down? Overloaded?                           â”‚
â”‚  â”œâ”€ How big is the queue? Are tasks piling up?                  â”‚
â”‚  â”œâ”€ Which task is taking the longest?                           â”‚
â”‚  â”œâ”€ What was the error for task abc-123?                        â”‚
â”‚  â””â”€ Is Beat sending tasks on schedule?                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.2 Flower Setup

**Flower is a real-time web UI for Celery. It connects to the same broker your workers use and displays everything that is happening.**

```bash
# Install (add to your requirements.txt / pyproject.toml)
pip install flower

# Run it (separate terminal)
celery -A celery_app flower --port=5555

# That's it. Open http://localhost:5555 in your browser.
```

**How Flower connects:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FLOWER ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                    â”‚   FLOWER      â”‚                            â”‚
â”‚                    â”‚  :5555        â”‚                            â”‚
â”‚                    â”‚ (web UI)      â”‚                            â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                            â”‚ reads events                      â”‚
â”‚                            â–¼                                    â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚   BEAT   â”‚â”€â”€â”€â”€â–¶â”‚   REDIS   â”‚â—€â”€â”€â”€â”€â”‚ WORKER 1 â”‚             â”‚
â”‚    â”‚(schedulerâ”‚     â”‚  (broker) â”‚     â”‚          â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚           â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                     â”‚           â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚                     â”‚           â”‚â—€â”€â”€â”€â”€â”‚ WORKER 2 â”‚             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚          â”‚             â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                 â”‚
â”‚  Flower does NOT intercept or modify task execution.            â”‚
â”‚  It is read-only. It subscribes to Celery's event stream       â”‚
â”‚  and displays the data in a web interface.                      â”‚
â”‚  Adding or removing Flower has zero effect on your workers.     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**To get full event data, enable events on your workers:**

```bash
# Start worker with events enabled
celery -A celery_app worker --loglevel=info -E

# The -E flag (or --events) tells the worker to broadcast
# lifecycle events: task-sent, task-received, task-started,
# task-succeeded, task-failed, task-retried, etc.
# Flower listens for these events.
```

---

## 5.3 Reading the Dashboard

**Flower's web interface has several pages. Here's what each one shows and why you care:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FLOWER DASHBOARD LAYOUT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ğŸ  Dashboard        Workers   Tasks   Broker   Monitor   â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  Active Tasks: 3          Processed: 1,247                â”‚  â”‚
â”‚  â”‚  Reserved: 12             Failed: 23                      â”‚  â”‚
â”‚  â”‚  Workers: 2 online        Success Rate: 98.2%             â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  PAGE: Workers                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Shows each worker process:                                     â”‚
â”‚  â€¢ Name, status (online/offline)                                â”‚
â”‚  â€¢ Concurrency (how many task slots)                            â”‚
â”‚  â€¢ Active tasks (what it's doing right now)                     â”‚
â”‚  â€¢ Tasks completed (total count)                                â”‚
â”‚  â€¢ Load average                                                 â”‚
â”‚                                                                 â”‚
â”‚  WHY YOU CARE: If a worker disappears from this list,           â”‚
â”‚  it crashed. If active tasks = concurrency, it's at capacity.   â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  PAGE: Tasks                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚  Shows individual task executions:                              â”‚
â”‚  â€¢ Task name, ID, state (PENDING, STARTED, SUCCESS, FAILURE)   â”‚
â”‚  â€¢ Arguments it was called with                                 â”‚
â”‚  â€¢ Worker that executed it                                      â”‚
â”‚  â€¢ Start time, runtime duration                                 â”‚
â”‚  â€¢ Exception + traceback (for failed tasks)                     â”‚
â”‚  â€¢ Result (for successful tasks)                                â”‚
â”‚                                                                 â”‚
â”‚  WHY YOU CARE: When something breaks, you open this page,       â”‚
â”‚  filter by FAILURE, and read the traceback. Faster than         â”‚
â”‚  digging through log files.                                     â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  PAGE: Broker                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  Shows the Redis queue status:                                  â”‚
â”‚  â€¢ Queue names and message counts                               â”‚
â”‚  â€¢ Messages in each queue (backlog size)                        â”‚
â”‚                                                                 â”‚
â”‚  WHY YOU CARE: If the "celery" queue shows 5,000 pending        â”‚
â”‚  messages and it's growing, tasks are arriving faster than       â”‚
â”‚  workers can process them. You need more workers or faster       â”‚
â”‚  tasks.                                                         â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  PAGE: Monitor                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Real-time graphs:                                              â”‚
â”‚  â€¢ Tasks succeeded/failed over time                             â”‚
â”‚  â€¢ Task execution times                                         â”‚
â”‚  â€¢ Queue length over time                                       â”‚
â”‚                                                                 â”‚
â”‚  WHY YOU CARE: Trends. A slowly growing failure rate means       â”‚
â”‚  a degrading external dependency. A sudden spike in queue       â”‚
â”‚  length means something triggered a flood of tasks.             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What to look for â€” the key indicators:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FLOWER: WHAT HEALTHY LOOKS LIKE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  âœ… HEALTHY:                                                    â”‚
â”‚  â€¢ All workers show "online"                                    â”‚
â”‚  â€¢ Active tasks < concurrency (workers have spare capacity)     â”‚
â”‚  â€¢ Queue size near 0 or stable (not growing)                    â”‚
â”‚  â€¢ Failure rate < 2-5%                                          â”‚
â”‚  â€¢ Task durations consistent (no sudden spikes)                 â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  âš ï¸  WARNING SIGNS:                                             â”‚
â”‚  â€¢ Queue size growing steadily â†’ tasks arriving too fast        â”‚
â”‚  â€¢ Active tasks = concurrency on all workers â†’ at capacity      â”‚
â”‚  â€¢ Failure rate rising â†’ external dependency degrading          â”‚
â”‚  â€¢ One task type dominating active list â†’ might be hanging      â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  ğŸ”´ SOMETHING IS WRONG:                                         â”‚
â”‚  â€¢ Worker disappeared â†’ process crashed                         â”‚
â”‚  â€¢ Queue at 10,000+ and growing â†’ backlog crisis                â”‚
â”‚  â€¢ 100% failure rate on one task â†’ poison message or            â”‚
â”‚    complete outage of dependency                                â”‚
â”‚  â€¢ Same task_id active for hours â†’ stuck (no timeout set!)      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.4 Worker Inspection from Code

**You don't always need the Flower UI. Celery provides an inspect API for programmatic checks. This is useful for health endpoints in your FastAPI application.**

```python
# worker_inspection.py
"""
Programmatic inspection of Celery workers.
Use this in health check endpoints and alerting scripts.
"""

from celery_app import celery_app


def get_worker_status() -> dict:
    """
    Check which workers are alive and what they're doing.
    
    This sends a broadcast message to all workers via the
    broker and collects their responses. It has a timeout â€”
    if a worker doesn't respond, it's probably dead.
    """
    inspector = celery_app.control.inspect(timeout=5.0)
    
    # â”€â”€ Who's online? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ping_responses = inspector.ping()
    # Returns: {'worker1@host': {'ok': 'pong'}, ...}
    # or None if no workers respond
    
    if ping_responses is None:
        return {
            "status": "critical",
            "workers_online": 0,
            "detail": "No workers responded to ping",
        }
    
    # â”€â”€ What's running right now? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    active_tasks = inspector.active()
    # Returns: {'worker1@host': [{'id': '...', 'name': '...', ...}]}
    
    # â”€â”€ What's waiting in each worker's local buffer? â”€â”€
    reserved_tasks = inspector.reserved()
    # Returns tasks that the worker has fetched from the
    # broker but hasn't started executing yet.
    
    # â”€â”€ Build summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    workers = list(ping_responses.keys())
    total_active = sum(
        len(tasks) for tasks in (active_tasks or {}).values()
    )
    total_reserved = sum(
        len(tasks) for tasks in (reserved_tasks or {}).values()
    )
    
    return {
        "status": "healthy" if workers else "critical",
        "workers_online": len(workers),
        "workers": workers,
        "active_tasks": total_active,
        "reserved_tasks": total_reserved,
    }
```

**Hook this into your FastAPI application:**

```python
# routes/health.py
from fastapi import APIRouter, HTTPException
from worker_inspection import get_worker_status

router = APIRouter(tags=["health"])

@router.get("/health/workers")
async def check_workers():
    """
    Health check endpoint for Celery workers.
    
    Returns 200 if workers are online, 503 if not.
    Load balancers and uptime monitors hit this endpoint.
    """
    # NOTE: inspector.ping() is a blocking call.
    # In production, run this in a thread pool to avoid
    # blocking the async event loop.
    # For now, this is sufficient to learn the pattern.
    status = get_worker_status()
    
    if status["status"] != "healthy":
        raise HTTPException(
            status_code=503,
            detail=status,
        )
    
    return status
```

**What the endpoint returns:**

```
# Workers healthy:
GET /health/workers â†’ 200
{
    "status": "healthy",
    "workers_online": 2,
    "workers": ["worker1@hostname", "worker2@hostname"],
    "active_tasks": 3,
    "reserved_tasks": 7
}

# No workers:
GET /health/workers â†’ 503
{
    "status": "critical",
    "workers_online": 0,
    "detail": "No workers responded to ping"
}
```

---

# PART 6: ALERTING ON FAILURES

## 6.1 The Dashboard Nobody Watches

> "Flower is great. You have a beautiful dashboard. Graphs, tables, real-time updates. There's just one problem."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           THE DASHBOARD NOBODY WATCHES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Friday 5 PM:   You check Flower. Everything green. âœ…          â”‚
â”‚  Friday 8 PM:   External API changes their response format.    â”‚
â”‚  Friday 8:01 PM: Every parse_api_response task starts failing. â”‚
â”‚  Saturday:       Failures accumulate. Nobody's looking.         â”‚
â”‚  Sunday:         Queue backed up. Beat keeps sending tasks.     â”‚
â”‚  Monday 9 AM:    You open Flower. 4,000 failed tasks.          â”‚
â”‚                  Two days of data not synced.                   â”‚
â”‚                                                                 â”‚
â”‚  MONITORING without ALERTING is just a prettier log file.       â”‚
â”‚  The system must TELL YOU when something is wrong.              â”‚
â”‚  You should not have to remember to look.                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Factory analogy:**

> "A factory control room with screens and no alarms is a museum exhibit. The screens exist so that when the alarm goes off, the operator can look at them and figure out what happened. The alarm is the important part."

---

## 6.2 Celery Signals

**Celery emits signals at every stage of a task's lifecycle. You can attach handler functions to these signals â€” like attaching event listeners to DOM elements in JavaScript, but for task events.**

```python
# signals.py
"""
Celery signal handlers for alerting and observability.

Import this module in your celery_app.py to activate the handlers.
Signals fire automatically â€” you don't call these functions.
"""

import structlog
from celery.signals import (
    task_failure,
    task_success,
    task_retry,
    worker_ready,
    worker_shutting_down,
)

logger = structlog.get_logger()
```

**The task_failure signal â€” the most important one:**

```python
@task_failure.connect
def handle_task_failure(
    sender,       # The task class (not instance)
    task_id,      # Unique execution ID
    exception,    # The exception object
    args,         # Positional args the task was called with
    kwargs,       # Keyword args
    traceback,    # Traceback object
    einfo,        # ExceptionInfo with formatted traceback
    **kw,         # Always accept **kw for forward compatibility
):
    """
    Fires EVERY time a task fails â€” including each retry
    AND the final failure.
    
    Use this for centralized alerting across ALL tasks,
    not just tasks with base=AlertingTask.
    """
    logger.error(
        "task_failed",
        task_name=sender.name,
        task_id=task_id,
        error_type=type(exception).__name__,
        error=str(exception),
    )
    
    # You could send a Slack webhook, email, PagerDuty alert, etc.
    # Be careful: this fires on EVERY failure, including retries.
    # You probably don't want an alert for every retry attempt.
    # Filter by checking if retries are exhausted:
    
    # Only alert on FINAL failure (after all retries):
    task = sender
    request = kw.get("request", None)
    # Unfortunately, the signal doesn't directly tell you if
    # retries are exhausted. The on_failure hook (Part 4) is
    # better for final-failure-only alerting.
    # Use the signal for LOGGING; use on_failure for ALERTING.
```

**The difference between signals and on_failure:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SIGNAL (task_failure) vs HOOK (on_failure)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  task_failure SIGNAL:                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  â€¢ Fires on EVERY failure (including each retry attempt)        â”‚
â”‚  â€¢ Defined ONCE, applies to ALL tasks (centralized)             â”‚
â”‚  â€¢ Good for: logging, metrics, counters                         â”‚
â”‚  â€¢ Bad for: alerting (too noisy â€” fires on retries)             â”‚
â”‚                                                                 â”‚
â”‚  on_failure HOOK (from Part 4):                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  â€¢ Fires ONLY on final failure (after all retries exhausted)    â”‚
â”‚  â€¢ Defined per task class (via base=AlertingTask)               â”‚
â”‚  â€¢ Good for: alerting, dead letter storage                      â”‚
â”‚  â€¢ You choose which tasks get this behavior                     â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  RECOMMENDED PATTERN:                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Use task_failure signal for centralized LOGGING. â”‚           â”‚
â”‚  â”‚  Use on_failure hook for targeted ALERTING.       â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Other useful signals:**

```python
@task_retry.connect
def handle_task_retry(sender, request, reason, einfo, **kw):
    """
    Fires when a task is about to be retried.
    Useful for tracking retry rates.
    """
    logger.warning(
        "task_retrying",
        task_name=sender.name,
        task_id=request.id,
        retry_reason=str(reason),
    )


@worker_ready.connect
def handle_worker_ready(sender, **kw):
    """
    Fires when a worker has finished starting up and is 
    ready to accept tasks. Useful for deployment verification.
    """
    logger.info("worker_online", worker=str(sender))


@worker_shutting_down.connect
def handle_worker_shutdown(sender, sig, how, exitcode, **kw):
    """
    Fires when a worker begins shutting down.
    'how' is 'warm' (finish current tasks) or 'cold' (immediate).
    """
    logger.info(
        "worker_shutting_down",
        worker=str(sender),
        signal=sig,
        how=how,
    )
```

**Activating signals â€” make sure they're imported:**

```python
# celery_app.py â€” add this at the bottom

# Import signals module so handlers are registered.
# If you don't import it, the @signal.connect decorators
# never execute, and no handlers are attached.
import signals  # noqa: F401  (imported for side effect)
```

---

## 6.3 Health Check Endpoints

**Combine the worker inspection (Part 5) with queue depth checking for a comprehensive health endpoint:**

```python
# routes/health.py â€” extended
from fastapi import APIRouter, HTTPException
from celery_app import celery_app
from worker_inspection import get_worker_status

router = APIRouter(tags=["health"])


@router.get("/health/background")
async def background_system_health():
    """
    Comprehensive health check for the background task system.
    Checks: workers alive, queue depth, recent failure rate.
    """
    report = {}
    
    # â”€â”€ 1. Are workers running? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    worker_status = get_worker_status()
    report["workers"] = worker_status
    
    # â”€â”€ 2. How deep is the queue? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Use the broker connection to check queue length
    with celery_app.connection_or_acquire() as conn:
        queue_length = conn.default_channel.queue_declare(
            queue="celery", passive=True
        ).message_count
    
    report["queue_depth"] = queue_length
    
    # â”€â”€ 3. Determine overall status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if worker_status["workers_online"] == 0:
        report["status"] = "critical"
        report["reason"] = "No workers online"
        raise HTTPException(status_code=503, detail=report)
    
    if queue_length > 1000:
        report["status"] = "degraded"
        report["reason"] = f"Queue backlog: {queue_length} tasks"
        # Return 200 still â€” workers are running, just busy
    else:
        report["status"] = "healthy"
    
    return report
```

**What the health check tells you:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HEALTH CHECK RESPONSES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  HEALTHY â€” everything normal                                    â”‚
â”‚  {"status": "healthy", "workers": {"workers_online": 2, ...},  â”‚
â”‚   "queue_depth": 3}                                             â”‚
â”‚                                                                 â”‚
â”‚  DEGRADED â€” working but stressed                                â”‚
â”‚  {"status": "degraded", "reason": "Queue backlog: 2341 tasks", â”‚
â”‚   "workers": {"workers_online": 2, ...}, "queue_depth": 2341}  â”‚
â”‚                                                                 â”‚
â”‚  CRITICAL â€” broken, return 503                                  â”‚
â”‚  {"status": "critical", "reason": "No workers online",         â”‚
â”‚   "workers": {"workers_online": 0, ...}, "queue_depth": 8422}  â”‚
â”‚                                                                 â”‚
â”‚  An uptime monitor (or load balancer) hits this endpoint        â”‚
â”‚  periodically. If it gets 503, it pages the on-call engineer.   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6.4 Common Mistakes

### Mistake 1: Running multiple Beat instances

```
# âŒ WRONG: Starting 3 workers with --beat
celery -A celery_app worker --beat --concurrency=4  # instance 1
celery -A celery_app worker --beat --concurrency=4  # instance 2
celery -A celery_app worker --beat --concurrency=4  # instance 3

# Result: every scheduled task runs 3 times.
# Triple the database writes, triple the API calls,
# triple the emails sent to customers.

# âœ… CORRECT: One dedicated Beat, scale workers separately
celery -A celery_app beat                            # ONE Beat
celery -A celery_app worker --concurrency=4          # Worker 1
celery -A celery_app worker --concurrency=4          # Worker 2
celery -A celery_app worker --concurrency=4          # Worker 3
```

---

### Mistake 2: No timeouts anywhere

```python
# âŒ WRONG: No timeout set on a task that calls an external API
@celery_app.task
def sync_all_users():
    for user in get_all_users():      # 10,000 users
        httpx.get(f"https://api.example.com/users/{user.id}")
        # If the API hangs, this task runs FOREVER.
        # It holds a worker slot FOREVER.
        # Beat sends a new sync_all_users every day.
        # Now TWO tasks are hanging. Then three. Then four.
        # All worker slots consumed. System frozen.

# âœ… CORRECT: Always set timeouts
@celery_app.task(soft_time_limit=1800, time_limit=1860)
def sync_all_users():
    for user in get_all_users():
        httpx.get(
            f"https://api.example.com/users/{user.id}",
            timeout=10,  # HTTP timeout too
        )
```

---

### Mistake 3: Alerting on every retry

```python
# âŒ WRONG: Send a Slack message on every failure signal
@task_failure.connect
def alert_on_failure(sender, task_id, exception, **kw):
    send_slack_message(f"ğŸš¨ Task {sender.name} failed: {exception}")
    
# If a task retries 3 times before succeeding, you get 3 Slack
# messages for something that ultimately WORKED.
# 100 tasks Ã— 3 retries = 300 Slack messages per hour.
# Your team mutes the channel. Now real alerts are invisible.
# This is called ALERT FATIGUE.

# âœ… CORRECT: Alert only on FINAL failure (using on_failure hook)
class AlertingTask(Task):
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        # This only fires once: after all retries are exhausted.
        send_slack_message(
            f"ğŸš¨ DEAD LETTER: {self.name} [{task_id}] "
            f"failed permanently: {exc}"
        )
```

---

### Mistake 4: Using the schedule state file in Docker without a volume

```bash
# âŒ WRONG: Beat in Docker without persistent storage
# Beat creates celerybeat-schedule inside the container.
# When the container restarts, the file is gone.
# Beat thinks it has never run â†’ fires ALL tasks immediately.

# âœ… CORRECT: Mount the schedule file to a volume
# In your docker-compose.yml (preview â€” Week 15):
# volumes:
#   - beat-schedule:/app/celerybeat-schedule
```

---

### Mistake 5: Forgetting timezone on crontab

```python
# âŒ WRONG: No timezone set
celery_app.conf.beat_schedule = {
    "daily-report": {
        "task": "tasks.generate_daily_report",
        "schedule": crontab(minute=0, hour=0),  # Midnight... which timezone?
    },
}
# On your laptop: midnight EST. On production: midnight UTC.
# That's a 5-hour difference.

# âœ… CORRECT: Explicit timezone
celery_app.conf.timezone = "UTC"
# Now crontab(minute=0, hour=0) ALWAYS means midnight UTC.
# Predictable. Deployable. Debuggable.
```

---

# Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SCHEDULING & MONITORING QUICK REFERENCE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  CELERY BEAT CONFIG:                                            â”‚
â”‚      celery_app.conf.beat_schedule = {                          â”‚
â”‚          "name": {                                              â”‚
â”‚              "task": "tasks.my_task",                           â”‚
â”‚              "schedule": crontab(minute=0, hour="*/3"),         â”‚
â”‚          },                                                     â”‚
â”‚      }                                                          â”‚
â”‚      celery_app.conf.timezone = "UTC"                           â”‚
â”‚                                                                 â”‚
â”‚  SCHEDULE TYPES:                                                â”‚
â”‚      timedelta(minutes=5)            Every 5 minutes            â”‚
â”‚      crontab(minute=0)              Every hour at :00           â”‚
â”‚      crontab(minute=0, hour=0)      Daily at midnight           â”‚
â”‚      crontab(minute=0, hour=9,      Mondays at 9 AM            â”‚
â”‚              day_of_week="mon")                                 â”‚
â”‚      crontab(minute="*/15")         Every 15 minutes            â”‚
â”‚                                                                 â”‚
â”‚  RUN BEAT:                                                      â”‚
â”‚      celery -A celery_app beat --loglevel=info                  â”‚
â”‚      âš ï¸  ONE Beat instance only. Ever.                          â”‚
â”‚                                                                 â”‚
â”‚  TIMEOUTS:                                                      â”‚
â”‚      @celery_app.task(soft_time_limit=120, time_limit=150)      â”‚
â”‚      soft â†’ raises SoftTimeLimitExceeded (catchable)            â”‚
â”‚      hard â†’ SIGKILL (not catchable, process killed)             â”‚
â”‚                                                                 â”‚
â”‚  DEAD LETTERS:                                                  â”‚
â”‚      class AlertingTask(Task):                                  â”‚
â”‚          def on_failure(self, exc, task_id, args, kwargs, ...): â”‚
â”‚              # Store in DB, send alert                          â”‚
â”‚                                                                 â”‚
â”‚  FLOWER:                                                        â”‚
â”‚      pip install flower                                         â”‚
â”‚      celery -A celery_app flower --port=5555                    â”‚
â”‚      celery -A celery_app worker -E  (enable events)            â”‚
â”‚                                                                 â”‚
â”‚  SIGNALS:                                                       â”‚
â”‚      @task_failure.connect     â†’ log all failures               â”‚
â”‚      @task_retry.connect       â†’ track retry rates              â”‚
â”‚      @worker_ready.connect     â†’ deployment verification        â”‚
â”‚                                                                 â”‚
â”‚  HEALTH CHECK:                                                  â”‚
â”‚      inspector = celery_app.control.inspect()                   â”‚
â”‚      inspector.ping()          â†’ which workers are alive        â”‚
â”‚      inspector.active()        â†’ what's running now             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Summary: The Key Mental Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  RELIABLE BACKGROUND SYSTEMS = SCHEDULE + PROTECT +             â”‚
â”‚                                OBSERVE + REACT                  â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ SCHEDULE â”‚   â”‚ PROTECT  â”‚   â”‚ OBSERVE  â”‚   â”‚  REACT   â”‚    â”‚
â”‚  â”‚          â”‚   â”‚          â”‚   â”‚          â”‚   â”‚          â”‚    â”‚
â”‚  â”‚  Celery  â”‚â”€â”€â–¶â”‚ Timeouts â”‚â”€â”€â–¶â”‚  Flower  â”‚â”€â”€â–¶â”‚  Alerts  â”‚    â”‚
â”‚  â”‚   Beat   â”‚   â”‚ soft/hardâ”‚   â”‚ + Health â”‚   â”‚ Signals  â”‚    â”‚
â”‚  â”‚          â”‚   â”‚          â”‚   â”‚  Checks  â”‚   â”‚ on_fail  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚       "When"        "Stop it       "See it"      "Tell me"     â”‚
â”‚                    if stuck"                                    â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  THE FACTORY FLOOR ANALOGY:                                     â”‚
â”‚  â”œâ”€ Beat = Production schedule board read by the shift manager  â”‚
â”‚  â”œâ”€ soft_time_limit = Warning buzzer on the machine             â”‚
â”‚  â”œâ”€ time_limit = Emergency power cutoff                         â”‚
â”‚  â”œâ”€ Dead letters = Reject bin for parts that always fail QC     â”‚
â”‚  â”œâ”€ Flower = Control room with screens showing every machine    â”‚
â”‚  â””â”€ Signals + on_failure = Alarm system that pages the manager  â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  THE RULE:                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  If nobody is watching, nobody knows it broke.         â”‚     â”‚
â”‚  â”‚  If nobody is alerted, nobody fixes it.                â”‚     â”‚
â”‚  â”‚  Build systems that report their own health.           â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Connection to Upcoming Content

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHERE THIS LEADS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WEEK 11, LECTURE 4 (Next):                                     â”‚
â”‚  â””â”€ Event-Driven Architecture Introduction                      â”‚
â”‚     Redis Pub/Sub for broadcasting events.                      â”‚
â”‚     When to reach for Kafka vs Celery vs BackgroundTasks.       â”‚
â”‚     Your dead letter handling connects to event sourcing        â”‚
â”‚     concepts â€” the idea that you record what happened.          â”‚
â”‚                                                                 â”‚
â”‚  WEEK 11 PROJECT:                                               â”‚
â”‚  â””â”€ Background Processing Pipeline                              â”‚
â”‚     You'll USE Beat for scheduled jobs, timeouts to protect     â”‚
â”‚     workers, Flower to verify everything works, and alerting    â”‚
â”‚     hooks to catch failures. This lecture gives you the tools.  â”‚
â”‚                                                                 â”‚
â”‚  WEEK 12 (Performance):                                         â”‚
â”‚  â””â”€ Load testing with locust                                    â”‚
â”‚     The health check endpoints you built today become           â”‚
â”‚     part of your load testing strategy â€” verify background      â”‚
â”‚     system health under load.                                   â”‚
â”‚                                                                 â”‚
â”‚  WEEK 13-14 (Capstone):                                         â”‚
â”‚  â””â”€ Your SaaS platform will have scheduled tasks:               â”‚
â”‚     Report generation, data cleanup, notification digests.      â”‚
â”‚     Everything from today goes into production architecture.    â”‚
â”‚                                                                 â”‚
â”‚  WEEK 15 (Docker & CI/CD):                                      â”‚
â”‚  â””â”€ Beat, workers, and Flower each become their own service     â”‚
â”‚     in Docker Compose. The single-Beat rule becomes a           â”‚
â”‚     deployment architecture decision.                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```