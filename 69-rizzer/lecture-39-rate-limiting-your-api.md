# LECTURE DESIGN PHILOSOPHY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PEDAGOGICAL APPROACH                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  DESTRUCTION FIRST, PROTECTION SECOND                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚  Students must BREAK their own API before learning to defend    â”‚
â”‚  it. We hammer an unprotected endpoint and watch it suffer.     â”‚
â”‚                                                                 â”‚
â”‚  ANALOGY-DRIVEN                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  A nightclub bouncer analogy carries through the lecture.       â”‚
â”‚  Every algorithm maps to a bouncer counting strategy.           â”‚
â”‚                                                                 â”‚
â”‚  BOTH SIDES OF THE COIN                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  Week 8 taught RESPECTING rate limits as a client.              â”‚
â”‚  Now we ENFORCE them as a server. Full circle.                  â”‚
â”‚                                                                 â”‚
â”‚  NUMBERS DON'T LIE â€” BUT AVERAGES DO                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  Percentile latencies are taught through visceral examples.     â”‚
â”‚  Students learn why "average response time" is a trap.          â”‚
â”‚                                                                 â”‚
â”‚  CONNECT TO PRIOR KNOWLEDGE                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  Decorators (Week 1) â†’ @limiter.limit is a decorator           â”‚
â”‚  Redis (Week 10)     â†’ Distributed rate limit storage           â”‚
â”‚  Middleware (Week 9, 12-L3) â†’ Where the limiter lives           â”‚
â”‚  locust (Week 12-L3) â†’ Testing rate limits under load           â”‚
â”‚  Auth (Week 9)       â†’ Per-user limits, login protection        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#=# LECTURE OUTLINE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RATE LIMITING YOUR API                         â”‚
â”‚                   (3â€“4 Hour Lecture)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  PART 1: THE PROBLEM (35 min)                                   â”‚
â”‚  â”œâ”€ 1.1 The Unprotected API (Demonstration)                     â”‚
â”‚  â”œâ”€ 1.2 Three Reasons to Rate Limit                             â”‚
â”‚  â”œâ”€ 1.3 Who Gets Limited? (Identification)                      â”‚
â”‚  â””â”€ 1.4 The Nightclub Bouncer Analogy                           â”‚
â”‚                                                                 â”‚
â”‚  PART 2: THE ALGORITHMS (35 min)                                â”‚
â”‚  â”œâ”€ 2.1 Fixed Window Counter                                    â”‚
â”‚  â”œâ”€ 2.2 Sliding Window Log                                      â”‚
â”‚  â”œâ”€ 2.3 Token Bucket                                            â”‚
â”‚  â””â”€ 2.4 Choosing the Right Algorithm                            â”‚
â”‚                                                                 â”‚
â”‚  PART 3: THE IMPLEMENTATION (60 min)                            â”‚
â”‚  â”œâ”€ 3.1 slowapi Setup                                           â”‚
â”‚  â”œâ”€ 3.2 @limiter.limit() â€” Setting Limits                       â”‚
â”‚  â”œâ”€ 3.3 Key Functions â€” Identifying Clients                     â”‚
â”‚  â”œâ”€ 3.4 Per-Endpoint and Global Limits                          â”‚
â”‚  â”œâ”€ 3.5 Redis-Backed Rate Limiting (Connection to Week 10)      â”‚
â”‚  â””â”€ 3.6 Custom 429 Handler                                      â”‚
â”‚                                                                 â”‚
â”‚  PART 4: THE PROTOCOL (25 min)                                  â”‚
â”‚  â”œâ”€ 4.1 Rate Limit Headers (X-RateLimit-*)                      â”‚
â”‚  â”œâ”€ 4.2 429 Too Many Requests + Retry-After                     â”‚
â”‚  â””â”€ 4.3 Full Circle: Server â†” Client (Connection to Week 8)     â”‚
â”‚                                                                 â”‚
â”‚  PART 5: MEASURING IMPACT (45 min)                              â”‚
â”‚  â”œâ”€ 5.1 Why Averages Lie                                        â”‚
â”‚  â”œâ”€ 5.2 Percentile Latencies (p50, p95, p99)                    â”‚
â”‚  â”œâ”€ 5.3 Load Testing with Rate Limits (Connection to L3)        â”‚
â”‚  â”œâ”€ 5.4 Performance Regression Testing in CI                    â”‚
â”‚  â””â”€ 5.5 Setting Thresholds and Alerts                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 1: THE PROBLEM

## 1.1 The Unprotected API

**Start with destruction. Let them watch their API get hammered.**

Your Task Manager API now has authentication, caching, background jobs, and WebSocket notifications. It's feature-complete. It handles real users gracefully.

But it's *defenseless*.

```python
# the_hammer.py â€” What a malicious client (or a buggy script) looks like
import asyncio
import httpx
import time

async def hammer_api():
    """Send 200 requests as fast as possible."""
    async with httpx.AsyncClient(base_url="http://localhost:8000") as client:
        start = time.time()

        # Fire 200 requests concurrently â€” you learned this in Week 8
        tasks = [
            client.get(
                "/api/v1/tasks",
                headers={"Authorization": "Bearer <valid_token>"}
            )
            for _ in range(200)
        ]
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        elapsed = time.time() - start

        # Count what happened
        statuses: dict[int, int] = {}
        errors = 0
        for r in responses:
            if isinstance(r, Exception):
                errors += 1
            else:
                statuses[r.status_code] = statuses.get(r.status_code, 0) + 1

        print(f"200 requests in {elapsed:.1f}s")
        print(f"Status codes: {statuses}")
        print(f"Connection errors: {errors}")

asyncio.run(hammer_api())
```

**Run it. Watch the output.**

```
200 requests in 2.8s
Status codes: {200: 200}
Connection errors: 0
```

**Now ask the class:**

> "All 200 succeeded. Sounds great, right? So what's the problem?"

The problems are invisible â€” they're happening *behind* that 200 OK:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THE INVISIBLE DAMAGE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WHILE THOSE 200 REQUESTS WERE BEING SERVED:                    â”‚
â”‚                                                                 â”‚
â”‚  ğŸ’¾ Database connection pool: EXHAUSTED                          â”‚
â”‚     â””â”€ All 20 pool connections occupied by one client            â”‚
â”‚     â””â”€ Other users' requests QUEUING behind this flood           â”‚
â”‚                                                                 â”‚
â”‚  ğŸŒ External API quotas: BURNED                                  â”‚
â”‚     â””â”€ Your OpenWeatherMap free tier: 60 calls/min               â”‚
â”‚     â””â”€ This one client just consumed 200 calls                   â”‚
â”‚     â””â”€ Every other user gets "503 Service Unavailable"           â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“Š Celery workers: BURIED                                       â”‚
â”‚     â””â”€ 200 notification tasks queued simultaneously              â”‚
â”‚     â””â”€ Legitimate background jobs delayed by minutes             â”‚
â”‚                                                                 â”‚
â”‚  ğŸ”‹ Server resources: STRAINED                                   â”‚
â”‚     â””â”€ CPU spike â†’ OTHER users see slow responses                â”‚
â”‚     â””â”€ Memory spike â†’ OOM risk under sustained attack            â”‚
â”‚                                                                 â”‚
â”‚  ğŸ’¸ Your cloud bill: GROWING                                     â”‚
â”‚     â””â”€ Every request costs compute, bandwidth, DB time           â”‚
â”‚     â””â”€ Attacker pays nothing. YOU pay for their abuse.           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The punchline:**

> "You authenticated this user. You validated their input with Pydantic. You cached their queries with Redis. You did everything right â€” except *control how often they can knock on your door.*"

---

## 1.2 Three Reasons to Rate Limit

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHY RATE LIMIT?                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. FAIRNESS  ğŸ¤                                                 â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚     One client should not consume all your server's capacity.   â”‚
â”‚     100 users sharing an API deserve equal access.              â”‚
â”‚     Without limits, the fastest client wins and everyone        â”‚
â”‚     else starves.                                               â”‚
â”‚                                                                 â”‚
â”‚  2. PROTECTION  ğŸ›¡ï¸                                               â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚     Defense against:                                            â”‚
â”‚     â€¢ DDoS attacks (flood of requests to crash your server)     â”‚
â”‚     â€¢ Brute force (Week 9: we rate-limited login for this)      â”‚
â”‚     â€¢ Scrapers (bots harvesting your data)                      â”‚
â”‚     â€¢ Buggy clients (infinite loop sending requests)            â”‚
â”‚                                                                 â”‚
â”‚  3. COST CONTROL  ğŸ’°                                             â”‚
â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚     Every request costs:                                        â”‚
â”‚     â€¢ CPU cycles (your cloud bill)                              â”‚
â”‚     â€¢ Database connections (pooled, finite)                     â”‚
â”‚     â€¢ External API calls (someone else's rate limits!)          â”‚
â”‚     â€¢ Bandwidth (data transfer charges)                         â”‚
â”‚     Rate limiting caps your worst-case cost.                    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.3 Who Gets Limited?

**Before you can limit someone, you need to identify them.**

> "Think of it this way: if 200 requests arrive, how do you know if it's one person sending 200 or 200 people sending one each?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IDENTIFICATION STRATEGIES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  BY IP ADDRESS                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  Simplest. Works for anonymous endpoints.                       â”‚
â”‚  Problem: Users behind a shared NAT/VPN share one IP.           â”‚
â”‚  All of a company's employees look like one client.             â”‚
â”‚                                                                 â”‚
â”‚  BY AUTHENTICATED USER                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚  Use the JWT user ID (Week 9's get_current_user).               â”‚
â”‚  Most fair â€” limits follow the person, not the network.         â”‚
â”‚  Problem: Doesn't protect unauthenticated endpoints.            â”‚
â”‚                                                                 â”‚
â”‚  BY API KEY                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚  Common for third-party API providers.                          â”‚
â”‚  Each developer/app gets a key with its own quota.              â”‚
â”‚  Problem: Keys can be shared or stolen.                         â”‚
â”‚                                                                 â”‚
â”‚  COMBINED (Production reality)                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  IP for anonymous endpoints (login, register)                   â”‚
â”‚  User ID for authenticated endpoints                            â”‚
â”‚  API key for third-party integrations                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This concept of identifying the client is called a **key function** â€” you'll implement it in Part 3.

---

## 1.4 The Nightclub Bouncer Analogy

**This analogy will carry us through the entire lecture.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THE NIGHTCLUB BOUNCER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WITHOUT A BOUNCER (No rate limiting)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚                                                                 â”‚
â”‚  Everyone rushes the door at once.                              â”‚
â”‚  Club is overcrowded. Music lags. Drinks run out.               â”‚
â”‚  Fire code violated. EVERYONE has a terrible time.              â”‚
â”‚  One group of 50 friends ruins the night for 200 others.        â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  WITH A BOUNCER (Rate limiting)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚                                                                 â”‚
â”‚  Bouncer stands at the door.                                    â”‚
â”‚  Counts people entering. "10 per minute, that's the rule."      â”‚
â”‚  If you're over the limit: "Sorry, come back in 5 minutes."     â”‚
â”‚  Club stays comfortable. Music is crisp. Everyone has fun.      â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  THE MAPPING:                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚                                                                 â”‚
â”‚  Nightclub                â”‚  Your API                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚  Bouncer                  â”‚  Rate limiter middleware             â”‚
â”‚  Club capacity            â”‚  Server resources                   â”‚
â”‚  Patrons arriving         â”‚  Incoming requests                  â”‚
â”‚  Counting heads           â”‚  Tracking request count             â”‚
â”‚  "Come back later"        â”‚  429 Too Many Requests              â”‚
â”‚  VIP wristband (skip)     â”‚  Admin role â†’ higher limits         â”‚
â”‚  Checking ID at door      â”‚  Key function (IP / user ID)        â”‚
â”‚  Different rooms have     â”‚  Different endpoints have           â”‚
â”‚    different capacities   â”‚    different rate limits             â”‚
â”‚  Hourly head-count reset  â”‚  Time window reset                  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# PART 2: THE ALGORITHMS

**The bouncer needs a *strategy* for counting. There are three main ones.**

## 2.1 Fixed Window Counter

**The simplest approach: count requests in fixed time chunks.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FIXED WINDOW COUNTER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Rule: "10 requests per minute"                                 â”‚
â”‚                                                                 â”‚
â”‚  The bouncer resets his count at the top of every minute:       â”‚
â”‚                                                                 â”‚
â”‚  12:00:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 12:01:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 12:02:00   â”‚
â”‚  â”‚      Window 1          â”‚      Window 2          â”‚            â”‚
â”‚  â”‚                        â”‚                        â”‚            â”‚
â”‚  â”‚  Counter: 0â†’1â†’2...â†’10  â”‚  Counter: 0â†’1â†’2...     â”‚            â”‚
â”‚  â”‚  (resets at boundary)  â”‚  (resets at boundary)  â”‚            â”‚
â”‚                                                                 â”‚
â”‚  HOW IT WORKS:                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  1. Request arrives                                             â”‚
â”‚  2. Look up counter for current window                          â”‚
â”‚  3. If counter < limit â†’ allow, increment counter               â”‚
â”‚  4. If counter >= limit â†’ reject with 429                       â”‚
â”‚  5. Counter resets when new window begins                       â”‚
â”‚                                                                 â”‚
â”‚  PROS:                        CONS:                             â”‚
â”‚  â€¢ Dead simple                â€¢ Boundary burst problem          â”‚
â”‚  â€¢ Low memory (one counter)   â€¢ Uneven distribution             â”‚
â”‚  â€¢ Fast (single increment)    â€¢ (see below)                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The boundary burst problem â€” the fatal flaw of fixed windows:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THE BOUNDARY BURST PROBLEM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Rule: 10 requests per minute                                   â”‚
â”‚                                                                 â”‚
â”‚      Window 1                      Window 2                     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  12:00             12:00:59    12:01:00             12:01:59    â”‚
â”‚                                                                 â”‚
â”‚  A clever (or unlucky) client sends:                            â”‚
â”‚                                                                 â”‚
â”‚  â€¢ 0 requests from 12:00:00 â€“ 12:00:29                          â”‚
â”‚  â€¢ 10 requests from 12:00:30 â€“ 12:00:59  â† Within limit âœ…      â”‚
â”‚  â€¢ 10 requests from 12:01:00 â€“ 12:01:30  â† Within limit âœ…      â”‚
â”‚  â€¢ 0 requests from 12:01:30 â€“ 12:01:59                          â”‚
â”‚                                                                 â”‚
â”‚  Result: 20 requests in 60 seconds!                             â”‚
â”‚          DOUBLE the intended rate!  ğŸ˜±                           â”‚
â”‚                                                                 â”‚
â”‚  Both bursts happen right at the window boundary.               â”‚
â”‚  Each window individually sees only 10 requests.                â”‚
â”‚  But the actual throughput across the boundary is 2x.           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "The bouncer resets his counter at midnight on the dot. A group of 10 rushes in at 11:59 PM. Counter resets. The same group's friends â€” another 10 â€” walk in at 12:01 AM. Both batches are 'within limits.' But 20 people entered in 2 minutes."

---

## 2.2 Sliding Window Log

**Instead of fixed boundaries, track the actual timestamps.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SLIDING WINDOW LOG                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Rule: "10 requests per minute"                                 â”‚
â”‚                                                                 â”‚
â”‚  The bouncer writes down the EXACT TIME each person enters:     â”‚
â”‚                                                                 â”‚
â”‚  Log: [12:00:15, 12:00:22, 12:00:31, 12:00:45, ...]            â”‚
â”‚                                                                 â”‚
â”‚  When a new request arrives at 12:01:10:                        â”‚
â”‚  1. Look back exactly 60 seconds: window is 12:00:10â€“12:01:10  â”‚
â”‚  2. Count entries in that window                                â”‚
â”‚  3. Remove entries older than 12:00:10 (they've expired)        â”‚
â”‚  4. If count < 10 â†’ allow, add timestamp to log                 â”‚
â”‚  5. If count >= 10 â†’ reject with 429                            â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  VISUALIZATION:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  Time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶          â”‚
â”‚        12:00:10                           12:01:10              â”‚
â”‚            â”‚â—„â”€â”€â”€â”€ Sliding 60-second window â”€â”€â”€â”€â–ºâ”‚               â”‚
â”‚            â”‚                                    â”‚               â”‚
â”‚            â”‚  [x] [x] [x] [x] [x] [x] [x]     â”‚               â”‚
â”‚            â”‚  7 requests in window â†’ ALLOW      â”‚               â”‚
â”‚            â”‚                                    â”‚               â”‚
â”‚  Expired:  â”‚                                                    â”‚
â”‚  [x] [x]â”€â”€â”˜ (older than 60s, removed)                          â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  PROS:                          CONS:                           â”‚
â”‚  â€¢ Precise (no boundary burst)  â€¢ Higher memory (stores log)    â”‚
â”‚  â€¢ Smooth enforcement           â€¢ More expensive per check      â”‚
â”‚  â€¢ Accurate over any interval   â€¢ Log can grow large            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "This bouncer doesn't reset at midnight. He looks at his clipboard and counts everyone who entered in the *last 60 minutes from right now*. No gaming the boundary."

---

## 2.3 Token Bucket

**The most commonly used algorithm in production. Allows controlled bursts.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TOKEN BUCKET                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Rule: Bucket holds max 10 tokens. Refills 1 token per second.  â”‚
â”‚                                                                 â”‚
â”‚  The bouncer has a bowl of wristbands (tokens):                 â”‚
â”‚  â€¢ Every person who enters takes one wristband                  â”‚
â”‚  â€¢ New wristbands are added to the bowl at a steady rate        â”‚
â”‚  â€¢ If the bowl is empty: "Sorry, no wristbands. Wait."          â”‚
â”‚  â€¢ Bowl never overflows past maximum capacity                   â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  VISUALIZATION:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚                                                                 â”‚
â”‚  Time 0s:  [ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢]  10 tokens (full bucket)     â”‚
â”‚                                                                 â”‚
â”‚  Burst! 6 requests arrive at once:                              â”‚
â”‚  Time 1s:  [ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢â¬šâ¬šâ¬šâ¬šâ¬šâ¬š]   4 tokens left                  â”‚
â”‚                 All 6 served âœ…  (burst allowed!)                â”‚
â”‚                                                                 â”‚
â”‚  Time 2s:  [ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢â¬šâ¬šâ¬šâ¬šâ¬š]   5 tokens (+1 refilled)        â”‚
â”‚  Time 3s:  [ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢â¬šâ¬šâ¬šâ¬š]   6 tokens (+1 refilled)        â”‚
â”‚  ...                                                            â”‚
â”‚  Time 7s:  [ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢]  10 tokens (full again)      â”‚
â”‚                                                                 â”‚
â”‚  Another burst! 12 requests arrive:                             â”‚
â”‚  Time 8s:  [â¬šâ¬šâ¬šâ¬šâ¬šâ¬šâ¬šâ¬šâ¬šâ¬š]   0 tokens                         â”‚
â”‚                 10 served âœ…, 2 rejected 429 âŒ                   â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  KEY INSIGHT:                                                   â”‚
â”‚  Token bucket allows BURSTS up to the bucket size,              â”‚
â”‚  but SUSTAINED rate is limited to the refill rate.              â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  PROS:                          CONS:                           â”‚
â”‚  â€¢ Allows controlled bursts     â€¢ Slightly more complex logic   â”‚
â”‚  â€¢ Smooth long-term rate        â€¢ Two parameters to tune        â”‚
â”‚  â€¢ Industry standard            â”‚  (bucket size + refill rate)  â”‚
â”‚  â€¢ Memory efficient             â”‚                               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "This bouncer is the smartest one. He doesn't just count â€” he rations. You get a burst when you first arrive, but if you keep coming back too fast, you'll find the bowl empty."

---

## 2.4 Choosing the Right Algorithm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ALGORITHM DECISION FRAMEWORK                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                    Which algorithm?                              â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â–¼                                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚              â”‚ Do you need to      â”‚                            â”‚
â”‚              â”‚ allow short bursts? â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                   â”‚          â”‚                                  â”‚
â”‚                  YES         NO                                 â”‚
â”‚                   â”‚          â”‚                                  â”‚
â”‚                   â–¼          â–¼                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚ TOKEN BUCKET â”‚  â”‚ Does precision      â”‚               â”‚
â”‚         â”‚ âœ… Best for   â”‚  â”‚ matter more than    â”‚               â”‚
â”‚         â”‚ general APIs â”‚  â”‚ simplicity?          â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                â”‚          â”‚                     â”‚
â”‚                               YES         NO                    â”‚
â”‚                                â”‚          â”‚                     â”‚
â”‚                                â–¼          â–¼                     â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                     â”‚ SLIDING      â”‚ â”‚ FIXED        â”‚           â”‚
â”‚                     â”‚ WINDOW       â”‚ â”‚ WINDOW       â”‚           â”‚
â”‚                     â”‚ âœ… Precise    â”‚ â”‚ âœ… Simplest   â”‚           â”‚
â”‚                     â”‚ No boundary  â”‚ â”‚ Fast check   â”‚           â”‚
â”‚                     â”‚ exploits     â”‚ â”‚ Low memory   â”‚           â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                 â”‚
â”‚  IN PRACTICE:                                                   â”‚
â”‚  Token bucket is the most common default.                       â”‚
â”‚  slowapi uses fixed window internally (from the limits lib)     â”‚
â”‚  but the principle is the same for your purposes.               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Now ask the class:**

> "For a login endpoint that you want to protect from brute force, which algorithm would you prefer? What about a search endpoint that legitimate users might hit rapidly?"

---

# PART 3: THE IMPLEMENTATION

## 3.1 slowapi Setup

**slowapi is the standard rate limiting library for FastAPI.**

It's a rate limiting library for Starlette and FastAPI adapted from flask-limiter.[[3]](https://github.com/laurentS/slowapi) The actual rate limiting work is done by the `limits` library â€” slowapi is just a wrapper around it.[[2]](https://slowapi.readthedocs.io/)

```python
# Install it
# pip install slowapi
```

**Basic setup â€” 4 lines to protect your entire app:**

```python
# app/core/rate_limit.py
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Create the limiter â€” key_func decides HOW to identify clients
limiter = Limiter(key_func=get_remote_address)
```

```python
# app/main.py
from fastapi import FastAPI
from app.core.rate_limit import limiter
from slowapi.errors import RateLimitExceeded
from slowapi import _rate_limit_exceeded_handler

app = FastAPI()

# Two critical lines:
app.state.limiter = limiter                                         # 1. Store limiter in app state
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)  # 2. Handle 429s
```

**What each piece does:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SETUP BREAKDOWN                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Limiter(key_func=get_remote_address)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  Creates the bouncer. key_func tells the bouncer                â”‚
â”‚  HOW to identify each patron (by IP address here).              â”‚
â”‚                                                                 â”‚
â”‚  app.state.limiter = limiter                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  Posts the bouncer at the door (attaches to the app).           â”‚
â”‚  slowapi looks for this exact attribute name.                   â”‚
â”‚                                                                 â”‚
â”‚  app.add_exception_handler(RateLimitExceeded, ...)              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  Tells FastAPI: "When the bouncer rejects someone,              â”‚
â”‚  respond with a 429 status code."                               â”‚
â”‚  (Same custom exception handler pattern from Week 3)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.2 @limiter.limit() â€” Setting Limits

**Apply limits to individual endpoints with a decorator:**

```python
from fastapi import FastAPI, Request
from app.core.rate_limit import limiter

app = FastAPI()

# The route decorator MUST be above the limit decorator
@app.get("/api/v1/tasks")
@limiter.limit("30/minute")
async def get_tasks(request: Request):      # â† request MUST be a parameter!
    return {"tasks": []}

@app.post("/api/v1/tasks")
@limiter.limit("10/minute")
async def create_task(request: Request):    # â† request MUST be a parameter!
    return {"task": "created"}
```

**Two critical rules to remember:**

The request argument must be explicitly passed to your endpoint, or slowapi won't be able to hook into it. In other words, write `@limiter.limit("5/minute") async def myendpoint(request: Request)` and not `@limiter.limit("5/minute") async def myendpoint()`.[[3]](https://github.com/laurentS/slowapi)

```python
# âŒ WRONG: Missing request parameter â€” slowapi silently fails
@app.get("/items")
@limiter.limit("5/minute")
async def get_items():              # No request param!
    return {"items": []}

# âŒ WRONG: Decorators in wrong order
@limiter.limit("5/minute")         # Limit decorator on top
@app.get("/items")                  # Route decorator below
async def get_items(request: Request):
    return {"items": []}

# âœ… CORRECT: Route decorator first, request parameter present
@app.get("/items")
@limiter.limit("5/minute")
async def get_items(request: Request):
    return {"items": []}
```

**The rate limit string format:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RATE LIMIT STRING FORMAT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  FORMAT: "{count}/{period}"                                     â”‚
â”‚                                                                 â”‚
â”‚  "5/minute"       â†’  5 requests per minute                      â”‚
â”‚  "100/hour"       â†’  100 requests per hour                      â”‚
â”‚  "1/second"       â†’  1 request per second                       â”‚
â”‚  "1000/day"       â†’  1000 requests per day                      â”‚
â”‚                                                                 â”‚
â”‚  MULTIPLE LIMITS (semicolon-separated):                         â”‚
â”‚  "5/minute;100/hour"                                            â”‚
â”‚  â†’ Max 5 per minute AND max 100 per hour                        â”‚
â”‚  â†’ Both must be satisfied. Whichever hits first, blocks.        â”‚
â”‚                                                                 â”‚
â”‚  ALTERNATIVE SYNTAX:                                            â”‚
â”‚  "5 per minute"   â†’  Same as "5/minute"                         â”‚
â”‚  "100 per hour"   â†’  Same as "100/hour"                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.3 Key Functions â€” Identifying Clients

**Remember Section 1.3? The key function is *how* the bouncer identifies each patron.**

`get_remote_address` uses the client's IP. But for your authenticated API, you want per-user limits:

```python
# app/core/rate_limit.py
from fastapi import Request
from slowapi import Limiter
from slowapi.util import get_remote_address


def get_user_or_ip(request: Request) -> str:
    """
    Use the authenticated user's ID if available,
    fall back to IP address for anonymous requests.

    Connection to Week 9: request.state.user comes from
    your get_current_user dependency.
    """
    # If auth middleware has set a user on the request state
    if hasattr(request.state, "user") and request.state.user is not None:
        return str(request.state.user.id)

    # Fallback to IP for unauthenticated endpoints
    return get_remote_address(request)


# Use our custom key function
limiter = Limiter(key_func=get_user_or_ip)
```

**Per-endpoint key function override:**

```python
# Some endpoints need IP-based limiting regardless of auth
# (e.g., login â€” the user isn't authenticated YET)

@app.post("/api/v1/auth/login")
@limiter.limit("5/minute", key_func=get_remote_address)  # Override: always use IP
async def login(request: Request):
    # ... login logic from Week 9 ...
    pass


# Admin endpoints might get higher limits
def get_admin_key(request: Request) -> str:
    """Admins get their own separate limit bucket."""
    return f"admin:{get_remote_address(request)}"


@app.get("/api/v1/admin/users")
@limiter.limit("60/minute", key_func=get_admin_key)
async def admin_list_users(request: Request):
    # ... admin logic ...
    pass
```

**Visualize how key functions create separate buckets:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KEY FUNCTIONS â†’ SEPARATE BUCKETS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  get_remote_address:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ 192.168.1.1  â”‚  â”‚ 10.0.0.42    â”‚  â”‚ 172.16.0.5   â”‚          â”‚
â”‚  â”‚ Count: 4/10  â”‚  â”‚ Count: 8/10  â”‚  â”‚ Count: 1/10  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  Each IP gets its own counter.                                  â”‚
â”‚                                                                 â”‚
â”‚  get_user_or_ip:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ user:42      â”‚  â”‚ user:107     â”‚  â”‚ 10.0.0.99    â”‚          â”‚
â”‚  â”‚ Count: 4/10  â”‚  â”‚ Count: 8/10  â”‚  â”‚ Count: 1/10  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚  Authed users tracked by ID. Anon tracked by IP.                â”‚
â”‚  User 42 can switch WiFi â†’ still same bucket.                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.4 Per-Endpoint and Global Limits

**Different endpoints deserve different limits:**

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

# Global default: applies to ALL endpoints that don't have their own limit
limiter = Limiter(
    key_func=get_remote_address,
    default_limits=["100/minute"]        # Global safety net
)
```

```python
# READS are cheap â†’ higher limit
@app.get("/api/v1/tasks")
@limiter.limit("60/minute")
async def list_tasks(request: Request):
    return {"tasks": []}

# WRITES are expensive â†’ lower limit
@app.post("/api/v1/tasks")
@limiter.limit("20/minute")
async def create_task(request: Request):
    return {"task": "created"}

# SEARCH hits the database hard â†’ strict limit
@app.get("/api/v1/tasks/search")
@limiter.limit("10/minute")
async def search_tasks(request: Request):
    return {"results": []}

# AUTH endpoints: brute force protection â†’ very strict
@app.post("/api/v1/auth/login")
@limiter.limit("5/minute;20/hour", key_func=get_remote_address)
async def login(request: Request):
    return {"token": "..."}

# HEALTH CHECK: no limit (monitoring tools poll this)
@app.get("/health")
@limiter.exempt                          # â† Skip rate limiting entirely
async def health_check():
    return {"status": "ok"}
```

**The tier pattern:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ENDPOINT LIMIT TIERS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TIER        â”‚ ENDPOINTS              â”‚ LIMIT                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  Unrestrictedâ”‚ /health, /ready        â”‚ @limiter.exempt          â”‚
â”‚  Generous    â”‚ GET /tasks, GET /users â”‚ 60/minute               â”‚
â”‚  Moderate    â”‚ POST, PUT, PATCH       â”‚ 20/minute               â”‚
â”‚  Strict      â”‚ Search, export, report â”‚ 10/minute               â”‚
â”‚  Fortress    â”‚ Login, register, reset â”‚ 5/minute; 20/hour       â”‚
â”‚                                                                 â”‚
â”‚  PRINCIPLE: The more expensive the operation                    â”‚
â”‚  (CPU, DB, external calls), the tighter the limit.             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3.5 Redis-Backed Rate Limiting (Connection to Week 10)

**Remember the problem with in-memory rate limiting?**

> "Your Task Manager runs behind a load balancer with 3 server instances. In-memory counters are per-process. A client sends 10 requests â€” 3 hit instance A, 4 hit instance B, 3 hit instance C. Each instance sees under 10. *No one triggers the limit.* Your bouncer has amnesia across doors."

In a single-instance setup, SlowAPI keeps counters in memory. But once you scale horizontally (multiple containers or pods), each instance tracks limits independently â€” causing inconsistent throttling.[[7]](https://blog.schogini.com/static/html_files/multi-tenant-saas-with-redis-rate-limit.html)

Redis solves this by acting as a central store for counters and tokens.[[7]](https://blog.schogini.com/static/html_files/multi-tenant-saas-with-redis-rate-limit.html)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              IN-MEMORY vs REDIS STORAGE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  IN-MEMORY (default â€” dev only):                                â”‚
â”‚                                                                 â”‚
â”‚   Instance A          Instance B          Instance C            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Counter: 3â”‚       â”‚ Counter: 4â”‚       â”‚ Counter: 3â”‚         â”‚
â”‚  â”‚ (< 10 âœ…) â”‚       â”‚ (< 10 âœ…) â”‚       â”‚ (< 10 âœ…) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  Each thinks it's fine. Total: 10 requests. NONE blocked. âŒ    â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  REDIS (production):                                            â”‚
â”‚                                                                 â”‚
â”‚   Instance A          Instance B          Instance C            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Check â”€â”€â”€â”€â”‚â”€â”€â”    â”‚ Check â”€â”€â”€â”€â”‚â”€â”€â”    â”‚ Check â”€â”€â”€â”€â”‚â”€â”€â”      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚
â”‚                 â”‚                   â”‚                   â”‚      â”‚
â”‚                 â–¼                   â–¼                   â–¼      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚              â”‚         REDIS (shared counter)           â”‚       â”‚
â”‚              â”‚         Counter: 10 â†’ BLOCKED âŒ          â”‚       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  All instances share one counter. 10th request blocked. âœ…      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration â€” one line changes everything:**

```python
# app/core/rate_limit.py

# Development: in-memory (fine for single instance)
limiter = Limiter(key_func=get_user_or_ip)

# Production: Redis backend (Week 10 â€” you already have Redis running!)
limiter = Limiter(
    key_func=get_user_or_ip,
    storage_uri="redis://localhost:6379/1"   # Use DB 1 (DB 0 is your cache)
)
```

In production, pull from environment variables using `pydantic-settings` (same pattern you've used since Week 9):

```python
# app/core/config.py (your existing settings â€” just add one field)
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... your existing settings ...
    RATE_LIMIT_STORAGE_URI: str = "memory://"    # default: in-memory for dev

    model_config = {"env_file": ".env"}

settings = Settings()
```

```python
# app/core/rate_limit.py
from app.core.config import settings

limiter = Limiter(
    key_func=get_user_or_ip,
    storage_uri=settings.RATE_LIMIT_STORAGE_URI
)
```

```bash
# .env (production)
RATE_LIMIT_STORAGE_URI=redis://redis:6379/1

# .env (development)
RATE_LIMIT_STORAGE_URI=memory://
```

---

## 3.6 Custom 429 Handler

**The default `_rate_limit_exceeded_handler` returns a plain text response. For an API, you want structured JSON:**

```python
# app/core/rate_limit.py
from fastapi import Request
from fastapi.responses import JSONResponse
from slowapi.errors import RateLimitExceeded


async def custom_rate_limit_handler(
    request: Request,
    exc: RateLimitExceeded
) -> JSONResponse:
    """
    Return a structured 429 response that matches
    your API's error format (consistent with Week 3's
    custom exception handlers).
    """
    # exc.detail contains the limit that was exceeded, e.g. "5 per 1 minute"
    return JSONResponse(
        status_code=429,
        content={
            "error": "rate_limit_exceeded",
            "message": f"Rate limit exceeded: {exc.detail}",
            "retry_after": _get_retry_after(exc),
        },
        headers={
            "Retry-After": str(_get_retry_after(exc)),
        }
    )


def _get_retry_after(exc: RateLimitExceeded) -> int:
    """Extract or estimate seconds until the limit resets."""
    # Parse from the detail string â€” e.g., "5 per 1 minute" â†’ 60 seconds
    detail = exc.detail.lower()
    if "second" in detail:
        return 1
    elif "minute" in detail:
        return 60
    elif "hour" in detail:
        return 3600
    return 60  # sensible default
```

```python
# app/main.py â€” use custom handler instead of default
from app.core.rate_limit import limiter, custom_rate_limit_handler
from slowapi.errors import RateLimitExceeded

app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, custom_rate_limit_handler)
```

**Now a rate-limited client sees:**

```json
HTTP/1.1 429 Too Many Requests
Retry-After: 60
Content-Type: application/json

{
    "error": "rate_limit_exceeded",
    "message": "Rate limit exceeded: 5 per 1 minute",
    "retry_after": 60
}
```

> "This is what a well-behaved API looks like. It doesn't just slam the door â€” it tells the client *when to come back*. That's the Retry-After header. We'll formalize this in Part 4."

---

# PART 4: THE PROTOCOL

## 4.1 Rate Limit Headers (X-RateLimit-*)

**Rate limiting isn't just about blocking â€” it's about *communicating*.**

A good API tells clients their quota status on *every response*, not just when they're blocked. The commonly used header field names are: `X-RateLimit-Limit` â€” the maximum number of requests you're permitted to make per hour, `X-RateLimit-Remaining` â€” the number of requests remaining in the current rate limit window, and `X-RateLimit-Reset` â€” the time at which the current rate limit window resets in UTC epoch seconds.[[6]](https://www.gvj-web.com/blog/rate-limiting-restful-api)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RATE LIMIT HEADERS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  EVERY response (200, 201, 404, etc.) includes:                 â”‚
â”‚                                                                 â”‚
â”‚  X-RateLimit-Limit: 60        â† "Your quota is 60/minute"      â”‚
â”‚  X-RateLimit-Remaining: 42    â† "You have 42 requests left"    â”‚
â”‚  X-RateLimit-Reset: 1739548800â† "Quota resets at this time"     â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  WHEN BLOCKED (429 response) also includes:                     â”‚
â”‚                                                                 â”‚
â”‚  Retry-After: 35              â† "Try again in 35 seconds"      â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  THINK OF IT LIKE:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  The bouncer stamps your hand each time you enter:              â”‚
â”‚                                                                 â”‚
â”‚  X-RateLimit-Limit:     "Club capacity is 60 people/hour"       â”‚
â”‚  X-RateLimit-Remaining: "42 spots left this hour"               â”‚
â”‚  X-RateLimit-Reset:     "Counter resets at 2:00 AM"             â”‚
â”‚  Retry-After:           "Come back in 35 minutes"               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementing custom headers with middleware:**

```python
# app/middleware/rate_limit_headers.py
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import Response
import time


class RateLimitHeaderMiddleware(BaseHTTPMiddleware):
    """
    Inject X-RateLimit-* headers into every response.
    
    slowapi's default handler adds some headers on 429,
    but we want them on EVERY response so clients can
    proactively throttle themselves.
    """

    async def dispatch(self, request: Request, call_next) -> Response:
        response = await call_next(request)

        # These headers might already be set by slowapi on 429s
        # We ensure they're present on ALL responses
        # In production, you'd read actual values from your limiter/Redis
        # This is the pattern â€” actual values depend on your storage layer
        if "X-RateLimit-Limit" not in response.headers:
            # Provide informational defaults from your config
            response.headers["X-RateLimit-Limit"] = "60"
            response.headers["X-RateLimit-Remaining"] = "unknown"
            response.headers["X-RateLimit-Reset"] = str(
                int(time.time()) + 60
            )

        return response
```

> "Note: there's currently an IETF draft working on standardizing these as `RateLimit-Policy` and `RateLimit` headers. The `X-` prefix headers are the de facto standard that the industry uses today."

The IETF draft defines a set of standard HTTP header fields: `RateLimit-Policy` â€” a quota policy defined by the server, and `RateLimit` â€” the currently remaining quota available for a specific policy.[[5]](https://greenbytes.de/tech/webdav/draft-ietf-httpapi-ratelimit-headers-latest.html) Commonly used header field names today are: `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`.[[1]](https://datatracker.ietf.org/doc/draft-ietf-httpapi-ratelimit-headers/)

---

## 4.2 429 Too Many Requests + Retry-After

**429 is the HTTP status code specifically designed for rate limiting.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THE 429 RESPONSE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  HTTP/1.1 429 Too Many Requests                                 â”‚
â”‚  Content-Type: application/json                                 â”‚
â”‚  Retry-After: 45                                                â”‚
â”‚  X-RateLimit-Limit: 10                                          â”‚
â”‚  X-RateLimit-Remaining: 0                                       â”‚
â”‚  X-RateLimit-Reset: 1739548845                                  â”‚
â”‚                                                                 â”‚
â”‚  {                                                              â”‚
â”‚      "error": "rate_limit_exceeded",                            â”‚
â”‚      "message": "Rate limit exceeded: 10 per 1 minute",        â”‚
â”‚      "retry_after": 45                                          â”‚
â”‚  }                                                              â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  KEY: Retry-After tells the client EXACTLY when to try again.   â”‚
â”‚  A well-behaved client reads this and sleeps. A bad client      â”‚
â”‚  ignores it and gets blocked again. That's their problem.       â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  Retry-After can be:                                            â”‚
â”‚  â€¢ Seconds (integer):  Retry-After: 45                          â”‚
â”‚  â€¢ HTTP date:          Retry-After: Fri, 14 Feb 2026 12:30:00   â”‚
â”‚                                                                 â”‚
â”‚  Prefer seconds â€” simpler for clients to parse.                 â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4.3 Full Circle: Server â†” Client (Connection to Week 8)

**Remember Week 8? You learned to RESPECT rate limits as a client. Now close the loop.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WEEK 8 (Client) â†â†’ WEEK 12 (Server)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WEEK 8: You were the CLIENT                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  â€¢ You read X-RateLimit-Remaining headers                       â”‚
â”‚  â€¢ You implemented exponential backoff (tenacity)               â”‚
â”‚  â€¢ You respected Retry-After                                    â”‚
â”‚  â€¢ You built client-side rate limiting (token bucket)           â”‚
â”‚                                                                 â”‚
â”‚  WEEK 12: You are now the SERVER                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  â€¢ You SEND X-RateLimit-* headers                               â”‚
â”‚  â€¢ You RETURN 429 + Retry-After                                 â”‚
â”‚  â€¢ You ENFORCE server-side rate limiting                        â”‚
â”‚  â€¢ You PROTECT your resources from abusive clients              â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  THE FULL PICTURE:                                              â”‚
â”‚                                                                 â”‚
â”‚  [Your Client Code]  â”€â”€requestâ”€â”€â–¶  [External API]               â”‚
â”‚  (Week 8: respect                  (Their rate limiter)         â”‚
â”‚   their limits)                                                 â”‚
â”‚                                                                 â”‚
â”‚  [Someone's Client]  â”€â”€requestâ”€â”€â–¶  [YOUR API]                   â”‚
â”‚  (Their responsibility             (Week 12: YOUR rate limiter) â”‚
â”‚   to back off)                                                  â”‚
â”‚                                                                 â”‚
â”‚  You've now built BOTH sides of the contract.                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> "In Week 8 you learned to be a good guest at someone else's party. Now you're the host. Set clear rules, communicate them in headers, and enforce them fairly."

---

# PART 5: MEASURING IMPACT

## 5.1 Why Averages Lie

**Before we measure rate limiting's impact, we need to understand HOW to measure.**

> "I'm about to show you why 'average response time' is the most dangerous metric in your dashboard."

```python
# 100 API requests. 99 are fast. 1 is a disaster.
response_times_ms = [12, 11, 13, 10, 14, 12, 11, 13, 10, 12,  # ... 
                     11, 13, 12, 10, 14, 11, 12, 13, 10, 11,
                     # ... (99 requests between 10-14ms)
                     4200]  # One request took 4.2 seconds

average = sum(response_times_ms) / len(response_times_ms)
# average â‰ˆ 53ms
```

> "53ms average. Dashboard shows green. Ship it?"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHY AVERAGES LIE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  100 requests:                                                  â”‚
â”‚  â”œâ”€ 99 requests at ~12ms                                        â”‚
â”‚  â””â”€  1 request  at 4200ms                                       â”‚
â”‚                                                                 â”‚
â”‚  Average:  53ms      â† "Looks fine!" ğŸ˜Š                         â”‚
â”‚  Reality:  1 user waited 4.2 SECONDS                            â”‚
â”‚                                                                 â”‚
â”‚  Now scale it:                                                  â”‚
â”‚  â€¢ 10,000 requests/day at 1% failure = 100 terrible experiences â”‚
â”‚  â€¢ 1,000,000 requests/day at 1% = 10,000 angry users           â”‚
â”‚                                                                 â”‚
â”‚  The average HIDES the suffering of the tail.                   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

A handful of very slow outliers (GC pauses, cold starts, retries, network hiccups, lock contention) can inflate the average without affecting most users.[[1]](https://oneuptime.com/blog/post/2025-09-15-p50-vs-p95-vs-p99-latency-percentiles/view)

Don't trust averages â€” they lie. Percentiles tell you what your users are actually experiencing.[[4]](https://medium.com/@subodh.shetty87/not-all-slowness-is-equal-a-developers-guide-to-p50-p95-and-p99-latencies-c473b9ea6fb9)

---

## 5.2 Percentile Latencies (p50, p95, p99)

**Percentiles show you the DISTRIBUTION of response times, not just one misleading number.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              WHAT PERCENTILES MEAN                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Sort ALL response times from fastest to slowest:               â”‚
â”‚                                                                 â”‚
â”‚  User   1:  â–ˆâ–ˆ 10ms                                             â”‚
â”‚  User   2:  â–ˆâ–ˆ 11ms                                             â”‚
â”‚  ...                                                            â”‚
â”‚  User  50:  â–ˆâ–ˆâ–ˆ 13ms            â† p50 (MEDIAN): 13ms           â”‚
â”‚  ...                               "Typical" user experience    â”‚
â”‚  User  95:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45ms         â† p95: 45ms                    â”‚
â”‚  ...                               5% of users are slower       â”‚
â”‚  User  99:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 180ms â† p99: 180ms                   â”‚
â”‚  User 100:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 4200ms               â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  p50 (median):  13ms   "What a TYPICAL user sees"               â”‚
â”‚  p95:           45ms   "What an UNLUCKY user sees"              â”‚
â”‚  p99:          180ms   "What the WORST 1% experience"           â”‚
â”‚  Average:       53ms   "A number that describes NOBODY"         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How to calculate them:**

```python
import numpy as np

response_times = [12, 11, 45, 13, 180, 10, 14, 12, 4200, 11, ...]  
# (pretend this is 100 values)

p50 = np.percentile(response_times, 50)   # Median
p95 = np.percentile(response_times, 95)   # 95th percentile
p99 = np.percentile(response_times, 99)   # 99th percentile

print(f"p50: {p50:.0f}ms  p95: {p95:.0f}ms  p99: {p99:.0f}ms")
```

**What each tells you:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PERCENTILE CHEAT SHEET                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  p50 (median)                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  The "typical" experience. Half faster, half slower.            â”‚
â”‚  Use it to: Detect broad regressions affecting everyone.        â”‚
â”‚  If p50 is bad, your whole system is slow.                      â”‚
â”‚                                                                 â”‚
â”‚  p95                                                            â”‚
â”‚  â”€â”€â”€â”€                                                           â”‚
â”‚  The "bad luck" experience. 1 in 20 users sees this.            â”‚
â”‚  Use it to: Tune system performance. Set SLOs.                  â”‚
â”‚  This is usually the PRIMARY metric for SLOs.                   â”‚
â”‚                                                                 â”‚
â”‚  p99                                                            â”‚
â”‚  â”€â”€â”€â”€                                                           â”‚
â”‚  The "worst reasonable case." 1 in 100 users.                   â”‚
â”‚  Use it to: Expose architectural bottlenecks.                   â”‚
â”‚  If p99 is 10x worse than p50, something is very wrong.         â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  HEALTHY SYSTEM:                                                â”‚
â”‚  p50: 20ms    p95: 40ms     p99: 80ms                           â”‚
â”‚  (Tight spread. Consistent. Good.)                              â”‚
â”‚                                                                 â”‚
â”‚  SICK SYSTEM:                                                   â”‚
â”‚  p50: 20ms    p95: 200ms    p99: 2000ms                         â”‚
â”‚  (Huge spread. Something is occasionally VERY slow.)            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

A healthy system has percentiles that are relatively close together â€” p50 of 20ms, p95 of 40ms, p99 of 80ms. The slow requests are only a few times slower than the fast ones.[[8]](https://www.zoyla.app/resources/latency-percentiles-guide) A problematic system has a long tail â€” p50 of 20ms, p95 of 200ms, p99 of 2000ms. Something is causing occasional massive slowdowns â€” maybe a database query that sometimes hits a slow path, maybe garbage collection pauses, maybe external service timeouts. The gap between p50 and p99 tells you how consistent your performance is.[[8]](https://www.zoyla.app/resources/latency-percentiles-guide)

**Now ask the class:**

> "Your Task Manager API has p50 of 35ms but p99 of 3200ms. Is your API healthy? Where would you start investigating?"

The answer: If your p99 is bad but p50 is fine, you have an outlier problem â€” something is occasionally slow. Common causes: slow database queries that only trigger on certain data, external services with variable response times, and resource contention under load.[[8]](https://www.zoyla.app/resources/latency-percentiles-guide) They should check their EXPLAIN plans from Week 7, their Redis cache hit rates from Week 10, and their external API response times from Week 8.

---

## 5.3 Load Testing with Rate Limits (Connection to Lecture 3)

**In Lecture 3 you learned locust. Now use it to verify your rate limiting works.**

The test has two phases: measure WITHOUT limits (baseline), then WITH limits (protected).

```python
# tests/load/locustfile.py
from locust import HttpUser, task, between


class TaskManagerUser(HttpUser):
    """Simulates a typical user of our Task Manager API."""
    wait_time = between(0.5, 2)    # Random wait between requests
    host = "http://localhost:8000"

    def on_start(self):
        """Login once to get a token (Week 9 auth)."""
        response = self.client.post("/api/v1/auth/login", json={
            "email": "loadtest@example.com",
            "password": "testpassword123"
        })
        self.token = response.json()["access_token"]
        self.headers = {"Authorization": f"Bearer {self.token}"}

    @task(5)    # Weight: 5x more likely than create
    def list_tasks(self):
        self.client.get("/api/v1/tasks", headers=self.headers)

    @task(2)
    def get_single_task(self):
        self.client.get("/api/v1/tasks/1", headers=self.headers)

    @task(1)
    def create_task(self):
        self.client.post("/api/v1/tasks", headers=self.headers, json={
            "title": "Load test task",
            "description": "Created during load test",
            "priority": "medium"
        })

    @task(1)
    def search_tasks(self):
        self.client.get(
            "/api/v1/tasks/search?q=load",
            headers=self.headers
        )
```

**Run the load test and observe 429s:**

```bash
# Headless mode â€” outputs to terminal and CSV
locust --headless \
    --host=http://localhost:8000 \
    -u 50 -r 10 \
    --run-time 60s \
    --csv=results \
    -f tests/load/locustfile.py
```

**What to look for in the output:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LOAD TEST: WITH vs WITHOUT RATE LIMITS             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WITHOUT RATE LIMITS:                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  Requests:  2847 total, 0 failures                              â”‚
â”‚  p50: 35ms    p95: 180ms    p99: 890ms                          â”‚
â”‚  Status codes: {200: 2847}                                      â”‚
â”‚                                                                 â”‚
â”‚  â†’ All requests served. But p99 is climbing.                    â”‚
â”‚    The server is straining. At 100 users it would buckle.       â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  WITH RATE LIMITS (60/min per user):                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚  Requests:  2847 total, 1923 "failures" (429s)                  â”‚
â”‚  p50: 22ms    p95: 45ms     p99: 120ms                          â”‚
â”‚  Status codes: {200: 924, 429: 1923}                            â”‚
â”‚                                                                 â”‚
â”‚  â†’ 429s are NOT failures â€” they're PROTECTION WORKING.          â”‚
â”‚    The served requests are faster (lower p99) because           â”‚
â”‚    the server isn't drowning.                                   â”‚
â”‚                                                                 â”‚
â”‚  THE INSIGHT:                                                   â”‚
â”‚  Rate limiting made the 200 responses FASTER for everyone.      â”‚
â”‚  Fewer requests served, but served BETTER.                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.4 Performance Regression Testing in CI

**You measured performance today. How do you ensure it doesn't degrade tomorrow?**

The answer: **run a lightweight load test in your CI pipeline and fail the build if performance drops.** You'll formalize your CI pipeline in Week 15 â€” this is the performance testing piece that slots into it.

```python
# scripts/check_performance.py
"""
Read locust CSV output and check against performance thresholds.
Exit with code 1 (fail the build) if any threshold is exceeded.
"""
import csv
import sys


THRESHOLDS = {
    "p95_ms": 200,          # p95 must be under 200ms
    "p99_ms": 500,          # p99 must be under 500ms
    "failure_pct": 5.0,     # Max 5% non-429 failures
}


def check_thresholds(csv_path: str) -> bool:
    passed = True

    with open(csv_path) as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row["Name"] != "Aggregated":
                continue

            p95 = float(row["95%"])
            p99 = float(row["99%"])
            total = int(row["Request Count"])
            failures = int(row["Failure Count"])
            failure_pct = (failures / total) * 100 if total > 0 else 0

            # Check each threshold
            if p95 > THRESHOLDS["p95_ms"]:
                print(f"âŒ FAIL: p95 = {p95:.0f}ms (threshold: {THRESHOLDS['p95_ms']}ms)")
                passed = False
            else:
                print(f"âœ… PASS: p95 = {p95:.0f}ms")

            if p99 > THRESHOLDS["p99_ms"]:
                print(f"âŒ FAIL: p99 = {p99:.0f}ms (threshold: {THRESHOLDS['p99_ms']}ms)")
                passed = False
            else:
                print(f"âœ… PASS: p99 = {p99:.0f}ms")

            if failure_pct > THRESHOLDS["failure_pct"]:
                print(f"âŒ FAIL: failure rate = {failure_pct:.1f}% (threshold: {THRESHOLDS['failure_pct']}%)")
                passed = False
            else:
                print(f"âœ… PASS: failure rate = {failure_pct:.1f}%")

    return passed


if __name__ == "__main__":
    csv_path = sys.argv[1] if len(sys.argv) > 1 else "results_stats.csv"

    if not check_thresholds(csv_path):
        print("\nğŸš¨ Performance regression detected!")
        sys.exit(1)     # Fail the CI build

    print("\nğŸ‰ All performance thresholds met.")
    sys.exit(0)
```

**How this fits into CI (preview for Week 15):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CI PIPELINE WITH PERFORMANCE GATE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  git push                                                       â”‚
â”‚    â”‚                                                            â”‚
â”‚    â–¼                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ 1. Lint (ruff)  â”‚ â† Already doing this (Week 1)             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ 2. Type check   â”‚ â† mypy (Week 1)                           â”‚
â”‚  â”‚    (mypy)       â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ 3. Unit tests   â”‚ â† pytest (Week 2)                         â”‚
â”‚  â”‚    (pytest)     â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ 4. Start services (docker compose up)       â”‚ â† NEW         â”‚
â”‚  â”‚    Run load test (locust --headless, 60s)   â”‚                â”‚
â”‚  â”‚    Check thresholds (check_performance.py)  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚           â”‚                                                     â”‚
â”‚      PASS â”‚  FAIL                                               â”‚
â”‚           â”‚    â”‚                                                â”‚
â”‚           â–¼    â–¼                                                â”‚
â”‚         Deploy  Block + Notify                                  â”‚
â”‚                                                                 â”‚
â”‚  The performance gate catches regressions BEFORE production.    â”‚
â”‚  "We don't ship code that makes the API slower."                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5.5 Setting Thresholds and Alerts

**How do you decide what "too slow" means?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              THRESHOLD GUIDELINES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  STEP 1: MEASURE YOUR BASELINE                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  Run your load test on the CURRENT healthy system.              â”‚
â”‚  Record p50, p95, p99. This is your baseline.                   â”‚
â”‚                                                                 â”‚
â”‚  Example baseline:                                              â”‚
â”‚    p50: 25ms    p95: 80ms    p99: 150ms                         â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  STEP 2: SET THRESHOLDS WITH HEADROOM                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚  Add 2x-3x margin above your baseline.                         â”‚
â”‚  Thresholds that are too tight cause false alarms.              â”‚
â”‚  Thresholds that are too loose miss real regressions.           â”‚
â”‚                                                                 â”‚
â”‚  Threshold recommendation:                                      â”‚
â”‚    p95 threshold: 200ms   (2.5x baseline)                       â”‚
â”‚    p99 threshold: 500ms   (3.3x baseline)                       â”‚
â”‚    Failure rate: < 1% (excluding 429s)                          â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  STEP 3: WATCH THE RATIO                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  The gap between p50 and p99 is as important as                 â”‚
â”‚  the absolute values.                                           â”‚
â”‚                                                                 â”‚
â”‚  p99 / p50 < 5x  â†’ Consistent. Healthy.                        â”‚
â”‚  p99 / p50 > 10x â†’ Investigate! Something is occasionally      â”‚
â”‚                     very slow (missing index? cold cache?       â”‚
â”‚                     external API timeout?)                      â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  STEP 4: DISTINGUISH 429s FROM REAL ERRORS                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚  In your load test results, 429 is NOT a failure.               â”‚
â”‚  It's your rate limiter WORKING AS DESIGNED.                    â”‚
â”‚  Filter 429s out when calculating error rate.                   â”‚
â”‚  Only 5xx and unexpected 4xx count as real failures.            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Use p50 to detect broad regressions, p95 to tune system performance, p99 to expose architectural bottlenecks and outliers.[[1]](https://oneuptime.com/blog/post/2025-09-15-p50-vs-p95-vs-p99-latency-percentiles/view)

---

# Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RATE LIMITING QUICK REFERENCE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INSTALL:                                                       â”‚
â”‚      pip install slowapi                                        â”‚
â”‚                                                                 â”‚
â”‚  SETUP:                                                         â”‚
â”‚      limiter = Limiter(key_func=get_remote_address)             â”‚
â”‚      app.state.limiter = limiter                                â”‚
â”‚      app.add_exception_handler(RateLimitExceeded, handler)      â”‚
â”‚                                                                 â”‚
â”‚  DECORATE (route decorator ABOVE limit decorator):              â”‚
â”‚      @app.get("/items")                                         â”‚
â”‚      @limiter.limit("30/minute")                                â”‚
â”‚      async def get_items(request: Request):  # â† Required!     â”‚
â”‚                                                                 â”‚
â”‚  LIMIT STRINGS:                                                 â”‚
â”‚      "5/minute"         "100/hour"         "1000/day"           â”‚
â”‚      "5/minute;100/hour"  (multiple, semicolon-separated)       â”‚
â”‚                                                                 â”‚
â”‚  KEY FUNCTIONS:                                                 â”‚
â”‚      get_remote_address          â†’ limit by IP                  â”‚
â”‚      custom: return user.id      â†’ limit by user                â”‚
â”‚      per-endpoint override       â†’ key_func= argument           â”‚
â”‚                                                                 â”‚
â”‚  REDIS BACKEND:                                                 â”‚
â”‚      Limiter(key_func=..., storage_uri="redis://host:6379/1")   â”‚
â”‚                                                                 â”‚
â”‚  EXEMPT AN ENDPOINT:                                            â”‚
â”‚      @limiter.exempt                                            â”‚
â”‚                                                                 â”‚
â”‚  GLOBAL DEFAULT:                                                â”‚
â”‚      Limiter(key_func=..., default_limits=["100/minute"])       â”‚
â”‚                                                                 â”‚
â”‚  HEADERS TO SEND:                                               â”‚
â”‚      X-RateLimit-Limit     â†’ quota total                        â”‚
â”‚      X-RateLimit-Remaining â†’ requests left                      â”‚
â”‚      X-RateLimit-Reset     â†’ when window resets (epoch)         â”‚
â”‚      Retry-After           â†’ seconds until retry (on 429 only)  â”‚
â”‚                                                                 â”‚
â”‚  PERCENTILES:                                                   â”‚
â”‚      p50 â†’ median ("typical" user)                              â”‚
â”‚      p95 â†’ tail latency ("unlucky" user, use for SLOs)          â”‚
â”‚      p99 â†’ critical tail ("worst 1%", find bottlenecks)         â”‚
â”‚                                                                 â”‚
â”‚  COMMON MISTAKES:                                               â”‚
â”‚      âŒ Missing request: Request in endpoint signature           â”‚
â”‚      âŒ Wrong decorator order (route must be above limit)        â”‚
â”‚      âŒ In-memory storage with multiple server instances         â”‚
â”‚      âŒ Trusting average response time instead of percentiles    â”‚
â”‚      âŒ Counting 429s as "failures" in load test reports         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Summary: The Key Mental Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  RATE LIMITING = CONTROLLED ACCESS                              â”‚
â”‚                                                                 â”‚
â”‚  Your API has finite resources. Rate limiting ensures            â”‚
â”‚  those resources are shared fairly, protected from abuse,       â”‚
â”‚  and consumed within your cost budget.                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Incoming â”‚ â”€â”€â–¶ â”‚ Rate     â”‚ â”€â”€â–¶ â”‚ Your API     â”‚            â”‚
â”‚  â”‚ Request  â”‚     â”‚ Limiter  â”‚     â”‚ (protected)  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚       â”‚                â”‚                                        â”‚
â”‚       â”‚          â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                  â”‚
â”‚       â”‚          â”‚   Under   â”‚ YES â†’ Allow through              â”‚
â”‚       â”‚          â”‚   limit?  â”‚                                  â”‚
â”‚       â”‚          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚       â”‚                â”‚ NO                                     â”‚
â”‚       â”‚                â–¼                                        â”‚
â”‚       â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚       â”‚          â”‚ 429 + Tellâ”‚                                  â”‚
â”‚       â”‚          â”‚ them when â”‚                                  â”‚
â”‚       â”‚          â”‚ to retry  â”‚                                  â”‚
â”‚       â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  THE NIGHTCLUB BOUNCER:                                         â”‚
â”‚  â”œâ”€ Key function   = How the bouncer identifies you             â”‚
â”‚  â”œâ”€ Algorithm      = How the bouncer counts                     â”‚
â”‚  â”œâ”€ slowapi        = The bouncer's toolkit                      â”‚
â”‚  â”œâ”€ Redis backend  = Shared clipboard across all doors          â”‚
â”‚  â”œâ”€ Rate headers   = Stamp on your hand showing quota left      â”‚
â”‚  â”œâ”€ 429 + Retry    = "Come back in 45 seconds"                  â”‚
â”‚  â””â”€ Percentiles    = How you MEASURE the bouncer's impact       â”‚
â”‚                                                                 â”‚
â”‚                                                                 â”‚
â”‚  MEASURE WITH PERCENTILES, NOT AVERAGES:                        â”‚
â”‚  â”œâ”€ p50 = Typical experience                                    â”‚
â”‚  â”œâ”€ p95 = The SLO target (what you promise your users)          â”‚
â”‚  â”œâ”€ p99 = Where hidden problems live                            â”‚
â”‚  â””â”€ avg = A liar. Never trust it alone.                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Connection to Upcoming Content

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHERE THIS LEADS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  WEEK 12 PROJECT (this week):                                   â”‚
â”‚  â””â”€ Load test your API. Target: 500 req/min, <200ms p95.       â”‚
â”‚     Rate limiting is one of the 3 documented optimizations.     â”‚
â”‚     Before/after metrics required.                              â”‚
â”‚                                                                 â”‚
â”‚  WEEK 13-14 (Capstone):                                         â”‚
â”‚  â””â”€ Multi-tenant SaaS backend                                   â”‚
â”‚     Rate limits PER TENANT (org A gets 100/min,                 â”‚
â”‚     org B gets 500/min). Dynamic limits, not hardcoded.         â”‚
â”‚     This is real SaaS product design.                           â”‚
â”‚                                                                 â”‚
â”‚  WEEK 15 (CI/CD):                                               â”‚
â”‚  â””â”€ Your check_performance.py script becomes a real             â”‚
â”‚     GitHub Actions step. Performance regression testing         â”‚
â”‚     runs on every pull request. Automated quality gates.        â”‚
â”‚                                                                 â”‚
â”‚  WEEK 16 (System Design):                                       â”‚
â”‚  â””â”€ "Design a rate limiter" is a CLASSIC system design          â”‚
â”‚     interview question. You now understand the algorithms,      â”‚
â”‚     the distributed storage (Redis), the trade-offs,            â”‚
â”‚     and the protocol (headers). You've BUILT one.               â”‚
â”‚     Most candidates can only describe one.                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```